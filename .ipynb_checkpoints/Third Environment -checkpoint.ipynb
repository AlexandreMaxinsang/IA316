{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Concatenate, Dropout, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third environmement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['action_history', 'nb_items', 'nb_users', 'next_state', 'rewards_history', 'state_history'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attributes fa\n",
    "USER_ID = 'IAHIZPIW80WPGDW7P7JE'\n",
    "BASE_URL = \"http://35.180.178.243\"\n",
    "url_reset = BASE_URL + \"/reset\"\n",
    "url_predict = BASE_URL + \"/predict\"\n",
    "params = {\"user_id\" : USER_ID}\n",
    "\n",
    "def load_data(url_reset,params) :\n",
    "    r = requests.get(url=url_reset, params=params)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "data = load_data(url_reset,params)\n",
    "nb_users = data[\"nb_users\"]\n",
    "nb_items = data[\"nb_items\"]\n",
    "next_state = data[\"next_state\"]\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #See the request predict from our API\n",
    "# params = {\"user_id\" : user_id,\"recommended_item\": 2}\n",
    "# requests.get(url=url_predict, params=params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[59, 0, 307.52158737176507, 1.359401132544833...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[57, 0, 307.52158737176507, 0.889921827740519...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[25, 0, 307.52158737176507, 0.749577322065220...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[11, 0, 307.52158737176507, 0.024178763006033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[28, 0, 307.52158737176507, 0.878714795386164...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0              10              0.0   \n",
       "1               3              0.0   \n",
       "2              12              0.0   \n",
       "3              29              0.0   \n",
       "4               6              0.0   \n",
       "\n",
       "                                       state_history  \n",
       "0  [[59, 0, 307.52158737176507, 1.359401132544833...  \n",
       "1  [[57, 0, 307.52158737176507, 0.889921827740519...  \n",
       "2  [[25, 0, 307.52158737176507, 0.749577322065220...  \n",
       "3  [[11, 0, 307.52158737176507, 0.024178763006033...  \n",
       "4  [[28, 0, 307.52158737176507, 0.878714795386164...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklisted_set = set((\"next_state\",\"nb_items\",\"nb_users\"))\n",
    "new_dict = {key : value for key, value in data.items() if key not in blacklisted_set}\n",
    "\n",
    "df = pd.DataFrame(new_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.820000</td>\n",
       "      <td>171.959403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.780266</td>\n",
       "      <td>316.225701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>185.971676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>998.902711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       action_history  rewards_history\n",
       "count      200.000000       200.000000\n",
       "mean        14.820000       171.959403\n",
       "std          8.780266       316.225701\n",
       "min          0.000000         0.000000\n",
       "25%          8.000000         0.000000\n",
       "50%         15.000000         0.000000\n",
       "75%         23.000000       185.971676\n",
       "max         29.000000       998.902711"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action_history     141\n",
       "rewards_history    141\n",
       "state_history      141\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['rewards_history']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>307.521587</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.508785</td>\n",
       "      <td>0.377067</td>\n",
       "      <td>1.097383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>968.823563</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>0.808408</td>\n",
       "      <td>-0.452218</td>\n",
       "      <td>1.624187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>826.061991</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.542081</td>\n",
       "      <td>0.462825</td>\n",
       "      <td>1.233995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>467.110221</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>0.361892</td>\n",
       "      <td>0.311249</td>\n",
       "      <td>0.442961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>185.971676</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>2.055851</td>\n",
       "      <td>1.647847</td>\n",
       "      <td>0.399498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>209.336191</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.076881</td>\n",
       "      <td>1.802337</td>\n",
       "      <td>0.890867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>921.811589</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>0.753801</td>\n",
       "      <td>-0.223070</td>\n",
       "      <td>0.001840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>405.733607</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.535002</td>\n",
       "      <td>0.826051</td>\n",
       "      <td>2.544656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59</td>\n",
       "      <td>8</td>\n",
       "      <td>32.439328</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.274251</td>\n",
       "      <td>0.077147</td>\n",
       "      <td>-0.650352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59</td>\n",
       "      <td>9</td>\n",
       "      <td>279.119184</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>0.571544</td>\n",
       "      <td>2.196910</td>\n",
       "      <td>1.475017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>555.558416</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.583433</td>\n",
       "      <td>0.144717</td>\n",
       "      <td>1.371365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>59</td>\n",
       "      <td>11</td>\n",
       "      <td>290.976585</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>-0.055865</td>\n",
       "      <td>1.775633</td>\n",
       "      <td>1.316965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>327.381251</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>2.043326</td>\n",
       "      <td>1.548533</td>\n",
       "      <td>0.386704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>59</td>\n",
       "      <td>13</td>\n",
       "      <td>215.535596</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>0.300978</td>\n",
       "      <td>0.652537</td>\n",
       "      <td>1.427994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>59</td>\n",
       "      <td>14</td>\n",
       "      <td>963.796932</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.040349</td>\n",
       "      <td>0.383342</td>\n",
       "      <td>0.245015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59</td>\n",
       "      <td>15</td>\n",
       "      <td>618.798931</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.756298</td>\n",
       "      <td>0.562498</td>\n",
       "      <td>0.644122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>59</td>\n",
       "      <td>16</td>\n",
       "      <td>172.645709</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.285766</td>\n",
       "      <td>2.293980</td>\n",
       "      <td>-0.104596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>59</td>\n",
       "      <td>17</td>\n",
       "      <td>565.082253</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>2.674176</td>\n",
       "      <td>0.532584</td>\n",
       "      <td>0.813754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>59</td>\n",
       "      <td>18</td>\n",
       "      <td>913.213161</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>0.438503</td>\n",
       "      <td>-0.351983</td>\n",
       "      <td>-0.025743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>340.260286</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>-0.404461</td>\n",
       "      <td>1.569953</td>\n",
       "      <td>1.252886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59</td>\n",
       "      <td>20</td>\n",
       "      <td>808.822307</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>0.865467</td>\n",
       "      <td>2.306501</td>\n",
       "      <td>0.276289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>59</td>\n",
       "      <td>21</td>\n",
       "      <td>873.773138</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>0.852525</td>\n",
       "      <td>1.822191</td>\n",
       "      <td>1.541361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>59</td>\n",
       "      <td>22</td>\n",
       "      <td>907.518233</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>2.083926</td>\n",
       "      <td>-0.225465</td>\n",
       "      <td>2.509941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>59</td>\n",
       "      <td>23</td>\n",
       "      <td>925.729803</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>0.236227</td>\n",
       "      <td>-0.785335</td>\n",
       "      <td>0.810007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>59</td>\n",
       "      <td>24</td>\n",
       "      <td>653.783183</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.394127</td>\n",
       "      <td>2.779995</td>\n",
       "      <td>1.553988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>59</td>\n",
       "      <td>25</td>\n",
       "      <td>471.488142</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>-0.305295</td>\n",
       "      <td>1.674047</td>\n",
       "      <td>1.258930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>59</td>\n",
       "      <td>26</td>\n",
       "      <td>998.902711</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>-0.018837</td>\n",
       "      <td>-0.224247</td>\n",
       "      <td>-0.839753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>59</td>\n",
       "      <td>27</td>\n",
       "      <td>951.980184</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.207486</td>\n",
       "      <td>2.243383</td>\n",
       "      <td>-0.049420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>795.676216</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>0.701131</td>\n",
       "      <td>0.576746</td>\n",
       "      <td>0.830703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>59</td>\n",
       "      <td>29</td>\n",
       "      <td>678.943523</td>\n",
       "      <td>1.359401</td>\n",
       "      <td>-0.047853</td>\n",
       "      <td>1.470439</td>\n",
       "      <td>1.492885</td>\n",
       "      <td>3.523529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>795.676216</td>\n",
       "      <td>1.822076</td>\n",
       "      <td>1.876768</td>\n",
       "      <td>0.701131</td>\n",
       "      <td>0.576746</td>\n",
       "      <td>1.523260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5920</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>678.943523</td>\n",
       "      <td>1.822076</td>\n",
       "      <td>1.876768</td>\n",
       "      <td>1.470439</td>\n",
       "      <td>1.492885</td>\n",
       "      <td>-0.732746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5921</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>307.521587</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.508785</td>\n",
       "      <td>0.377067</td>\n",
       "      <td>2.255857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>968.823563</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>0.808408</td>\n",
       "      <td>-0.452218</td>\n",
       "      <td>0.459862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>826.061991</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.542081</td>\n",
       "      <td>0.462825</td>\n",
       "      <td>2.889117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>467.110221</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>0.361892</td>\n",
       "      <td>0.311249</td>\n",
       "      <td>0.106354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925</th>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>185.971676</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>2.055851</td>\n",
       "      <td>1.647847</td>\n",
       "      <td>1.967072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5926</th>\n",
       "      <td>99</td>\n",
       "      <td>5</td>\n",
       "      <td>209.336191</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.076881</td>\n",
       "      <td>1.802337</td>\n",
       "      <td>0.270491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5927</th>\n",
       "      <td>99</td>\n",
       "      <td>6</td>\n",
       "      <td>921.811589</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>0.753801</td>\n",
       "      <td>-0.223070</td>\n",
       "      <td>0.246986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5928</th>\n",
       "      <td>99</td>\n",
       "      <td>7</td>\n",
       "      <td>405.733607</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.535002</td>\n",
       "      <td>0.826051</td>\n",
       "      <td>2.740238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5929</th>\n",
       "      <td>99</td>\n",
       "      <td>8</td>\n",
       "      <td>32.439328</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.274251</td>\n",
       "      <td>0.077147</td>\n",
       "      <td>0.398225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5930</th>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "      <td>279.119184</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>0.571544</td>\n",
       "      <td>2.196910</td>\n",
       "      <td>0.240248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "      <td>555.558416</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.583433</td>\n",
       "      <td>0.144717</td>\n",
       "      <td>1.107337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>99</td>\n",
       "      <td>12</td>\n",
       "      <td>327.381251</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>2.043326</td>\n",
       "      <td>1.548533</td>\n",
       "      <td>-1.166868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>99</td>\n",
       "      <td>13</td>\n",
       "      <td>215.535596</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>0.300978</td>\n",
       "      <td>0.652537</td>\n",
       "      <td>0.079760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5934</th>\n",
       "      <td>99</td>\n",
       "      <td>14</td>\n",
       "      <td>963.796932</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.040349</td>\n",
       "      <td>0.383342</td>\n",
       "      <td>-1.432364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>99</td>\n",
       "      <td>15</td>\n",
       "      <td>618.798931</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.756298</td>\n",
       "      <td>0.562498</td>\n",
       "      <td>0.844611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>99</td>\n",
       "      <td>16</td>\n",
       "      <td>172.645709</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.285766</td>\n",
       "      <td>2.293980</td>\n",
       "      <td>1.028488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>99</td>\n",
       "      <td>17</td>\n",
       "      <td>565.082253</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>2.674176</td>\n",
       "      <td>0.532584</td>\n",
       "      <td>1.667871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>99</td>\n",
       "      <td>18</td>\n",
       "      <td>913.213161</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>0.438503</td>\n",
       "      <td>-0.351983</td>\n",
       "      <td>0.270501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>99</td>\n",
       "      <td>19</td>\n",
       "      <td>340.260286</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>-0.404461</td>\n",
       "      <td>1.569953</td>\n",
       "      <td>0.607123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5940</th>\n",
       "      <td>99</td>\n",
       "      <td>20</td>\n",
       "      <td>808.822307</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>0.865467</td>\n",
       "      <td>2.306501</td>\n",
       "      <td>0.589030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>99</td>\n",
       "      <td>21</td>\n",
       "      <td>873.773138</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>0.852525</td>\n",
       "      <td>1.822191</td>\n",
       "      <td>1.759993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5942</th>\n",
       "      <td>99</td>\n",
       "      <td>22</td>\n",
       "      <td>907.518233</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>2.083926</td>\n",
       "      <td>-0.225465</td>\n",
       "      <td>0.997740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5943</th>\n",
       "      <td>99</td>\n",
       "      <td>23</td>\n",
       "      <td>925.729803</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>0.236227</td>\n",
       "      <td>-0.785335</td>\n",
       "      <td>0.387011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5944</th>\n",
       "      <td>99</td>\n",
       "      <td>24</td>\n",
       "      <td>653.783183</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.394127</td>\n",
       "      <td>2.779995</td>\n",
       "      <td>1.372170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945</th>\n",
       "      <td>99</td>\n",
       "      <td>25</td>\n",
       "      <td>471.488142</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>-0.305295</td>\n",
       "      <td>1.674047</td>\n",
       "      <td>0.551679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5946</th>\n",
       "      <td>99</td>\n",
       "      <td>26</td>\n",
       "      <td>998.902711</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>-0.018837</td>\n",
       "      <td>-0.224247</td>\n",
       "      <td>1.752287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>99</td>\n",
       "      <td>28</td>\n",
       "      <td>795.676216</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>0.701131</td>\n",
       "      <td>0.576746</td>\n",
       "      <td>1.189223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>99</td>\n",
       "      <td>29</td>\n",
       "      <td>678.943523</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>1.579951</td>\n",
       "      <td>1.470439</td>\n",
       "      <td>1.492885</td>\n",
       "      <td>1.499343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5949 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id       price        v1        v2        v3        v4  \\\n",
       "0          59        0  307.521587  1.359401 -0.047853  1.508785  0.377067   \n",
       "1          59        1  968.823563  1.359401 -0.047853  0.808408 -0.452218   \n",
       "2          59        2  826.061991  1.359401 -0.047853  1.542081  0.462825   \n",
       "3          59        3  467.110221  1.359401 -0.047853  0.361892  0.311249   \n",
       "4          59        4  185.971676  1.359401 -0.047853  2.055851  1.647847   \n",
       "5          59        5  209.336191  1.359401 -0.047853  1.076881  1.802337   \n",
       "6          59        6  921.811589  1.359401 -0.047853  0.753801 -0.223070   \n",
       "7          59        7  405.733607  1.359401 -0.047853  1.535002  0.826051   \n",
       "8          59        8   32.439328  1.359401 -0.047853  1.274251  0.077147   \n",
       "9          59        9  279.119184  1.359401 -0.047853  0.571544  2.196910   \n",
       "10         59       10  555.558416  1.359401 -0.047853  1.583433  0.144717   \n",
       "11         59       11  290.976585  1.359401 -0.047853 -0.055865  1.775633   \n",
       "12         59       12  327.381251  1.359401 -0.047853  2.043326  1.548533   \n",
       "13         59       13  215.535596  1.359401 -0.047853  0.300978  0.652537   \n",
       "14         59       14  963.796932  1.359401 -0.047853  1.040349  0.383342   \n",
       "15         59       15  618.798931  1.359401 -0.047853  1.756298  0.562498   \n",
       "16         59       16  172.645709  1.359401 -0.047853  1.285766  2.293980   \n",
       "17         59       17  565.082253  1.359401 -0.047853  2.674176  0.532584   \n",
       "18         59       18  913.213161  1.359401 -0.047853  0.438503 -0.351983   \n",
       "19         59       19  340.260286  1.359401 -0.047853 -0.404461  1.569953   \n",
       "20         59       20  808.822307  1.359401 -0.047853  0.865467  2.306501   \n",
       "21         59       21  873.773138  1.359401 -0.047853  0.852525  1.822191   \n",
       "22         59       22  907.518233  1.359401 -0.047853  2.083926 -0.225465   \n",
       "23         59       23  925.729803  1.359401 -0.047853  0.236227 -0.785335   \n",
       "24         59       24  653.783183  1.359401 -0.047853  1.394127  2.779995   \n",
       "25         59       25  471.488142  1.359401 -0.047853 -0.305295  1.674047   \n",
       "26         59       26  998.902711  1.359401 -0.047853 -0.018837 -0.224247   \n",
       "27         59       27  951.980184  1.359401 -0.047853  1.207486  2.243383   \n",
       "28         59       28  795.676216  1.359401 -0.047853  0.701131  0.576746   \n",
       "29         59       29  678.943523  1.359401 -0.047853  1.470439  1.492885   \n",
       "...       ...      ...         ...       ...       ...       ...       ...   \n",
       "5919       29       28  795.676216  1.822076  1.876768  0.701131  0.576746   \n",
       "5920       29       29  678.943523  1.822076  1.876768  1.470439  1.492885   \n",
       "5921       99        0  307.521587  0.033482  1.579951  1.508785  0.377067   \n",
       "5922       99        1  968.823563  0.033482  1.579951  0.808408 -0.452218   \n",
       "5923       99        2  826.061991  0.033482  1.579951  1.542081  0.462825   \n",
       "5924       99        3  467.110221  0.033482  1.579951  0.361892  0.311249   \n",
       "5925       99        4  185.971676  0.033482  1.579951  2.055851  1.647847   \n",
       "5926       99        5  209.336191  0.033482  1.579951  1.076881  1.802337   \n",
       "5927       99        6  921.811589  0.033482  1.579951  0.753801 -0.223070   \n",
       "5928       99        7  405.733607  0.033482  1.579951  1.535002  0.826051   \n",
       "5929       99        8   32.439328  0.033482  1.579951  1.274251  0.077147   \n",
       "5930       99        9  279.119184  0.033482  1.579951  0.571544  2.196910   \n",
       "5931       99       10  555.558416  0.033482  1.579951  1.583433  0.144717   \n",
       "5932       99       12  327.381251  0.033482  1.579951  2.043326  1.548533   \n",
       "5933       99       13  215.535596  0.033482  1.579951  0.300978  0.652537   \n",
       "5934       99       14  963.796932  0.033482  1.579951  1.040349  0.383342   \n",
       "5935       99       15  618.798931  0.033482  1.579951  1.756298  0.562498   \n",
       "5936       99       16  172.645709  0.033482  1.579951  1.285766  2.293980   \n",
       "5937       99       17  565.082253  0.033482  1.579951  2.674176  0.532584   \n",
       "5938       99       18  913.213161  0.033482  1.579951  0.438503 -0.351983   \n",
       "5939       99       19  340.260286  0.033482  1.579951 -0.404461  1.569953   \n",
       "5940       99       20  808.822307  0.033482  1.579951  0.865467  2.306501   \n",
       "5941       99       21  873.773138  0.033482  1.579951  0.852525  1.822191   \n",
       "5942       99       22  907.518233  0.033482  1.579951  2.083926 -0.225465   \n",
       "5943       99       23  925.729803  0.033482  1.579951  0.236227 -0.785335   \n",
       "5944       99       24  653.783183  0.033482  1.579951  1.394127  2.779995   \n",
       "5945       99       25  471.488142  0.033482  1.579951 -0.305295  1.674047   \n",
       "5946       99       26  998.902711  0.033482  1.579951 -0.018837 -0.224247   \n",
       "5947       99       28  795.676216  0.033482  1.579951  0.701131  0.576746   \n",
       "5948       99       29  678.943523  0.033482  1.579951  1.470439  1.492885   \n",
       "\n",
       "            v5  \n",
       "0     1.097383  \n",
       "1     1.624187  \n",
       "2     1.233995  \n",
       "3     0.442961  \n",
       "4     0.399498  \n",
       "5     0.890867  \n",
       "6     0.001840  \n",
       "7     2.544656  \n",
       "8    -0.650352  \n",
       "9     1.475017  \n",
       "10    1.371365  \n",
       "11    1.316965  \n",
       "12    0.386704  \n",
       "13    1.427994  \n",
       "14    0.245015  \n",
       "15    0.644122  \n",
       "16   -0.104596  \n",
       "17    0.813754  \n",
       "18   -0.025743  \n",
       "19    1.252886  \n",
       "20    0.276289  \n",
       "21    1.541361  \n",
       "22    2.509941  \n",
       "23    0.810007  \n",
       "24    1.553988  \n",
       "25    1.258930  \n",
       "26   -0.839753  \n",
       "27   -0.049420  \n",
       "28    0.830703  \n",
       "29    3.523529  \n",
       "...        ...  \n",
       "5919  1.523260  \n",
       "5920 -0.732746  \n",
       "5921  2.255857  \n",
       "5922  0.459862  \n",
       "5923  2.889117  \n",
       "5924  0.106354  \n",
       "5925  1.967072  \n",
       "5926  0.270491  \n",
       "5927  0.246986  \n",
       "5928  2.740238  \n",
       "5929  0.398225  \n",
       "5930  0.240248  \n",
       "5931  1.107337  \n",
       "5932 -1.166868  \n",
       "5933  0.079760  \n",
       "5934 -1.432364  \n",
       "5935  0.844611  \n",
       "5936  1.028488  \n",
       "5937  1.667871  \n",
       "5938  0.270501  \n",
       "5939  0.607123  \n",
       "5940  0.589030  \n",
       "5941  1.759993  \n",
       "5942  0.997740  \n",
       "5943  0.387011  \n",
       "5944  1.372170  \n",
       "5945  0.551679  \n",
       "5946  1.752287  \n",
       "5947  1.189223  \n",
       "5948  1.499343  \n",
       "\n",
       "[5949 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_items_df = []\n",
    "for i in range(df.shape[0]):\n",
    "    users_items_df += df['state_history'][i] \n",
    "    \n",
    "users_items_df = pd.DataFrame(users_items_df,columns = ['user_id','item_id',\n",
    "                                                        'price','v1','v2','v3','v4','v5'])\n",
    "users_items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the first two covariates variables are features about users \n",
    "# and the forth and fifth variable are features about items\n",
    "# The variable 5 depends on .....\n",
    "\n",
    "user_v1 = users_items_df['v1'].unique()\n",
    "user_v2 = users_items_df['v2'].unique()\n",
    "users_ids = users_items_df['user_id'].unique()\n",
    "users_content= pd.DataFrame({'user_id' : users_ids,'v1':user_v1,'v2':user_v2 })\n",
    "\n",
    "item_v3 = users_items_df['v3'].unique()\n",
    "item_v4 = users_items_df['v4'].unique()\n",
    "item_id = users_items_df['item_id'].unique()\n",
    "items_content= pd.DataFrame({'item_id' : item_id,'v3':item_v3,'v4':item_v4 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df[df[\"rewards_history\"] > 0].reset_index(drop=True)\n",
    "neg_df = df[df[\"rewards_history\"] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>290.976585</td>\n",
       "      <td>[[22, 0, 307.52158737176507, 0.380930085635196...</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>678.943523</td>\n",
       "      <td>[[91, 0, 307.52158737176507, 1.883101997404765...</td>\n",
       "      <td>91</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>172.645709</td>\n",
       "      <td>[[85, 0, 307.52158737176507, 2.657104637190327...</td>\n",
       "      <td>85</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>951.980184</td>\n",
       "      <td>[[99, 0, 307.52158737176507, 0.033482344477483...</td>\n",
       "      <td>99</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>618.798931</td>\n",
       "      <td>[[92, 0, 307.52158737176507, 2.475957343224402...</td>\n",
       "      <td>92</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0              11       290.976585   \n",
       "1              29       678.943523   \n",
       "2              16       172.645709   \n",
       "3              27       951.980184   \n",
       "4              15       618.798931   \n",
       "\n",
       "                                       state_history  user_id  item_id_action  \n",
       "0  [[22, 0, 307.52158737176507, 0.380930085635196...       22              11  \n",
       "1  [[91, 0, 307.52158737176507, 1.883101997404765...       91              29  \n",
       "2  [[85, 0, 307.52158737176507, 2.657104637190327...       85              16  \n",
       "3  [[99, 0, 307.52158737176507, 0.033482344477483...       99              27  \n",
       "4  [[92, 0, 307.52158737176507, 2.475957343224402...       92              15  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df[\"user_id\"] = [pos_df[\"state_history\"][i][0][0] for i in range(pos_df.shape[0])]\n",
    "pos_df[\"item_id_action\"] = [pos_df[\"state_history\"][i][pos_df[\"action_history\"][i]][1] for i in range(pos_df.shape[0])]\n",
    "pos_df.head()## There are some users that have more than one positive item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[59, 0, 307.52158737176507, 1.359401132544833...</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[57, 0, 307.52158737176507, 0.889921827740519...</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[25, 0, 307.52158737176507, 0.749577322065220...</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[11, 0, 307.52158737176507, 0.024178763006033...</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[28, 0, 307.52158737176507, 0.878714795386164...</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0              10              0.0   \n",
       "1               3              0.0   \n",
       "2              12              0.0   \n",
       "3              29              0.0   \n",
       "4               6              0.0   \n",
       "\n",
       "                                       state_history  user_id  item_id_action  \n",
       "0  [[59, 0, 307.52158737176507, 1.359401132544833...       59              10  \n",
       "1  [[57, 0, 307.52158737176507, 0.889921827740519...       57               3  \n",
       "2  [[25, 0, 307.52158737176507, 0.749577322065220...       25              12  \n",
       "3  [[11, 0, 307.52158737176507, 0.024178763006033...       11              29  \n",
       "4  [[28, 0, 307.52158737176507, 0.878714795386164...       28               6  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df[\"user_id\"] = [neg_df[\"state_history\"][i][0][0] for i in range(neg_df.shape[0])]\n",
    "neg_df[\"item_id_action\"] = [neg_df[\"state_history\"][i][neg_df[\"action_history\"][i]][1] for i in range(neg_df.shape[0])]\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"Ignore y_true and return the mean of y_pred\n",
    "    \n",
    "    This is a hack to work-around the design of the Keras API that is\n",
    "    not really suited to train networks with a triplet loss by default.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(y_pred + 0 * y_true)\n",
    "\n",
    "\n",
    "def margin_comparator_loss(inputs, margin=1.):\n",
    "    \"\"\"Comparator loss for a pair of precomputed similarities\n",
    "    \n",
    "    If the inputs are cosine similarities, they each have range in\n",
    "    (-1, 1), therefore their difference have range in (-2, 2). Using\n",
    "    a margin of 1. can therefore make sense.\n",
    "\n",
    "    If the input similarities are not normalized, it can be beneficial\n",
    "    to use larger values for the margin of the comparator loss.\n",
    "    \"\"\"\n",
    "    positive_pair_sim, negative_pair_sim = inputs\n",
    "    return tf.maximum(negative_pair_sim - positive_pair_sim + margin, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model(The most expensive item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model_expensive(state):\n",
    "    greater_index = 0\n",
    "    greater_value = 0\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if(state[i][2] > greater_value):\n",
    "            greater_value = state[i][2]\n",
    "            greater_index = i\n",
    "    return greater_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 61440.648459 and Total reward per run: 20480.216153\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 100\n",
    "total_reward = 0\n",
    "\n",
    "for k in range(number_run):\n",
    "    \n",
    "    for i in range(generations):\n",
    "\n",
    "        predicted_item = naive_model_expensive(next_state)\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        \n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model(The cheapest item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model_cheapest(state):\n",
    "    cheapest_index = 0\n",
    "    cheapest_value =  state[0][2]\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if(state[i][2] < cheapest_value):\n",
    "            cheapest_value = state[i][2]\n",
    "            cheapest_index = i\n",
    "            \n",
    "    return cheapest_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 6996.026593 and Total reward per run: 2332.008864\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 100\n",
    "total_reward = 0\n",
    "\n",
    "for k in range(number_run):\n",
    "    \n",
    "    for i in range(generations):\n",
    "\n",
    "        predicted_item = naive_model_cheapest(next_state)\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        \n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, Dense\n",
    "from keras.layers import Lambda, Dot\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import dot, concatenate\n",
    "\n",
    "\n",
    "def build_models(n_users, n_items, latent_dim=64, l2_reg=0):\n",
    "    \"\"\"Build a triplet model and its companion similarity model\n",
    "    \n",
    "    The triplet model is used to train the weights of the companion\n",
    "    similarity model. The triplet model takes 1 user, 1 positive item\n",
    "    (relative to the selected user) and one negative item and is\n",
    "    trained with comparator loss.\n",
    "    \n",
    "    The similarity model takes one user and one item as input and return\n",
    "    compatibility score (aka the match score).\n",
    "    \"\"\"\n",
    "    # Common architectural components for the two models:\n",
    "    # - symbolic input placeholders\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "\n",
    "    # - embeddings\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(input_dim=n_users + 1, output_dim=latent_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "    \n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(input_dim=n_items + 1, output_dim=latent_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # - similarity computation between embeddings\n",
    "    positive_similarity = Dot(name=\"positive_similarity\",\n",
    "                              axes=1, normalize=True)(\n",
    "        [user_embedding, positive_item_embedding])\n",
    "    negative_similarity = Dot(name=\"negative_similarity\",\n",
    "                              axes=1, normalize=True)(\n",
    "        [user_embedding, negative_item_embedding])\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss,\n",
    "                          name='comparator_loss',\n",
    "                          output_shape=(1,))([positive_similarity, negative_similarity])\n",
    "\n",
    "    triplet_model = Model(inputs=[user_input,\n",
    "                                  positive_item_input,\n",
    "                                  negative_item_input],\n",
    "                          outputs=triplet_loss)\n",
    "    \n",
    "    # The match-score model, only use at inference to rank items for a given\n",
    "    # model: the model weights are shared with the triplet_model therefore\n",
    "    # we do not need to train it and therefore we do not need to plug a loss\n",
    "    # and an optimizer.\n",
    "    match_model = Model(inputs=[user_input, positive_item_input],\n",
    "                        outputs=positive_similarity)\n",
    "    \n",
    "    return triplet_model, match_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_df,max_items):\n",
    "    \"\"\"\n",
    "    Sample negatives at random\n",
    "    \n",
    "    \"\"\"\n",
    "    range_itens = np.arange(max_items)\n",
    "    user_ids = pos_df['user_id'].values\n",
    "    pos_item_ids = pos_df['item_id_action'].values\n",
    "    \n",
    "    \n",
    "    user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "    neg_item_ids = np.array([])\n",
    "    for i in user_ids:\n",
    "        if i in set(user_pos_items.index):\n",
    "            number = np.random.choice([item for item in range_itens \n",
    "                                       if not item in user_pos_items[user_pos_items.index == i]])\n",
    "        else:\n",
    "            number = np.random.choice(range_itens,1)\n",
    "        neg_item_ids = np.append(neg_item_ids,number)\n",
    "    \n",
    "  \n",
    "    return [user_ids, pos_item_ids, neg_item_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 6ms/step - loss: 1.0889\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 68us/step - loss: 0.9766\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 69us/step - loss: 0.9177\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 68us/step - loss: 0.8900\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.8486\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 65us/step - loss: 0.8566\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 66us/step - loss: 0.7523\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.6709\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 67us/step - loss: 0.6408\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.6446\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 79us/step - loss: 0.6317\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 79us/step - loss: 0.5866\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.5769\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.5314\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 78us/step - loss: 0.5183\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 88us/step - loss: 0.5099\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.4754\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 93us/step - loss: 0.4347\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 68us/step - loss: 0.3353\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 389us/step - loss: 0.4032\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 69us/step - loss: 0.3341\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 73us/step - loss: 0.3553\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 143us/step - loss: 0.3453\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 81us/step - loss: 0.2985\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.2881\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.2424\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 79us/step - loss: 0.2795\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 74us/step - loss: 0.2109\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.2844\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 69us/step - loss: 0.2522\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5757\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0318\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0339\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5686\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6426\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7137\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5603\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0025\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5815\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4095\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6915e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9372\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2942\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9148\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5312\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5800\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0236\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5060\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5081\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2804\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5922\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7468\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8466\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3777\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4930\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 6ms/step - loss: 0.9399\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 88us/step - loss: 0.9395\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 115us/step - loss: 0.8776\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 76us/step - loss: 0.8711\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 78us/step - loss: 0.8407\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 81us/step - loss: 0.7979\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 133us/step - loss: 0.7364\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 68us/step - loss: 0.6420\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 82us/step - loss: 0.5837\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 74us/step - loss: 0.5594\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 71us/step - loss: 0.6031\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 175us/step - loss: 0.5171\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 152us/step - loss: 0.5129\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 74us/step - loss: 0.5310\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.4973\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 83us/step - loss: 0.4766\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 203us/step - loss: 0.4273\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 70us/step - loss: 0.3753\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 71us/step - loss: 0.3541\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.3072\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 91us/step - loss: 0.4223\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.3141\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 87us/step - loss: 0.3327\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 67us/step - loss: 0.2422\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.2954\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 81us/step - loss: 0.2724\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 65us/step - loss: 0.2958\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 75us/step - loss: 0.2708\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 64us/step - loss: 0.3210\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 65us/step - loss: 0.2579\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2388\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8180\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8146\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1222\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2586\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5492\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4142\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4904\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6088\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0564\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6800\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7866\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4562\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2433\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3646\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4063\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4841\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6112\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7718\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8197\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0775\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4993\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3183\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0096\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7794\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8777\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7264\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4474\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4295\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6268\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7255\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7452\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 8ms/step - loss: 1.0511\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 86us/step - loss: 0.9404\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 74us/step - loss: 0.8807\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 94us/step - loss: 0.7974\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 102us/step - loss: 0.8160\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 123us/step - loss: 0.7751\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 89us/step - loss: 0.7212\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 108us/step - loss: 0.7551\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.7168\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 90us/step - loss: 0.7112\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 106us/step - loss: 0.6040\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 99us/step - loss: 0.6178\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 97us/step - loss: 0.5761\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 103us/step - loss: 0.5086\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.5458\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 68us/step - loss: 0.4447\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 92us/step - loss: 0.4959\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 107us/step - loss: 0.4456\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 70us/step - loss: 0.3826\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.3654\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 66us/step - loss: 0.4066\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.3484\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 84us/step - loss: 0.3551\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 95us/step - loss: 0.3013\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 98us/step - loss: 0.3596\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 69us/step - loss: 0.2794\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 68us/step - loss: 0.2657\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 110us/step - loss: 0.3206\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 123us/step - loss: 0.3096\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 76us/step - loss: 0.2638\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2988\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3190\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6657\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5587\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6767\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4481\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3816\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8547\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3627\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7963\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3394\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0508\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6603\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8378\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1761\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8935\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0151\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0320\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5387\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6911\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5594\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6484e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0596\n",
      "Total reward: 46631.270122 and Total reward per run: 15543.756707\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "n_epochs = 30\n",
    "total_reward = 0\n",
    "generations = 100\n",
    "max_items = 30\n",
    "\n",
    "for k in range(number_run):\n",
    "\n",
    "    triplet_model, match_model = build_models(nb_users, nb_items, latent_dim=16, l2_reg=1e-6)\n",
    "    # we plug the identity loss and the a fake target variable ignored by\n",
    "    # the model to be able to use the Keras API to train the triplet model\n",
    "    triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "    fake_y = np.ones_like(pos_df['user_id'])\n",
    "    params = {'user_id': USER_ID}\n",
    "\n",
    "    positive_data = pos_df.copy()\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        # Sample new negatives to build different triplets at each epoch\n",
    "        triplet_inputs = sample_triplets(positive_data, max_items=nb_items)\n",
    "\n",
    "        # Fit the model incrementally by doing a single pass over the\n",
    "        # sampled triplets.\n",
    "        triplet_model.fit(triplet_inputs, fake_y, shuffle=True, epochs=1)\n",
    "\n",
    "    for j in range(generations):\n",
    "\n",
    "\n",
    "        # The user that we need to recommend a product\n",
    "        new_user_id = next_state[0][0]\n",
    "        # Take all the items in the new state\n",
    "        items_ids = np.array([next_state[i][1]\n",
    "                              for i in range(len(next_state))])\n",
    "\n",
    "        # Create an array of the same size of 'items_ids' of all values equal to 'new_user_id'\n",
    "        repeated_user_id = np.empty_like(items_ids)\n",
    "        repeated_user_id.fill(new_user_id)\n",
    "\n",
    "        # predict the product using our model\n",
    "        predicted = match_model.predict([repeated_user_id, items_ids])\n",
    "        predicted_item = np.argmax(predicted)\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        # take the reward from the API\n",
    "        r = requests.get(url=url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "\n",
    "        # (Online learning)Check if the reward is greater than 0 , if yes train again \n",
    "        # the model the sample point\n",
    "        if(reward > 0):\n",
    "            range_itens = np.arange(max_items)\n",
    "            user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "            if new_user_id in set(user_pos_items.index):\n",
    "                neg_item_id = np.random.choice([item for item in range_itens \n",
    "                                           if not item in user_pos_items[user_pos_items.index == new_user_id]])\n",
    "            else:\n",
    "                neg_item_id = np.random.choice(range_itens,1)\n",
    "            \n",
    "           \n",
    "            \n",
    "            triplet_inputs = [[new_user_id], [predicted_item], [neg_item_id]]\n",
    "    \n",
    "            fake_y = np.ones_like([1])\n",
    "            triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size=64, epochs=1)\n",
    "    \n",
    "            \n",
    "        # Take the nex_state\n",
    "        next_state = r['state']\n",
    "\n",
    "\n",
    "print(\"Total reward: %f and Total reward per run: %f\" %(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dense, Dropout\n",
    "from keras.layers import Concatenate, Lambda\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def make_interaction_mlp(input_dim, n_hidden=1, hidden_size=64,\n",
    "                         dropout=0, l2_reg=None):\n",
    "    \"\"\"\n",
    "    Build the shared multi layer perceptron\n",
    "\n",
    "    \"\"\"\n",
    "    mlp = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        # Plug the output unit directly: this is a simple\n",
    "        # linear regression model. Not dropout required.\n",
    "        mlp.add(Dense(1, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "    else:\n",
    "        mlp.add(Dense(hidden_size, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "        mlp.add(Dropout(dropout))\n",
    "        for i in range(n_hidden - 1):\n",
    "            mlp.add(Dense(hidden_size, activation='relu',\n",
    "                          kernel_regularizer=l2_reg))\n",
    "            mlp.add(Dropout(dropout))\n",
    "        mlp.add(Dense(1, activation='relu', kernel_regularizer=l2_reg))\n",
    "    return mlp\n",
    "\n",
    "\n",
    "def build_models(n_users, n_items, user_dim=32, item_dim=64,\n",
    "                 n_hidden=1, hidden_size=64, dropout=0, l2_reg=0):\n",
    "    \"\"\"\n",
    "    Build models to train a deep triplet network\n",
    "\n",
    "    \"\"\"\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "    positive_meta_input = Input((2,), name='positive_meta_input')\n",
    "    negative_meta_input = Input((2,), name='negative_meta_input')\n",
    "\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(n_users + 1, user_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "\n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(n_items + 1, item_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # Similarity computation between embeddings using a MLP similarity\n",
    "    positive_embeddings_pair = Concatenate(name=\"positive_embeddings_pair\")(\n",
    "        [user_embedding, positive_item_embedding, positive_meta_input])\n",
    "    positive_embeddings_pair = Dropout(dropout)(positive_embeddings_pair)\n",
    "\n",
    "    negative_embeddings_pair = Concatenate(name=\"negative_embeddings_pair\")(\n",
    "        [user_embedding, negative_item_embedding, negative_meta_input])\n",
    "    negative_embeddings_pair = Dropout(dropout)(negative_embeddings_pair)\n",
    "\n",
    "    # Instanciate the shared similarity architecture\n",
    "    interaction_layers = make_interaction_mlp(\n",
    "        user_dim + item_dim + 2, n_hidden=n_hidden, hidden_size=hidden_size,\n",
    "        dropout=dropout, l2_reg=l2_reg)\n",
    "\n",
    "    positive_similarity = interaction_layers(positive_embeddings_pair)\n",
    "    negative_similarity = interaction_layers(negative_embeddings_pair)\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss, output_shape=(1,),\n",
    "                          name='comparator_loss')(\n",
    "        [positive_similarity, negative_similarity])\n",
    "\n",
    "    deep_triplet_model = Model(inputs=[user_input,\n",
    "                                       positive_item_input,\n",
    "                                       negative_item_input,\n",
    "                                       positive_meta_input,\n",
    "                                       negative_meta_input\n",
    "                                       ],\n",
    "                               outputs=[triplet_loss])\n",
    "\n",
    "    # The match-score model, only used at inference\n",
    "    deep_match_model = Model(inputs=[user_input, positive_item_input, positive_meta_input],\n",
    "                             outputs=[positive_similarity])\n",
    "\n",
    "    return deep_match_model, deep_triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_df, items_content, max_items):\n",
    "    \"\"\"\n",
    "    Sample the data and put in the format \n",
    "    [user_ids, pos_item_ids, neg_item_ids,pos_item_metadata,neg_item_metadata]\n",
    "\n",
    "    \"\"\"\n",
    "    range_itens = np.arange(max_items)\n",
    "    user_ids = pos_df['user_id'].values\n",
    "    pos_item_ids = pos_df['item_id_action'].values\n",
    "\n",
    "    pos_item_metadata = items_content.loc[pos_item_ids][['v3', 'v4']].values\n",
    "\n",
    "    \n",
    "    user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "    neg_item_ids = np.array([])\n",
    "    for i in user_ids:\n",
    "        if i in set(user_pos_items.index):\n",
    "            number = np.random.choice([item for item in range_itens \n",
    "                                       if not item in user_pos_items[user_pos_items.index == i]])\n",
    "        else:\n",
    "            number = np.random.choice(range_itens, 1)\n",
    "        neg_item_ids = np.append(neg_item_ids, number)\n",
    "\n",
    "    neg_item_metadata = items_content.loc[neg_item_ids][['v3', 'v4']].values\n",
    "\n",
    "    return [user_ids, pos_item_ids, neg_item_ids, pos_item_metadata, neg_item_metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = dict(\n",
    "    user_dim=50,\n",
    "    item_dim=15,\n",
    "    n_hidden=3,\n",
    "    hidden_size=16,\n",
    "    dropout=0.5,\n",
    "    l2_reg=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "59/59 [==============================] - 1s 17ms/step - loss: 1.1842\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 50us/step - loss: 1.0033\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 50us/step - loss: 1.1872\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 53us/step - loss: 1.1167\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 1.0085\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 41us/step - loss: 0.9400\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 1.0576\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 45us/step - loss: 0.9642\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 45us/step - loss: 0.9233\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 46us/step - loss: 1.1527\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 55us/step - loss: 1.3998\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 0.9717\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 50us/step - loss: 1.0729\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 49us/step - loss: 1.0652\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 50us/step - loss: 0.9863\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 46us/step - loss: 1.1506\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 41us/step - loss: 1.0474\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 44us/step - loss: 1.2121\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 48us/step - loss: 0.9828\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 42us/step - loss: 0.9935\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 40us/step - loss: 1.0991\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 42us/step - loss: 1.1033\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 45us/step - loss: 1.2148\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 46us/step - loss: 1.0656\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 42us/step - loss: 0.9219\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 41us/step - loss: 1.2387\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 1.0292\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 0.9382\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 0.9543\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 42us/step - loss: 1.2212\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3361\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1723\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4862\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0265\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5671\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7226\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6603\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7520\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2928\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2260\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9913\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9652\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9438\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0167\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9496\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7313\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 1s 19ms/step - loss: 0.9805\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 1.0043\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 48us/step - loss: 1.0547\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 44us/step - loss: 0.9256\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 46us/step - loss: 0.9764\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 54us/step - loss: 1.1193\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 46us/step - loss: 0.9969\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 49us/step - loss: 1.0160\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 1.0952\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 57us/step - loss: 1.0041\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 1.0901\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 48us/step - loss: 1.0785\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 52us/step - loss: 0.9408\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 1.0215\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 1.0022\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 42us/step - loss: 1.0820\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 1.2472\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 44us/step - loss: 0.9748\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 0.8840\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 50us/step - loss: 1.0294\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 49us/step - loss: 1.0849\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 1.0617\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 0.9595\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 0.9792\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 50us/step - loss: 0.9941\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 62us/step - loss: 1.0273\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 46us/step - loss: 0.9916\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 44us/step - loss: 0.9002\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 48us/step - loss: 1.0540\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 49us/step - loss: 0.8963\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2054\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9720\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3518\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6827\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0762\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9256\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1729\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0476\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1487\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8002\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3038\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3542\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4768\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3703\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2567\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1844\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7097\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0431\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9139\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7882\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9458\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7084\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0414\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0235\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8759\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9316\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0819\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 1s 17ms/step - loss: 0.9985\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 56us/step - loss: 1.0147\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 51us/step - loss: 0.9787\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 51us/step - loss: 0.9703\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 1.0085\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 0.9976\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 48us/step - loss: 0.9778\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 44us/step - loss: 1.0082\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 48us/step - loss: 1.0132\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 41us/step - loss: 1.0221\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 52us/step - loss: 1.0254\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 46us/step - loss: 1.0317\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 50us/step - loss: 0.9984\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 48us/step - loss: 0.9983\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 41us/step - loss: 0.9742\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 46us/step - loss: 1.0060\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 0.9921\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 46us/step - loss: 1.0188\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 48us/step - loss: 0.9665\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 45us/step - loss: 1.0163\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 48us/step - loss: 1.0056\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 0.9753\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 56us/step - loss: 0.9930\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 48us/step - loss: 1.0166\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 45us/step - loss: 1.0118\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 41us/step - loss: 0.9821\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 43us/step - loss: 1.0071\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 44us/step - loss: 0.9870\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 1.0083\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 47us/step - loss: 1.0170\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9793\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7208\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0177\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9737\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9848\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9555\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6359\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0102\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9494\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9174\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0265\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1439\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9908\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9786\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9509\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0891\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0300\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8606\n",
      "Total reward: 57826.231287 and Total reward per run: 19275.410429\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "n_epochs = 30\n",
    "total_reward = 0\n",
    "generations = 100\n",
    "\n",
    "\n",
    "for k in range(number_run):\n",
    "\n",
    "    deep_match_model, deep_triplet_model = build_models(nb_users, nb_items,**hyper_parameters)\n",
    "    # we plug the identity loss and the a fake target variable ignored by\n",
    "    # the model to be able to use the Keras API to train the triplet model\n",
    "    deep_triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "    fake_y = np.ones_like(pos_df['user_id'])\n",
    "    params = {'user_id': USER_ID}\n",
    "    \n",
    "    positive_data = pos_df.copy()\n",
    "    \n",
    "    # Training the model \n",
    "    for i in range(n_epochs):\n",
    "        # Sample new negatives to build different triplets at each epoch\n",
    "        triplet_inputs = sample_triplets(\n",
    "            positive_data, items_content, max_items=nb_items)\n",
    "\n",
    "        # Fit the model incrementally by doing a single pass over the\n",
    "        # sampled triplets.\n",
    "        deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size=64, epochs=1)\n",
    "\n",
    "\n",
    "    for j in range(generations):\n",
    "        \n",
    "        # Predicting\n",
    "        new_user_id = next_state[0][0]\n",
    "        items_ids = np.array([next_state[i][1]\n",
    "                              for i in range(len(next_state))])\n",
    "        repeated_user_id = np.empty_like(items_ids)\n",
    "        repeated_user_id.fill(new_user_id)\n",
    "\n",
    "        pos_metadata = items_content.loc[items_ids][['v3', 'v4']].values\n",
    "        predicted = deep_match_model.predict(\n",
    "            [repeated_user_id, items_ids, pos_metadata])\n",
    "        predicted_item = np.argmax(predicted)\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url=url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "\n",
    "        # Online learning\n",
    "        if(reward > 0):\n",
    "            range_itens = np.arange(max_items)\n",
    "            user_pos_items = positive_data.groupby('user_id')['item_id_action'].apply(list)\n",
    "            if new_user_id in set(user_pos_items.index):\n",
    "                neg_item_id = np.random.choice([item for item in range_itens \n",
    "                                           if not item in user_pos_items[user_pos_items.index == new_user_id]])\n",
    "            else:\n",
    "                neg_item_id = np.random.choice(range_itens,1)\n",
    "            \n",
    "            pos_item_metadata = items_content.set_index('item_id').loc[predicted_item].values\n",
    "            neg_item_metadata = items_content.set_index('item_id').loc[neg_item_id].values\n",
    "            \n",
    "            triplet_inputs = [[new_user_id], [predicted_item], [neg_item_id], \n",
    "                              pos_item_metadata.reshape((1,2)), neg_item_metadata.reshape((1,2))]\n",
    "    \n",
    "            fake_y = np.ones_like([1])\n",
    "            deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size = 8, epochs=1)\n",
    "\n",
    "        next_state = r['state']\n",
    "\n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Concatenate, Dropout, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third environmement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['action_history', 'nb_items', 'nb_users', 'next_state', 'rewards_history', 'state_history'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attributes fa\n",
    "USER_ID = 'IAHIZPIW80WPGDW7P7JE'\n",
    "BASE_URL = \"http://35.180.178.243\"\n",
    "url_reset = BASE_URL + \"/reset\"\n",
    "url_predict = BASE_URL + \"/predict\"\n",
    "params = {\"user_id\" : USER_ID}\n",
    "\n",
    "def load_data(url_reset,params) :\n",
    "    r = requests.get(url=url_reset, params=params)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "data = load_data(url_reset,params)\n",
    "nb_users = data[\"nb_users\"]\n",
    "nb_items = data[\"nb_items\"]\n",
    "next_state = data[\"next_state\"]\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #See the request predict from our API\n",
    "# params = {\"user_id\" : user_id,\"recommended_item\": 2}\n",
    "# requests.get(url=url_predict, params=params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[96, 0, 298.8684368616429, 3.551004894370762,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[73, 0, 298.8684368616429, -0.257449587762428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>220.500578</td>\n",
       "      <td>[[88, 0, 298.8684368616429, 1.2703544096780341...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>342.696272</td>\n",
       "      <td>[[58, 0, 298.8684368616429, 1.462631311727477,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[20, 0, 298.8684368616429, -1.657406472788035...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0               9         0.000000   \n",
       "1              13         0.000000   \n",
       "2              19       220.500578   \n",
       "3               8       342.696272   \n",
       "4              26         0.000000   \n",
       "\n",
       "                                       state_history  \n",
       "0  [[96, 0, 298.8684368616429, 3.551004894370762,...  \n",
       "1  [[73, 0, 298.8684368616429, -0.257449587762428...  \n",
       "2  [[88, 0, 298.8684368616429, 1.2703544096780341...  \n",
       "3  [[58, 0, 298.8684368616429, 1.462631311727477,...  \n",
       "4  [[20, 0, 298.8684368616429, -1.657406472788035...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklisted_set = set((\"next_state\",\"nb_items\",\"nb_users\"))\n",
    "new_dict = {key : value for key, value in data.items() if key not in blacklisted_set}\n",
    "\n",
    "df = pd.DataFrame(new_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.455000</td>\n",
       "      <td>160.537738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.515901</td>\n",
       "      <td>277.378168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.250000</td>\n",
       "      <td>241.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>962.503193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       action_history  rewards_history\n",
       "count      200.000000       200.000000\n",
       "mean        14.455000       160.537738\n",
       "std          8.515901       277.378168\n",
       "min          0.000000         0.000000\n",
       "25%          8.000000         0.000000\n",
       "50%         14.500000         0.000000\n",
       "75%         21.250000       241.001800\n",
       "max         29.000000       962.503193"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action_history     136\n",
       "rewards_history    136\n",
       "state_history      136\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['rewards_history']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>298.868437</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.022067</td>\n",
       "      <td>0.789290</td>\n",
       "      <td>2.283803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>144.752869</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.367082</td>\n",
       "      <td>1.489990</td>\n",
       "      <td>0.800584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>53.112327</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>2.501812</td>\n",
       "      <td>0.683460</td>\n",
       "      <td>0.964481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>402.226952</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.528292</td>\n",
       "      <td>3.519404</td>\n",
       "      <td>-0.338393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "      <td>667.895336</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>0.222637</td>\n",
       "      <td>1.493626</td>\n",
       "      <td>2.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>96</td>\n",
       "      <td>5</td>\n",
       "      <td>920.193097</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>-1.305086</td>\n",
       "      <td>0.821526</td>\n",
       "      <td>2.322430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96</td>\n",
       "      <td>6</td>\n",
       "      <td>590.271714</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>0.164866</td>\n",
       "      <td>0.742618</td>\n",
       "      <td>1.160948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>96</td>\n",
       "      <td>7</td>\n",
       "      <td>288.745635</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>-0.827556</td>\n",
       "      <td>0.837463</td>\n",
       "      <td>0.867349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>96</td>\n",
       "      <td>8</td>\n",
       "      <td>342.696272</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>0.095627</td>\n",
       "      <td>1.802618</td>\n",
       "      <td>-0.096715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>96</td>\n",
       "      <td>9</td>\n",
       "      <td>962.503193</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.877117</td>\n",
       "      <td>0.654706</td>\n",
       "      <td>-0.016004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>96</td>\n",
       "      <td>10</td>\n",
       "      <td>213.526369</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.217421</td>\n",
       "      <td>0.839702</td>\n",
       "      <td>0.054537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>96</td>\n",
       "      <td>11</td>\n",
       "      <td>940.113056</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.068888</td>\n",
       "      <td>-0.062645</td>\n",
       "      <td>-0.097379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>96</td>\n",
       "      <td>12</td>\n",
       "      <td>623.398679</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>0.530291</td>\n",
       "      <td>1.882382</td>\n",
       "      <td>0.849372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>96</td>\n",
       "      <td>13</td>\n",
       "      <td>363.030532</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>-0.638493</td>\n",
       "      <td>1.002502</td>\n",
       "      <td>1.291779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>96</td>\n",
       "      <td>14</td>\n",
       "      <td>836.029816</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>0.785146</td>\n",
       "      <td>1.608671</td>\n",
       "      <td>3.111055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>96</td>\n",
       "      <td>15</td>\n",
       "      <td>327.243675</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>-0.771600</td>\n",
       "      <td>0.428693</td>\n",
       "      <td>0.425127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>96</td>\n",
       "      <td>16</td>\n",
       "      <td>831.250068</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>2.454080</td>\n",
       "      <td>1.387184</td>\n",
       "      <td>1.247214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>96</td>\n",
       "      <td>17</td>\n",
       "      <td>241.001800</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.578955</td>\n",
       "      <td>2.225853</td>\n",
       "      <td>0.580070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>96</td>\n",
       "      <td>18</td>\n",
       "      <td>899.026448</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.035393</td>\n",
       "      <td>2.682739</td>\n",
       "      <td>-0.352419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "      <td>220.500578</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>2.656373</td>\n",
       "      <td>0.508312</td>\n",
       "      <td>1.627302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>96</td>\n",
       "      <td>20</td>\n",
       "      <td>172.396413</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.700099</td>\n",
       "      <td>-0.165836</td>\n",
       "      <td>1.357936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>96</td>\n",
       "      <td>21</td>\n",
       "      <td>563.457385</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.242704</td>\n",
       "      <td>2.654412</td>\n",
       "      <td>1.447835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>96</td>\n",
       "      <td>22</td>\n",
       "      <td>427.248903</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.463341</td>\n",
       "      <td>2.392778</td>\n",
       "      <td>1.093729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>96</td>\n",
       "      <td>23</td>\n",
       "      <td>913.848739</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>2.486287</td>\n",
       "      <td>1.671075</td>\n",
       "      <td>-0.200473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>96</td>\n",
       "      <td>24</td>\n",
       "      <td>289.641141</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>0.159176</td>\n",
       "      <td>1.518408</td>\n",
       "      <td>1.843948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>96</td>\n",
       "      <td>25</td>\n",
       "      <td>727.824889</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>-0.165833</td>\n",
       "      <td>2.021509</td>\n",
       "      <td>2.064701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>96</td>\n",
       "      <td>26</td>\n",
       "      <td>324.201031</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>-0.183592</td>\n",
       "      <td>0.434537</td>\n",
       "      <td>1.624292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>96</td>\n",
       "      <td>27</td>\n",
       "      <td>756.180664</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>0.652139</td>\n",
       "      <td>1.485638</td>\n",
       "      <td>0.974022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "      <td>844.153018</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>0.730094</td>\n",
       "      <td>0.170283</td>\n",
       "      <td>-0.270327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>96</td>\n",
       "      <td>29</td>\n",
       "      <td>415.061864</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.144372</td>\n",
       "      <td>1.979305</td>\n",
       "      <td>1.817345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5920</th>\n",
       "      <td>96</td>\n",
       "      <td>29</td>\n",
       "      <td>415.061864</td>\n",
       "      <td>3.551005</td>\n",
       "      <td>1.676545</td>\n",
       "      <td>1.144372</td>\n",
       "      <td>1.979305</td>\n",
       "      <td>-0.975903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5921</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>298.868437</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.022067</td>\n",
       "      <td>0.789290</td>\n",
       "      <td>1.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>144.752869</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.367082</td>\n",
       "      <td>1.489990</td>\n",
       "      <td>0.088917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>53.112327</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>2.501812</td>\n",
       "      <td>0.683460</td>\n",
       "      <td>1.677883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>402.226952</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.528292</td>\n",
       "      <td>3.519404</td>\n",
       "      <td>1.864099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925</th>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>667.895336</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>0.222637</td>\n",
       "      <td>1.493626</td>\n",
       "      <td>-0.523196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5926</th>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>920.193097</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>-1.305086</td>\n",
       "      <td>0.821526</td>\n",
       "      <td>3.190324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5927</th>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "      <td>590.271714</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>0.164866</td>\n",
       "      <td>0.742618</td>\n",
       "      <td>0.076249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5928</th>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>288.745635</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>-0.827556</td>\n",
       "      <td>0.837463</td>\n",
       "      <td>1.883262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5929</th>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>342.696272</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>0.095627</td>\n",
       "      <td>1.802618</td>\n",
       "      <td>0.451896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5930</th>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>962.503193</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.877117</td>\n",
       "      <td>0.654706</td>\n",
       "      <td>1.595328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>213.526369</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.217421</td>\n",
       "      <td>0.839702</td>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>940.113056</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.068888</td>\n",
       "      <td>-0.062645</td>\n",
       "      <td>1.895527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "      <td>623.398679</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>0.530291</td>\n",
       "      <td>1.882382</td>\n",
       "      <td>-0.464103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5934</th>\n",
       "      <td>63</td>\n",
       "      <td>13</td>\n",
       "      <td>363.030532</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>-0.638493</td>\n",
       "      <td>1.002502</td>\n",
       "      <td>-0.352402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>63</td>\n",
       "      <td>14</td>\n",
       "      <td>836.029816</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>0.785146</td>\n",
       "      <td>1.608671</td>\n",
       "      <td>1.276739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>63</td>\n",
       "      <td>15</td>\n",
       "      <td>327.243675</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>-0.771600</td>\n",
       "      <td>0.428693</td>\n",
       "      <td>2.736862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>831.250068</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>2.454080</td>\n",
       "      <td>1.387184</td>\n",
       "      <td>0.735048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>63</td>\n",
       "      <td>17</td>\n",
       "      <td>241.001800</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.578955</td>\n",
       "      <td>2.225853</td>\n",
       "      <td>1.339580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>63</td>\n",
       "      <td>19</td>\n",
       "      <td>220.500578</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>2.656373</td>\n",
       "      <td>0.508312</td>\n",
       "      <td>1.295611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5940</th>\n",
       "      <td>63</td>\n",
       "      <td>20</td>\n",
       "      <td>172.396413</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.700099</td>\n",
       "      <td>-0.165836</td>\n",
       "      <td>0.741737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>63</td>\n",
       "      <td>21</td>\n",
       "      <td>563.457385</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.242704</td>\n",
       "      <td>2.654412</td>\n",
       "      <td>2.157774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5942</th>\n",
       "      <td>63</td>\n",
       "      <td>22</td>\n",
       "      <td>427.248903</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.463341</td>\n",
       "      <td>2.392778</td>\n",
       "      <td>0.026324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5943</th>\n",
       "      <td>63</td>\n",
       "      <td>23</td>\n",
       "      <td>913.848739</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>2.486287</td>\n",
       "      <td>1.671075</td>\n",
       "      <td>1.998591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5944</th>\n",
       "      <td>63</td>\n",
       "      <td>24</td>\n",
       "      <td>289.641141</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>0.159176</td>\n",
       "      <td>1.518408</td>\n",
       "      <td>1.616380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945</th>\n",
       "      <td>63</td>\n",
       "      <td>25</td>\n",
       "      <td>727.824889</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>-0.165833</td>\n",
       "      <td>2.021509</td>\n",
       "      <td>2.040117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5946</th>\n",
       "      <td>63</td>\n",
       "      <td>26</td>\n",
       "      <td>324.201031</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>-0.183592</td>\n",
       "      <td>0.434537</td>\n",
       "      <td>0.948843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>63</td>\n",
       "      <td>27</td>\n",
       "      <td>756.180664</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>0.652139</td>\n",
       "      <td>1.485638</td>\n",
       "      <td>2.294981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>63</td>\n",
       "      <td>28</td>\n",
       "      <td>844.153018</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>0.730094</td>\n",
       "      <td>0.170283</td>\n",
       "      <td>1.343175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>63</td>\n",
       "      <td>29</td>\n",
       "      <td>415.061864</td>\n",
       "      <td>-1.256976</td>\n",
       "      <td>1.010199</td>\n",
       "      <td>1.144372</td>\n",
       "      <td>1.979305</td>\n",
       "      <td>1.993857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5950 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id       price        v1        v2        v3        v4  \\\n",
       "0          96        0  298.868437  3.551005  1.676545  1.022067  0.789290   \n",
       "1          96        1  144.752869  3.551005  1.676545  1.367082  1.489990   \n",
       "2          96        2   53.112327  3.551005  1.676545  2.501812  0.683460   \n",
       "3          96        3  402.226952  3.551005  1.676545  1.528292  3.519404   \n",
       "4          96        4  667.895336  3.551005  1.676545  0.222637  1.493626   \n",
       "5          96        5  920.193097  3.551005  1.676545 -1.305086  0.821526   \n",
       "6          96        6  590.271714  3.551005  1.676545  0.164866  0.742618   \n",
       "7          96        7  288.745635  3.551005  1.676545 -0.827556  0.837463   \n",
       "8          96        8  342.696272  3.551005  1.676545  0.095627  1.802618   \n",
       "9          96        9  962.503193  3.551005  1.676545  1.877117  0.654706   \n",
       "10         96       10  213.526369  3.551005  1.676545  1.217421  0.839702   \n",
       "11         96       11  940.113056  3.551005  1.676545  1.068888 -0.062645   \n",
       "12         96       12  623.398679  3.551005  1.676545  0.530291  1.882382   \n",
       "13         96       13  363.030532  3.551005  1.676545 -0.638493  1.002502   \n",
       "14         96       14  836.029816  3.551005  1.676545  0.785146  1.608671   \n",
       "15         96       15  327.243675  3.551005  1.676545 -0.771600  0.428693   \n",
       "16         96       16  831.250068  3.551005  1.676545  2.454080  1.387184   \n",
       "17         96       17  241.001800  3.551005  1.676545  1.578955  2.225853   \n",
       "18         96       18  899.026448  3.551005  1.676545  1.035393  2.682739   \n",
       "19         96       19  220.500578  3.551005  1.676545  2.656373  0.508312   \n",
       "20         96       20  172.396413  3.551005  1.676545  1.700099 -0.165836   \n",
       "21         96       21  563.457385  3.551005  1.676545  1.242704  2.654412   \n",
       "22         96       22  427.248903  3.551005  1.676545  1.463341  2.392778   \n",
       "23         96       23  913.848739  3.551005  1.676545  2.486287  1.671075   \n",
       "24         96       24  289.641141  3.551005  1.676545  0.159176  1.518408   \n",
       "25         96       25  727.824889  3.551005  1.676545 -0.165833  2.021509   \n",
       "26         96       26  324.201031  3.551005  1.676545 -0.183592  0.434537   \n",
       "27         96       27  756.180664  3.551005  1.676545  0.652139  1.485638   \n",
       "28         96       28  844.153018  3.551005  1.676545  0.730094  0.170283   \n",
       "29         96       29  415.061864  3.551005  1.676545  1.144372  1.979305   \n",
       "...       ...      ...         ...       ...       ...       ...       ...   \n",
       "5920       96       29  415.061864  3.551005  1.676545  1.144372  1.979305   \n",
       "5921       63        0  298.868437 -1.256976  1.010199  1.022067  0.789290   \n",
       "5922       63        1  144.752869 -1.256976  1.010199  1.367082  1.489990   \n",
       "5923       63        2   53.112327 -1.256976  1.010199  2.501812  0.683460   \n",
       "5924       63        3  402.226952 -1.256976  1.010199  1.528292  3.519404   \n",
       "5925       63        4  667.895336 -1.256976  1.010199  0.222637  1.493626   \n",
       "5926       63        5  920.193097 -1.256976  1.010199 -1.305086  0.821526   \n",
       "5927       63        6  590.271714 -1.256976  1.010199  0.164866  0.742618   \n",
       "5928       63        7  288.745635 -1.256976  1.010199 -0.827556  0.837463   \n",
       "5929       63        8  342.696272 -1.256976  1.010199  0.095627  1.802618   \n",
       "5930       63        9  962.503193 -1.256976  1.010199  1.877117  0.654706   \n",
       "5931       63       10  213.526369 -1.256976  1.010199  1.217421  0.839702   \n",
       "5932       63       11  940.113056 -1.256976  1.010199  1.068888 -0.062645   \n",
       "5933       63       12  623.398679 -1.256976  1.010199  0.530291  1.882382   \n",
       "5934       63       13  363.030532 -1.256976  1.010199 -0.638493  1.002502   \n",
       "5935       63       14  836.029816 -1.256976  1.010199  0.785146  1.608671   \n",
       "5936       63       15  327.243675 -1.256976  1.010199 -0.771600  0.428693   \n",
       "5937       63       16  831.250068 -1.256976  1.010199  2.454080  1.387184   \n",
       "5938       63       17  241.001800 -1.256976  1.010199  1.578955  2.225853   \n",
       "5939       63       19  220.500578 -1.256976  1.010199  2.656373  0.508312   \n",
       "5940       63       20  172.396413 -1.256976  1.010199  1.700099 -0.165836   \n",
       "5941       63       21  563.457385 -1.256976  1.010199  1.242704  2.654412   \n",
       "5942       63       22  427.248903 -1.256976  1.010199  1.463341  2.392778   \n",
       "5943       63       23  913.848739 -1.256976  1.010199  2.486287  1.671075   \n",
       "5944       63       24  289.641141 -1.256976  1.010199  0.159176  1.518408   \n",
       "5945       63       25  727.824889 -1.256976  1.010199 -0.165833  2.021509   \n",
       "5946       63       26  324.201031 -1.256976  1.010199 -0.183592  0.434537   \n",
       "5947       63       27  756.180664 -1.256976  1.010199  0.652139  1.485638   \n",
       "5948       63       28  844.153018 -1.256976  1.010199  0.730094  0.170283   \n",
       "5949       63       29  415.061864 -1.256976  1.010199  1.144372  1.979305   \n",
       "\n",
       "            v5  \n",
       "0     2.283803  \n",
       "1     0.800584  \n",
       "2     0.964481  \n",
       "3    -0.338393  \n",
       "4     2.296700  \n",
       "5     2.322430  \n",
       "6     1.160948  \n",
       "7     0.867349  \n",
       "8    -0.096715  \n",
       "9    -0.016004  \n",
       "10    0.054537  \n",
       "11   -0.097379  \n",
       "12    0.849372  \n",
       "13    1.291779  \n",
       "14    3.111055  \n",
       "15    0.425127  \n",
       "16    1.247214  \n",
       "17    0.580070  \n",
       "18   -0.352419  \n",
       "19    1.627302  \n",
       "20    1.357936  \n",
       "21    1.447835  \n",
       "22    1.093729  \n",
       "23   -0.200473  \n",
       "24    1.843948  \n",
       "25    2.064701  \n",
       "26    1.624292  \n",
       "27    0.974022  \n",
       "28   -0.270327  \n",
       "29    1.817345  \n",
       "...        ...  \n",
       "5920 -0.975903  \n",
       "5921  1.106700  \n",
       "5922  0.088917  \n",
       "5923  1.677883  \n",
       "5924  1.864099  \n",
       "5925 -0.523196  \n",
       "5926  3.190324  \n",
       "5927  0.076249  \n",
       "5928  1.883262  \n",
       "5929  0.451896  \n",
       "5930  1.595328  \n",
       "5931  2.197225  \n",
       "5932  1.895527  \n",
       "5933 -0.464103  \n",
       "5934 -0.352402  \n",
       "5935  1.276739  \n",
       "5936  2.736862  \n",
       "5937  0.735048  \n",
       "5938  1.339580  \n",
       "5939  1.295611  \n",
       "5940  0.741737  \n",
       "5941  2.157774  \n",
       "5942  0.026324  \n",
       "5943  1.998591  \n",
       "5944  1.616380  \n",
       "5945  2.040117  \n",
       "5946  0.948843  \n",
       "5947  2.294981  \n",
       "5948  1.343175  \n",
       "5949  1.993857  \n",
       "\n",
       "[5950 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_items_df = []\n",
    "for i in range(df.shape[0]):\n",
    "    users_items_df += df['state_history'][i] \n",
    "    \n",
    "users_items_df = pd.DataFrame(users_items_df,columns = ['user_id','item_id',\n",
    "                                                        'price','v1','v2','v3','v4','v5'])\n",
    "users_items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the first two covariates variables are features about users \n",
    "# and the forth and fifth variable are features about items\n",
    "# The variable 5 depends on .....\n",
    "\n",
    "user_v1 = users_items_df['v1'].unique()\n",
    "user_v2 = users_items_df['v2'].unique()\n",
    "users_ids = users_items_df['user_id'].unique()\n",
    "users_content= pd.DataFrame({'user_id' : users_ids,'v1':user_v1,'v2':user_v2 })\n",
    "\n",
    "item_v3 = users_items_df['v3'].unique()\n",
    "item_v4 = users_items_df['v4'].unique()\n",
    "item_id = users_items_df['item_id'].unique()\n",
    "items_content= pd.DataFrame({'item_id' : item_id,'v3':item_v3,'v4':item_v4 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df[df[\"rewards_history\"] > 0].reset_index(drop=True)\n",
    "neg_df = df[df[\"rewards_history\"] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>220.500578</td>\n",
       "      <td>[[88, 0, 298.8684368616429, 1.2703544096780341...</td>\n",
       "      <td>88</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>342.696272</td>\n",
       "      <td>[[58, 0, 298.8684368616429, 1.462631311727477,...</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>415.061864</td>\n",
       "      <td>[[41, 0, 298.8684368616429, 0.6231826361417208...</td>\n",
       "      <td>41</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>844.153018</td>\n",
       "      <td>[[74, 0, 298.8684368616429, 0.7655843226596099...</td>\n",
       "      <td>74</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>402.226952</td>\n",
       "      <td>[[83, 0, 298.8684368616429, 2.666291933399455,...</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0              19       220.500578   \n",
       "1               8       342.696272   \n",
       "2              29       415.061864   \n",
       "3              28       844.153018   \n",
       "4               3       402.226952   \n",
       "\n",
       "                                       state_history  user_id  item_id_action  \n",
       "0  [[88, 0, 298.8684368616429, 1.2703544096780341...       88              19  \n",
       "1  [[58, 0, 298.8684368616429, 1.462631311727477,...       58               8  \n",
       "2  [[41, 0, 298.8684368616429, 0.6231826361417208...       41              29  \n",
       "3  [[74, 0, 298.8684368616429, 0.7655843226596099...       74              28  \n",
       "4  [[83, 0, 298.8684368616429, 2.666291933399455,...       83               3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df[\"user_id\"] = [pos_df[\"state_history\"][i][0][0] for i in range(pos_df.shape[0])]\n",
    "pos_df[\"item_id_action\"] = [pos_df[\"state_history\"][i][pos_df[\"action_history\"][i]][1] for i in range(pos_df.shape[0])]\n",
    "pos_df.head()## There are some users that have more than one positive item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[96, 0, 298.8684368616429, 3.551004894370762,...</td>\n",
       "      <td>96</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[73, 0, 298.8684368616429, -0.257449587762428...</td>\n",
       "      <td>73</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[20, 0, 298.8684368616429, -1.657406472788035...</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[48, 0, 298.8684368616429, 0.2114771759183725...</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[28, 0, 298.8684368616429, 0.9619652995371561...</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0               9              0.0   \n",
       "1              13              0.0   \n",
       "2              26              0.0   \n",
       "3               4              0.0   \n",
       "4              11              0.0   \n",
       "\n",
       "                                       state_history  user_id  item_id_action  \n",
       "0  [[96, 0, 298.8684368616429, 3.551004894370762,...       96               9  \n",
       "1  [[73, 0, 298.8684368616429, -0.257449587762428...       73              13  \n",
       "2  [[20, 0, 298.8684368616429, -1.657406472788035...       20              26  \n",
       "3  [[48, 0, 298.8684368616429, 0.2114771759183725...       48               4  \n",
       "4  [[28, 0, 298.8684368616429, 0.9619652995371561...       28              11  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df[\"user_id\"] = [neg_df[\"state_history\"][i][0][0] for i in range(neg_df.shape[0])]\n",
    "neg_df[\"item_id_action\"] = [neg_df[\"state_history\"][i][neg_df[\"action_history\"][i]][1] for i in range(neg_df.shape[0])]\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"Ignore y_true and return the mean of y_pred\n",
    "    \n",
    "    This is a hack to work-around the design of the Keras API that is\n",
    "    not really suited to train networks with a triplet loss by default.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(y_pred + 0 * y_true)\n",
    "\n",
    "\n",
    "def margin_comparator_loss(inputs, margin=1.):\n",
    "    \"\"\"Comparator loss for a pair of precomputed similarities\n",
    "    \n",
    "    If the inputs are cosine similarities, they each have range in\n",
    "    (-1, 1), therefore their difference have range in (-2, 2). Using\n",
    "    a margin of 1. can therefore make sense.\n",
    "\n",
    "    If the input similarities are not normalized, it can be beneficial\n",
    "    to use larger values for the margin of the comparator loss.\n",
    "    \"\"\"\n",
    "    positive_pair_sim, negative_pair_sim = inputs\n",
    "    return tf.maximum(negative_pair_sim - positive_pair_sim + margin, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model(The most expensive item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model_expensive(state):\n",
    "    greater_index = 0\n",
    "    greater_value = 0\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if(state[i][2] > greater_value):\n",
    "            greater_value = state[i][2]\n",
    "            greater_index = i\n",
    "    return greater_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 94429.421857 and Total reward per run: 31476.473952\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 100\n",
    "total_reward = 0\n",
    "\n",
    "for k in range(number_run):\n",
    "    \n",
    "    for i in range(generations):\n",
    "\n",
    "        predicted_item = naive_model_expensive(next_state)\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        \n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model(The cheapest item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model_cheapest(state):\n",
    "    cheapest_index = 0\n",
    "    cheapest_value =  state[0][2]\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if(state[i][2] < cheapest_value):\n",
    "            cheapest_value = state[i][2]\n",
    "            cheapest_index = i\n",
    "            \n",
    "    return cheapest_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 10981.159575 and Total reward per run: 3660.386525\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 100\n",
    "total_reward = 0\n",
    "\n",
    "for k in range(number_run):\n",
    "    \n",
    "    for i in range(generations):\n",
    "\n",
    "        predicted_item = naive_model_cheapest(next_state)\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        \n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, Dense\n",
    "from keras.layers import Lambda, Dot\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import dot, concatenate\n",
    "\n",
    "\n",
    "def build_models(n_users, n_items, latent_dim=64, l2_reg=0):\n",
    "    \"\"\"Build a triplet model and its companion similarity model\n",
    "    \n",
    "    The triplet model is used to train the weights of the companion\n",
    "    similarity model. The triplet model takes 1 user, 1 positive item\n",
    "    (relative to the selected user) and one negative item and is\n",
    "    trained with comparator loss.\n",
    "    \n",
    "    The similarity model takes one user and one item as input and return\n",
    "    compatibility score (aka the match score).\n",
    "    \"\"\"\n",
    "    # Common architectural components for the two models:\n",
    "    # - symbolic input placeholders\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "\n",
    "    # - embeddings\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(input_dim=n_users + 1, output_dim=latent_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "    \n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(input_dim=n_items + 1, output_dim=latent_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # - similarity computation between embeddings\n",
    "    positive_similarity = Dot(name=\"positive_similarity\",\n",
    "                              axes=1, normalize=True)(\n",
    "        [user_embedding, positive_item_embedding])\n",
    "    negative_similarity = Dot(name=\"negative_similarity\",\n",
    "                              axes=1, normalize=True)(\n",
    "        [user_embedding, negative_item_embedding])\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss,\n",
    "                          name='comparator_loss',\n",
    "                          output_shape=(1,))([positive_similarity, negative_similarity])\n",
    "\n",
    "    triplet_model = Model(inputs=[user_input,\n",
    "                                  positive_item_input,\n",
    "                                  negative_item_input],\n",
    "                          outputs=triplet_loss)\n",
    "    \n",
    "    # The match-score model, only use at inference to rank items for a given\n",
    "    # model: the model weights are shared with the triplet_model therefore\n",
    "    # we do not need to train it and therefore we do not need to plug a loss\n",
    "    # and an optimizer.\n",
    "    match_model = Model(inputs=[user_input, positive_item_input],\n",
    "                        outputs=positive_similarity)\n",
    "    \n",
    "    return triplet_model, match_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_df,max_items):\n",
    "    \"\"\"\n",
    "    Sample negatives at random\n",
    "    \n",
    "    \"\"\"\n",
    "    range_itens = np.arange(max_items)\n",
    "    user_ids = pos_df['user_id'].values\n",
    "    pos_item_ids = pos_df['item_id_action'].values\n",
    "    \n",
    "    \n",
    "    user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "    neg_item_ids = np.array([])\n",
    "    for i in user_ids:\n",
    "        if i in set(user_pos_items.index):\n",
    "            number = np.random.choice([item for item in range_itens \n",
    "                                       if not item in user_pos_items[user_pos_items.index == i]])\n",
    "        else:\n",
    "            number = np.random.choice(range_itens,1)\n",
    "        neg_item_ids = np.append(neg_item_ids,number)\n",
    "    \n",
    "  \n",
    "    return [user_ids, pos_item_ids, neg_item_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0224\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 78us/step - loss: 0.9764\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 66us/step - loss: 0.9556\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 100us/step - loss: 0.8529\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 72us/step - loss: 0.8445\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 82us/step - loss: 0.7553\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 112us/step - loss: 0.7143\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 78us/step - loss: 0.6954\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 87us/step - loss: 0.6616\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 93us/step - loss: 0.6529\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 69us/step - loss: 0.6286\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.5228\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 80us/step - loss: 0.4996\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 77us/step - loss: 0.4970\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 87us/step - loss: 0.4953\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 87us/step - loss: 0.4395\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 59us/step - loss: 0.4278\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 92us/step - loss: 0.3501\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.4671\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 80us/step - loss: 0.3907\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 92us/step - loss: 0.4184\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 81us/step - loss: 0.3528\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 79us/step - loss: 0.3256\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 89us/step - loss: 0.3701\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 77us/step - loss: 0.3161\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 60us/step - loss: 0.3152\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 83us/step - loss: 0.2827\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 67us/step - loss: 0.2774\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 78us/step - loss: 0.2629\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 60us/step - loss: 0.2544\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3167\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1007\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4251\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5185\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8913\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0456\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1138\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0563\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7230\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5791\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8068\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2970\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3057\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9532\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3899\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7040\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6167e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6692\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4997\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3063\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7615\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6735\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1535\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4231\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0289\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 66us/step - loss: 1.0060\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 77us/step - loss: 0.9417\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 93us/step - loss: 0.9170\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 89us/step - loss: 0.8176\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 77us/step - loss: 0.7964\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 84us/step - loss: 0.6836\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 80us/step - loss: 0.6643\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 78us/step - loss: 0.6798\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 94us/step - loss: 0.6084\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 76us/step - loss: 0.5930\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 82us/step - loss: 0.5749\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 85us/step - loss: 0.5331\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 63us/step - loss: 0.5198\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 66us/step - loss: 0.4406\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 85us/step - loss: 0.4743\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 83us/step - loss: 0.4416\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 64us/step - loss: 0.4180\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 87us/step - loss: 0.3903\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 70us/step - loss: 0.4429\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.4466\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 88us/step - loss: 0.2706\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.3050\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 61us/step - loss: 0.3194\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 84us/step - loss: 0.3232\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 61us/step - loss: 0.3065\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 83us/step - loss: 0.2699\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 78us/step - loss: 0.2988\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 81us/step - loss: 0.2712\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 74us/step - loss: 0.2952\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9391\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0027\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7092\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7081\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3262\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7415\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3784\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6808\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6059\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3130\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1107\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2664\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5204\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8013\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6526\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1886\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0037\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1844\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9743\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1652\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3372\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0656\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.9897\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 71us/step - loss: 0.9428\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 61us/step - loss: 0.8774\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 143us/step - loss: 0.8242\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 64us/step - loss: 0.8009\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 90us/step - loss: 0.7505\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 79us/step - loss: 0.7180\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 61us/step - loss: 0.7494\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 68us/step - loss: 0.6462\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 59us/step - loss: 0.6393\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.6331\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 66us/step - loss: 0.5670\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 67us/step - loss: 0.5367\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 75us/step - loss: 0.5505\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 67us/step - loss: 0.4525\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 63us/step - loss: 0.4284\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 155us/step - loss: 0.3833\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 58us/step - loss: 0.4454\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.4368\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 60us/step - loss: 0.3793\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 74us/step - loss: 0.3202\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 77us/step - loss: 0.4211\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 91us/step - loss: 0.4062\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 139us/step - loss: 0.3545\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 64us/step - loss: 0.3486\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 101us/step - loss: 0.2934\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 105us/step - loss: 0.2688\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 64us/step - loss: 0.2596\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 80us/step - loss: 0.2748\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 79us/step - loss: 0.2595\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9358\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0047\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5154\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5502\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5078\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3314\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9648\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3731\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3451\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9930\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0138\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5216\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4900\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8314\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2263\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0370\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1089\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9786\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1685\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4546\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8449\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3274\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0820\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1815\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5630\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8625\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8299\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3309\n",
      "Total reward: 42826.796931 and Total reward per run: 14275.598977\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "n_epochs = 30\n",
    "total_reward = 0\n",
    "generations = 100\n",
    "max_items = 30\n",
    "\n",
    "for k in range(number_run):\n",
    "\n",
    "    triplet_model, match_model = build_models(nb_users, nb_items, latent_dim=16, l2_reg=1e-6)\n",
    "    # we plug the identity loss and the a fake target variable ignored by\n",
    "    # the model to be able to use the Keras API to train the triplet model\n",
    "    triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "    fake_y = np.ones_like(pos_df['user_id'])\n",
    "    params = {'user_id': USER_ID}\n",
    "\n",
    "    positive_data = pos_df.copy()\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        # Sample new negatives to build different triplets at each epoch\n",
    "        triplet_inputs = sample_triplets(positive_data, max_items=nb_items)\n",
    "\n",
    "        # Fit the model incrementally by doing a single pass over the\n",
    "        # sampled triplets.\n",
    "        triplet_model.fit(triplet_inputs, fake_y, shuffle=True, epochs=1)\n",
    "\n",
    "    for j in range(generations):\n",
    "\n",
    "\n",
    "        # The user that we need to recommend a product\n",
    "        new_user_id = next_state[0][0]\n",
    "        # Take all the items in the new state\n",
    "        items_ids = np.array([next_state[i][1]\n",
    "                              for i in range(len(next_state))])\n",
    "\n",
    "        # Create an array of the same size of 'items_ids' of all values equal to 'new_user_id'\n",
    "        repeated_user_id = np.empty_like(items_ids)\n",
    "        repeated_user_id.fill(new_user_id)\n",
    "\n",
    "        # predict the product using our model\n",
    "        predicted = match_model.predict([repeated_user_id, items_ids])\n",
    "        predicted_item = np.argmax(predicted)\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        # take the reward from the API\n",
    "        r = requests.get(url=url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "\n",
    "        # (Online learning)Check if the reward is greater than 0 , if yes train again \n",
    "        # the model the sample point\n",
    "        if(reward > 0):\n",
    "            range_itens = np.arange(max_items)\n",
    "            user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "            if new_user_id in set(user_pos_items.index):\n",
    "                neg_item_id = np.random.choice([item for item in range_itens \n",
    "                                           if not item in user_pos_items[user_pos_items.index == new_user_id]])\n",
    "            else:\n",
    "                neg_item_id = np.random.choice(range_itens,1)\n",
    "            \n",
    "           \n",
    "            \n",
    "            triplet_inputs = [[new_user_id], [predicted_item], [neg_item_id]]\n",
    "    \n",
    "            fake_y = np.ones_like([1])\n",
    "            triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size=64, epochs=1)\n",
    "    \n",
    "            \n",
    "        # Take the nex_state\n",
    "        next_state = r['state']\n",
    "\n",
    "\n",
    "print(\"Total reward: %f and Total reward per run: %f\" %(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dense, Dropout\n",
    "from keras.layers import Concatenate, Lambda\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def make_interaction_mlp(input_dim, n_hidden=1, hidden_size=64,\n",
    "                         dropout=0, l2_reg=None):\n",
    "    \"\"\"\n",
    "    Build the shared multi layer perceptron\n",
    "\n",
    "    \"\"\"\n",
    "    mlp = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        # Plug the output unit directly: this is a simple\n",
    "        # linear regression model. Not dropout required.\n",
    "        mlp.add(Dense(1, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "    else:\n",
    "        mlp.add(Dense(hidden_size, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "        mlp.add(Dropout(dropout))\n",
    "        for i in range(n_hidden - 1):\n",
    "            mlp.add(Dense(hidden_size, activation='relu',\n",
    "                          kernel_regularizer=l2_reg))\n",
    "            mlp.add(Dropout(dropout))\n",
    "        mlp.add(Dense(1, activation='relu', kernel_regularizer=l2_reg))\n",
    "    return mlp\n",
    "\n",
    "\n",
    "def build_models(n_users, n_items, user_dim=32, item_dim=64,\n",
    "                 n_hidden=1, hidden_size=64, dropout=0, l2_reg=0):\n",
    "    \"\"\"\n",
    "    Build models to train a deep triplet network\n",
    "\n",
    "    \"\"\"\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "    positive_meta_input = Input((2,), name='positive_meta_input')\n",
    "    negative_meta_input = Input((2,), name='negative_meta_input')\n",
    "\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(n_users + 1, user_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "\n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(n_items + 1, item_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # Similarity computation between embeddings using a MLP similarity\n",
    "    positive_embeddings_pair = Concatenate(name=\"positive_embeddings_pair\")(\n",
    "        [user_embedding, positive_item_embedding, positive_meta_input])\n",
    "    positive_embeddings_pair = Dropout(dropout)(positive_embeddings_pair)\n",
    "\n",
    "    negative_embeddings_pair = Concatenate(name=\"negative_embeddings_pair\")(\n",
    "        [user_embedding, negative_item_embedding, negative_meta_input])\n",
    "    negative_embeddings_pair = Dropout(dropout)(negative_embeddings_pair)\n",
    "\n",
    "    # Instanciate the shared similarity architecture\n",
    "    interaction_layers = make_interaction_mlp(\n",
    "        user_dim + item_dim + 2, n_hidden=n_hidden, hidden_size=hidden_size,\n",
    "        dropout=dropout, l2_reg=l2_reg)\n",
    "\n",
    "    positive_similarity = interaction_layers(positive_embeddings_pair)\n",
    "    negative_similarity = interaction_layers(negative_embeddings_pair)\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss, output_shape=(1,),\n",
    "                          name='comparator_loss')(\n",
    "        [positive_similarity, negative_similarity])\n",
    "\n",
    "    deep_triplet_model = Model(inputs=[user_input,\n",
    "                                       positive_item_input,\n",
    "                                       negative_item_input,\n",
    "                                       positive_meta_input,\n",
    "                                       negative_meta_input\n",
    "                                       ],\n",
    "                               outputs=[triplet_loss])\n",
    "\n",
    "    # The match-score model, only used at inference\n",
    "    deep_match_model = Model(inputs=[user_input, positive_item_input, positive_meta_input],\n",
    "                             outputs=[positive_similarity])\n",
    "\n",
    "    return deep_match_model, deep_triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_df, items_content, max_items):\n",
    "    \"\"\"\n",
    "    Sample the data and put in the format \n",
    "    [user_ids, pos_item_ids, neg_item_ids,pos_item_metadata,neg_item_metadata]\n",
    "\n",
    "    \"\"\"\n",
    "    range_itens = np.arange(max_items)\n",
    "    user_ids = pos_df['user_id'].values\n",
    "    pos_item_ids = pos_df['item_id_action'].values\n",
    "\n",
    "    pos_item_metadata = items_content.loc[pos_item_ids][['v3', 'v4']].values\n",
    "\n",
    "    \n",
    "    user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "    neg_item_ids = np.array([])\n",
    "    for i in user_ids:\n",
    "        if i in set(user_pos_items.index):\n",
    "            number = np.random.choice([item for item in range_itens \n",
    "                                       if not item in user_pos_items[user_pos_items.index == i]])\n",
    "        else:\n",
    "            number = np.random.choice(range_itens, 1)\n",
    "        neg_item_ids = np.append(neg_item_ids, number)\n",
    "\n",
    "    neg_item_metadata = items_content.loc[neg_item_ids][['v3', 'v4']].values\n",
    "\n",
    "    return [user_ids, pos_item_ids, neg_item_ids, pos_item_metadata, neg_item_metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = dict(\n",
    "    user_dim=50,\n",
    "    item_dim=15,\n",
    "    n_hidden=3,\n",
    "    hidden_size=16,\n",
    "    dropout=0.5,\n",
    "    l2_reg=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 1.0147\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 45us/step - loss: 0.9023\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 1.0016\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 1.0089\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 38us/step - loss: 1.0073\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 46us/step - loss: 0.9923\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 1.0206\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 40us/step - loss: 0.9922\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 39us/step - loss: 1.0724\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 54us/step - loss: 1.0451\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 0.9677\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 39us/step - loss: 1.0633\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 36us/step - loss: 0.9628\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 38us/step - loss: 1.0070\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 39us/step - loss: 0.9468\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 46us/step - loss: 0.9755\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 48us/step - loss: 0.9894\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 40us/step - loss: 1.0043\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 0.9466\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 1.0180\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 49us/step - loss: 0.9743\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 1.0405\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 49us/step - loss: 0.9965\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 40us/step - loss: 1.0172\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 48us/step - loss: 0.9993\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 45us/step - loss: 0.9937\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 45us/step - loss: 0.9350\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 36us/step - loss: 1.0118\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 52us/step - loss: 1.0115\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 0.9867\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9898\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0106\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9642\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8505\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8541\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9741\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9999\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0374\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9470\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8171\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9258\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0334\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9934\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3958\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7374\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6034\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0870\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9123\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0661\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9696\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2458\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9490\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0113\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1045\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0178\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0611\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0930\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 1.0374\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 152us/step - loss: 0.9718\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 48us/step - loss: 1.0021\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 38us/step - loss: 1.0024\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 44us/step - loss: 1.0296\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 46us/step - loss: 1.0031\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 39us/step - loss: 1.1037\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 1.0202\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 44us/step - loss: 1.0625\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 93us/step - loss: 1.0450\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 1.0695\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 1.0953\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 44us/step - loss: 1.0170\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 0.9969\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 44us/step - loss: 1.0089\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 0.8398\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 50us/step - loss: 1.0143\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 0.9164\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 45us/step - loss: 1.0299\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 43us/step - loss: 0.9801\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 63us/step - loss: 0.9654\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 0.9697\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 46us/step - loss: 1.0324\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 44us/step - loss: 1.0443\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 99us/step - loss: 0.9260\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 57us/step - loss: 0.9784\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 51us/step - loss: 0.9845\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 40us/step - loss: 0.9641\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 46us/step - loss: 0.9903\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 37us/step - loss: 1.0674\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6007\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1595\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6734\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6956\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2347\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9917\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0097\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8141\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2346\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8265\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.3435\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 1s 17ms/step - loss: 1.0091\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 43us/step - loss: 1.0066\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 1.0063\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 1.0026\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 43us/step - loss: 0.9973\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 1.0030\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 46us/step - loss: 1.0534\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 1.0056\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 43us/step - loss: 1.0004\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 67us/step - loss: 1.0122\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 46us/step - loss: 1.0047\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 49us/step - loss: 0.9794\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 1.0111\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 1.0172\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 1.0137\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 0.9952\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 51us/step - loss: 1.0233\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 50us/step - loss: 1.0193\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 41us/step - loss: 1.0053\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 55us/step - loss: 1.0110\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 43us/step - loss: 0.9674\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 125us/step - loss: 0.9988\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 51us/step - loss: 1.0087\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 51us/step - loss: 0.9736\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 50us/step - loss: 1.0068\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 48us/step - loss: 0.9984\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.9446\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 42us/step - loss: 0.9995\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 43us/step - loss: 0.9944\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 44us/step - loss: 0.9994\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5755\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7976\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9317\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9903\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0418\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9970\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0555\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9613\n",
      "Total reward: 27877.902235 and Total reward per run: 9292.634078\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "n_epochs = 30\n",
    "total_reward = 0\n",
    "generations = 100\n",
    "\n",
    "\n",
    "for k in range(number_run):\n",
    "\n",
    "    deep_match_model, deep_triplet_model = build_models(nb_users, nb_items,**hyper_parameters)\n",
    "    # we plug the identity loss and the a fake target variable ignored by\n",
    "    # the model to be able to use the Keras API to train the triplet model\n",
    "    deep_triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "    fake_y = np.ones_like(pos_df['user_id'])\n",
    "    params = {'user_id': USER_ID}\n",
    "    \n",
    "    positive_data = pos_df.copy()\n",
    "    \n",
    "    # Training the model \n",
    "    for i in range(n_epochs):\n",
    "        # Sample new negatives to build different triplets at each epoch\n",
    "        triplet_inputs = sample_triplets(\n",
    "            positive_data, items_content, max_items=nb_items)\n",
    "\n",
    "        # Fit the model incrementally by doing a single pass over the\n",
    "        # sampled triplets.\n",
    "        deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size=64, epochs=1)\n",
    "\n",
    "\n",
    "    for j in range(generations):\n",
    "        \n",
    "        # Predicting\n",
    "        new_user_id = next_state[0][0]\n",
    "        items_ids = np.array([next_state[i][1] for i in range(len(next_state))])\n",
    "        repeated_user_id = np.empty_like(items_ids)\n",
    "        repeated_user_id.fill(new_user_id)\n",
    "\n",
    "        pos_metadata = items_content.loc[items_ids][['v3', 'v4']].values\n",
    "        predicted = deep_match_model.predict([repeated_user_id, items_ids, pos_metadata])\n",
    "        predicted_item = np.argmax(predicted)\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url=url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "\n",
    "        # Online learning\n",
    "        if(reward > 0):\n",
    "            range_itens = np.arange(max_items)\n",
    "            user_pos_items = positive_data.groupby('user_id')['item_id_action'].apply(list)\n",
    "            if new_user_id in set(user_pos_items.index):\n",
    "                neg_item_id = np.random.choice([item for item in range_itens \n",
    "                                           if not item in user_pos_items[user_pos_items.index == new_user_id]])\n",
    "            else:\n",
    "                neg_item_id = np.random.choice(range_itens,1)\n",
    "            \n",
    "            pos_item_metadata = items_content.set_index('item_id').loc[predicted_item].values\n",
    "            neg_item_metadata = items_content.set_index('item_id').loc[neg_item_id].values\n",
    "            \n",
    "            triplet_inputs = [[new_user_id], [predicted_item], [neg_item_id], \n",
    "                              pos_item_metadata.reshape((1,2)), neg_item_metadata.reshape((1,2))]\n",
    "    \n",
    "            fake_y = np.ones_like([1])\n",
    "            deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size = 8, epochs=1)\n",
    "\n",
    "        next_state = r['state']\n",
    "\n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

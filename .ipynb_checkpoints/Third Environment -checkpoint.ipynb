{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project on recommander systems : Third Environmement\n",
    "\n",
    "#### Author : Raymond KUOCH and Alexandre MAXINSANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Concatenate, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['action_history', 'nb_items', 'nb_users', 'next_state', 'rewards_history', 'state_history'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attributes \n",
    "USER_ID = 'IAHIZPIW80WPGDW7P7JE'\n",
    "BASE_URL = \"http://35.180.178.243\"\n",
    "url_reset = BASE_URL + \"/reset\"\n",
    "url_predict = BASE_URL + \"/predict\"\n",
    "params = {\"user_id\" : USER_ID}\n",
    "\n",
    "def load_data(url_reset,params) :\n",
    "    r = requests.get(url=url_reset, params=params)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "data = load_data(url_reset,params)\n",
    "nb_users = data[\"nb_users\"]\n",
    "nb_items = data[\"nb_items\"]\n",
    "next_state = data[\"next_state\"]\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[8, 0, 45.63728801221079, 2.6725193925993143,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[29, 0, 45.63728801221079, 0.1561980677443061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[23, 0, 45.63728801221079, 3.5799797302261704...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>468.002443</td>\n",
       "      <td>[[49, 0, 45.63728801221079, 0.1356474524899955...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[50, 0, 45.63728801221079, -0.515370624544243...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0              20         0.000000   \n",
       "1               2         0.000000   \n",
       "2              26         0.000000   \n",
       "3               7       468.002443   \n",
       "4              24         0.000000   \n",
       "\n",
       "                                       state_history  \n",
       "0  [[8, 0, 45.63728801221079, 2.6725193925993143,...  \n",
       "1  [[29, 0, 45.63728801221079, 0.1561980677443061...  \n",
       "2  [[23, 0, 45.63728801221079, 3.5799797302261704...  \n",
       "3  [[49, 0, 45.63728801221079, 0.1356474524899955...  \n",
       "4  [[50, 0, 45.63728801221079, -0.515370624544243...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklisted_set = set((\"next_state\",\"nb_items\",\"nb_users\"))\n",
    "new_dict = {key : value for key, value in data.items() if key not in blacklisted_set}\n",
    "\n",
    "df = pd.DataFrame(new_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.450000</td>\n",
       "      <td>192.972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.204515</td>\n",
       "      <td>311.817769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.250000</td>\n",
       "      <td>468.002443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>990.309864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       action_history  rewards_history\n",
       "count      200.000000       200.000000\n",
       "mean        14.450000       192.972800\n",
       "std          8.204515       311.817769\n",
       "min          0.000000         0.000000\n",
       "25%          8.000000         0.000000\n",
       "50%         14.000000         0.000000\n",
       "75%         21.250000       468.002443\n",
       "max         29.000000       990.309864"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action_history     132\n",
       "rewards_history    132\n",
       "state_history      132\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['rewards_history']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>45.637288</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>1.997002</td>\n",
       "      <td>1.358978</td>\n",
       "      <td>1.121579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>36.177025</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.664057</td>\n",
       "      <td>0.911352</td>\n",
       "      <td>1.483312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>976.646282</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.337752</td>\n",
       "      <td>2.157311</td>\n",
       "      <td>-0.763668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>777.420662</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.649728</td>\n",
       "      <td>1.766307</td>\n",
       "      <td>0.530752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>887.343487</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>1.030800</td>\n",
       "      <td>1.038443</td>\n",
       "      <td>-0.860784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>557.167740</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.104437</td>\n",
       "      <td>0.745774</td>\n",
       "      <td>1.454469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>887.150616</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>-0.857658</td>\n",
       "      <td>0.691669</td>\n",
       "      <td>-0.258284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>468.002443</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.197570</td>\n",
       "      <td>0.650010</td>\n",
       "      <td>0.906133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>372.511602</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>-0.657884</td>\n",
       "      <td>0.058255</td>\n",
       "      <td>1.814888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>616.191471</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.675207</td>\n",
       "      <td>-0.216844</td>\n",
       "      <td>0.345592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>573.966331</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.949229</td>\n",
       "      <td>1.618393</td>\n",
       "      <td>1.218415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>496.516509</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>3.068601</td>\n",
       "      <td>1.523164</td>\n",
       "      <td>0.843901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>973.709496</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.082550</td>\n",
       "      <td>-0.096486</td>\n",
       "      <td>2.807787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>990.309864</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>2.077429</td>\n",
       "      <td>2.279315</td>\n",
       "      <td>2.349427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>934.658474</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>-0.196654</td>\n",
       "      <td>1.213828</td>\n",
       "      <td>0.567277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>744.775034</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>1.741799</td>\n",
       "      <td>0.125956</td>\n",
       "      <td>2.096912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>662.539654</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>1.671359</td>\n",
       "      <td>0.764067</td>\n",
       "      <td>-0.056717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>131.124946</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>2.817030</td>\n",
       "      <td>-0.636568</td>\n",
       "      <td>1.244330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>71.774054</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>-0.247987</td>\n",
       "      <td>1.902171</td>\n",
       "      <td>1.245850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>98.372427</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.210153</td>\n",
       "      <td>0.980105</td>\n",
       "      <td>2.344571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>631.568645</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.203142</td>\n",
       "      <td>0.927467</td>\n",
       "      <td>1.302444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>10.687884</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>1.275774</td>\n",
       "      <td>1.314107</td>\n",
       "      <td>3.213083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>116.810631</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>1.431943</td>\n",
       "      <td>0.154163</td>\n",
       "      <td>0.990744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>761.888879</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>1.609808</td>\n",
       "      <td>0.709375</td>\n",
       "      <td>1.467390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>250.052652</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>-1.287018</td>\n",
       "      <td>0.852805</td>\n",
       "      <td>1.736816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>154.396372</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.192514</td>\n",
       "      <td>1.632899</td>\n",
       "      <td>1.635923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>829.696632</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>1.228532</td>\n",
       "      <td>0.977877</td>\n",
       "      <td>2.853302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>637.027068</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>0.748047</td>\n",
       "      <td>0.419460</td>\n",
       "      <td>0.033745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>755.360432</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>1.061854</td>\n",
       "      <td>0.817771</td>\n",
       "      <td>-0.651229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>536.074738</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "      <td>1.845974</td>\n",
       "      <td>-0.479239</td>\n",
       "      <td>1.065143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>536.074738</td>\n",
       "      <td>0.505959</td>\n",
       "      <td>0.069411</td>\n",
       "      <td>1.845974</td>\n",
       "      <td>-0.479239</td>\n",
       "      <td>0.878303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>45.637288</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>1.997002</td>\n",
       "      <td>1.358978</td>\n",
       "      <td>1.958326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>36.177025</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.664057</td>\n",
       "      <td>0.911352</td>\n",
       "      <td>3.050351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5911</th>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>976.646282</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.337752</td>\n",
       "      <td>2.157311</td>\n",
       "      <td>-0.170475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5912</th>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>777.420662</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.649728</td>\n",
       "      <td>1.766307</td>\n",
       "      <td>1.872452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>887.343487</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>1.030800</td>\n",
       "      <td>1.038443</td>\n",
       "      <td>-0.511306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5914</th>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>557.167740</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.104437</td>\n",
       "      <td>0.745774</td>\n",
       "      <td>-0.814250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "      <td>887.150616</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>-0.857658</td>\n",
       "      <td>0.691669</td>\n",
       "      <td>-0.722852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>77</td>\n",
       "      <td>7</td>\n",
       "      <td>468.002443</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.197570</td>\n",
       "      <td>0.650010</td>\n",
       "      <td>0.151958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>77</td>\n",
       "      <td>8</td>\n",
       "      <td>372.511602</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>-0.657884</td>\n",
       "      <td>0.058255</td>\n",
       "      <td>-0.548611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>616.191471</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.675207</td>\n",
       "      <td>-0.216844</td>\n",
       "      <td>0.654347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>77</td>\n",
       "      <td>10</td>\n",
       "      <td>573.966331</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.949229</td>\n",
       "      <td>1.618393</td>\n",
       "      <td>0.564426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5920</th>\n",
       "      <td>77</td>\n",
       "      <td>11</td>\n",
       "      <td>496.516509</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>3.068601</td>\n",
       "      <td>1.523164</td>\n",
       "      <td>1.603661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5921</th>\n",
       "      <td>77</td>\n",
       "      <td>12</td>\n",
       "      <td>973.709496</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.082550</td>\n",
       "      <td>-0.096486</td>\n",
       "      <td>0.572603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>77</td>\n",
       "      <td>13</td>\n",
       "      <td>990.309864</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>2.077429</td>\n",
       "      <td>2.279315</td>\n",
       "      <td>-0.127259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>77</td>\n",
       "      <td>14</td>\n",
       "      <td>934.658474</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>-0.196654</td>\n",
       "      <td>1.213828</td>\n",
       "      <td>0.205245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>77</td>\n",
       "      <td>15</td>\n",
       "      <td>744.775034</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>1.741799</td>\n",
       "      <td>0.125956</td>\n",
       "      <td>0.440658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925</th>\n",
       "      <td>77</td>\n",
       "      <td>16</td>\n",
       "      <td>662.539654</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>1.671359</td>\n",
       "      <td>0.764067</td>\n",
       "      <td>-0.001231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5926</th>\n",
       "      <td>77</td>\n",
       "      <td>17</td>\n",
       "      <td>131.124946</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>2.817030</td>\n",
       "      <td>-0.636568</td>\n",
       "      <td>0.261785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5927</th>\n",
       "      <td>77</td>\n",
       "      <td>18</td>\n",
       "      <td>71.774054</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>-0.247987</td>\n",
       "      <td>1.902171</td>\n",
       "      <td>0.017394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5928</th>\n",
       "      <td>77</td>\n",
       "      <td>19</td>\n",
       "      <td>98.372427</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.210153</td>\n",
       "      <td>0.980105</td>\n",
       "      <td>1.094432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5929</th>\n",
       "      <td>77</td>\n",
       "      <td>20</td>\n",
       "      <td>631.568645</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.203142</td>\n",
       "      <td>0.927467</td>\n",
       "      <td>-0.283377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5930</th>\n",
       "      <td>77</td>\n",
       "      <td>21</td>\n",
       "      <td>10.687884</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>1.275774</td>\n",
       "      <td>1.314107</td>\n",
       "      <td>0.763971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "      <td>116.810631</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>1.431943</td>\n",
       "      <td>0.154163</td>\n",
       "      <td>0.813567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>77</td>\n",
       "      <td>23</td>\n",
       "      <td>761.888879</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>1.609808</td>\n",
       "      <td>0.709375</td>\n",
       "      <td>1.151952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>77</td>\n",
       "      <td>24</td>\n",
       "      <td>250.052652</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>-1.287018</td>\n",
       "      <td>0.852805</td>\n",
       "      <td>2.366565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5934</th>\n",
       "      <td>77</td>\n",
       "      <td>25</td>\n",
       "      <td>154.396372</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.192514</td>\n",
       "      <td>1.632899</td>\n",
       "      <td>0.148209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>77</td>\n",
       "      <td>26</td>\n",
       "      <td>829.696632</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>1.228532</td>\n",
       "      <td>0.977877</td>\n",
       "      <td>1.525799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>77</td>\n",
       "      <td>27</td>\n",
       "      <td>637.027068</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>0.748047</td>\n",
       "      <td>0.419460</td>\n",
       "      <td>-0.975688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>77</td>\n",
       "      <td>29</td>\n",
       "      <td>536.074738</td>\n",
       "      <td>1.053297</td>\n",
       "      <td>1.148210</td>\n",
       "      <td>1.845974</td>\n",
       "      <td>-0.479239</td>\n",
       "      <td>1.507319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5938 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id       price        v1        v2        v3        v4  \\\n",
       "0           8        0   45.637288  2.672519  2.710697  1.997002  1.358978   \n",
       "1           8        1   36.177025  2.672519  2.710697  0.664057  0.911352   \n",
       "2           8        2  976.646282  2.672519  2.710697  0.337752  2.157311   \n",
       "3           8        3  777.420662  2.672519  2.710697  0.649728  1.766307   \n",
       "4           8        4  887.343487  2.672519  2.710697  1.030800  1.038443   \n",
       "5           8        5  557.167740  2.672519  2.710697  0.104437  0.745774   \n",
       "6           8        6  887.150616  2.672519  2.710697 -0.857658  0.691669   \n",
       "7           8        7  468.002443  2.672519  2.710697  0.197570  0.650010   \n",
       "8           8        8  372.511602  2.672519  2.710697 -0.657884  0.058255   \n",
       "9           8        9  616.191471  2.672519  2.710697  0.675207 -0.216844   \n",
       "10          8       10  573.966331  2.672519  2.710697  0.949229  1.618393   \n",
       "11          8       11  496.516509  2.672519  2.710697  3.068601  1.523164   \n",
       "12          8       12  973.709496  2.672519  2.710697  0.082550 -0.096486   \n",
       "13          8       13  990.309864  2.672519  2.710697  2.077429  2.279315   \n",
       "14          8       14  934.658474  2.672519  2.710697 -0.196654  1.213828   \n",
       "15          8       15  744.775034  2.672519  2.710697  1.741799  0.125956   \n",
       "16          8       16  662.539654  2.672519  2.710697  1.671359  0.764067   \n",
       "17          8       17  131.124946  2.672519  2.710697  2.817030 -0.636568   \n",
       "18          8       18   71.774054  2.672519  2.710697 -0.247987  1.902171   \n",
       "19          8       19   98.372427  2.672519  2.710697  0.210153  0.980105   \n",
       "20          8       20  631.568645  2.672519  2.710697  0.203142  0.927467   \n",
       "21          8       21   10.687884  2.672519  2.710697  1.275774  1.314107   \n",
       "22          8       22  116.810631  2.672519  2.710697  1.431943  0.154163   \n",
       "23          8       23  761.888879  2.672519  2.710697  1.609808  0.709375   \n",
       "24          8       24  250.052652  2.672519  2.710697 -1.287018  0.852805   \n",
       "25          8       25  154.396372  2.672519  2.710697  0.192514  1.632899   \n",
       "26          8       26  829.696632  2.672519  2.710697  1.228532  0.977877   \n",
       "27          8       27  637.027068  2.672519  2.710697  0.748047  0.419460   \n",
       "28          8       28  755.360432  2.672519  2.710697  1.061854  0.817771   \n",
       "29          8       29  536.074738  2.672519  2.710697  1.845974 -0.479239   \n",
       "...       ...      ...         ...       ...       ...       ...       ...   \n",
       "5908       66       29  536.074738  0.505959  0.069411  1.845974 -0.479239   \n",
       "5909       77        0   45.637288  1.053297  1.148210  1.997002  1.358978   \n",
       "5910       77        1   36.177025  1.053297  1.148210  0.664057  0.911352   \n",
       "5911       77        2  976.646282  1.053297  1.148210  0.337752  2.157311   \n",
       "5912       77        3  777.420662  1.053297  1.148210  0.649728  1.766307   \n",
       "5913       77        4  887.343487  1.053297  1.148210  1.030800  1.038443   \n",
       "5914       77        5  557.167740  1.053297  1.148210  0.104437  0.745774   \n",
       "5915       77        6  887.150616  1.053297  1.148210 -0.857658  0.691669   \n",
       "5916       77        7  468.002443  1.053297  1.148210  0.197570  0.650010   \n",
       "5917       77        8  372.511602  1.053297  1.148210 -0.657884  0.058255   \n",
       "5918       77        9  616.191471  1.053297  1.148210  0.675207 -0.216844   \n",
       "5919       77       10  573.966331  1.053297  1.148210  0.949229  1.618393   \n",
       "5920       77       11  496.516509  1.053297  1.148210  3.068601  1.523164   \n",
       "5921       77       12  973.709496  1.053297  1.148210  0.082550 -0.096486   \n",
       "5922       77       13  990.309864  1.053297  1.148210  2.077429  2.279315   \n",
       "5923       77       14  934.658474  1.053297  1.148210 -0.196654  1.213828   \n",
       "5924       77       15  744.775034  1.053297  1.148210  1.741799  0.125956   \n",
       "5925       77       16  662.539654  1.053297  1.148210  1.671359  0.764067   \n",
       "5926       77       17  131.124946  1.053297  1.148210  2.817030 -0.636568   \n",
       "5927       77       18   71.774054  1.053297  1.148210 -0.247987  1.902171   \n",
       "5928       77       19   98.372427  1.053297  1.148210  0.210153  0.980105   \n",
       "5929       77       20  631.568645  1.053297  1.148210  0.203142  0.927467   \n",
       "5930       77       21   10.687884  1.053297  1.148210  1.275774  1.314107   \n",
       "5931       77       22  116.810631  1.053297  1.148210  1.431943  0.154163   \n",
       "5932       77       23  761.888879  1.053297  1.148210  1.609808  0.709375   \n",
       "5933       77       24  250.052652  1.053297  1.148210 -1.287018  0.852805   \n",
       "5934       77       25  154.396372  1.053297  1.148210  0.192514  1.632899   \n",
       "5935       77       26  829.696632  1.053297  1.148210  1.228532  0.977877   \n",
       "5936       77       27  637.027068  1.053297  1.148210  0.748047  0.419460   \n",
       "5937       77       29  536.074738  1.053297  1.148210  1.845974 -0.479239   \n",
       "\n",
       "            v5  \n",
       "0     1.121579  \n",
       "1     1.483312  \n",
       "2    -0.763668  \n",
       "3     0.530752  \n",
       "4    -0.860784  \n",
       "5     1.454469  \n",
       "6    -0.258284  \n",
       "7     0.906133  \n",
       "8     1.814888  \n",
       "9     0.345592  \n",
       "10    1.218415  \n",
       "11    0.843901  \n",
       "12    2.807787  \n",
       "13    2.349427  \n",
       "14    0.567277  \n",
       "15    2.096912  \n",
       "16   -0.056717  \n",
       "17    1.244330  \n",
       "18    1.245850  \n",
       "19    2.344571  \n",
       "20    1.302444  \n",
       "21    3.213083  \n",
       "22    0.990744  \n",
       "23    1.467390  \n",
       "24    1.736816  \n",
       "25    1.635923  \n",
       "26    2.853302  \n",
       "27    0.033745  \n",
       "28   -0.651229  \n",
       "29    1.065143  \n",
       "...        ...  \n",
       "5908  0.878303  \n",
       "5909  1.958326  \n",
       "5910  3.050351  \n",
       "5911 -0.170475  \n",
       "5912  1.872452  \n",
       "5913 -0.511306  \n",
       "5914 -0.814250  \n",
       "5915 -0.722852  \n",
       "5916  0.151958  \n",
       "5917 -0.548611  \n",
       "5918  0.654347  \n",
       "5919  0.564426  \n",
       "5920  1.603661  \n",
       "5921  0.572603  \n",
       "5922 -0.127259  \n",
       "5923  0.205245  \n",
       "5924  0.440658  \n",
       "5925 -0.001231  \n",
       "5926  0.261785  \n",
       "5927  0.017394  \n",
       "5928  1.094432  \n",
       "5929 -0.283377  \n",
       "5930  0.763971  \n",
       "5931  0.813567  \n",
       "5932  1.151952  \n",
       "5933  2.366565  \n",
       "5934  0.148209  \n",
       "5935  1.525799  \n",
       "5936 -0.975688  \n",
       "5937  1.507319  \n",
       "\n",
       "[5938 rows x 8 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_items_df = []\n",
    "for i in range(df.shape[0]):\n",
    "    users_items_df += df['state_history'][i] \n",
    "    \n",
    "users_items_df = pd.DataFrame(users_items_df,columns = ['user_id','item_id',\n",
    "                                                        'price','v1','v2','v3','v4','v5'])\n",
    "users_items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate the 'v5'\n",
    "new_users_items_df = users_items_df.drop(columns='v5').drop_duplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the first two covariates variables are features about users \n",
    "# and the forth and fifth variable are features about items\n",
    "# The variable 5 depends on .....\n",
    "\n",
    "user_v1 = users_items_df['v1'].unique()\n",
    "user_v2 = users_items_df['v2'].unique()\n",
    "users_ids = users_items_df['user_id'].unique()\n",
    "users_content= pd.DataFrame({'user_id' : users_ids,'v1':user_v1,'v2':user_v2 })\n",
    "\n",
    "item_v3 = users_items_df['v3'].unique()\n",
    "item_v4 = users_items_df['v4'].unique()\n",
    "item_id = users_items_df['item_id'].unique()\n",
    "items_content= pd.DataFrame({'item_id' : item_id,'v3':item_v3,'v4':item_v4 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>2.672519</td>\n",
       "      <td>2.710697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>0.156198</td>\n",
       "      <td>1.211929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>3.579980</td>\n",
       "      <td>-0.429931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>0.135647</td>\n",
       "      <td>3.574934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>-0.515371</td>\n",
       "      <td>0.407560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>61</td>\n",
       "      <td>2.040257</td>\n",
       "      <td>-0.485274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0.230367</td>\n",
       "      <td>1.841019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35</td>\n",
       "      <td>3.205260</td>\n",
       "      <td>-0.386073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>93</td>\n",
       "      <td>1.900613</td>\n",
       "      <td>0.735791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>83</td>\n",
       "      <td>0.137273</td>\n",
       "      <td>1.274005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41</td>\n",
       "      <td>-0.431000</td>\n",
       "      <td>0.668752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>95</td>\n",
       "      <td>1.836884</td>\n",
       "      <td>1.117188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>43</td>\n",
       "      <td>-0.477873</td>\n",
       "      <td>2.412459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18</td>\n",
       "      <td>2.639947</td>\n",
       "      <td>1.266837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.009381</td>\n",
       "      <td>3.642123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>79</td>\n",
       "      <td>0.607514</td>\n",
       "      <td>0.461494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>33</td>\n",
       "      <td>0.327233</td>\n",
       "      <td>2.524449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>72</td>\n",
       "      <td>-0.298992</td>\n",
       "      <td>2.389939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>67</td>\n",
       "      <td>0.549574</td>\n",
       "      <td>-0.934782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>89</td>\n",
       "      <td>1.109672</td>\n",
       "      <td>0.545934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>96</td>\n",
       "      <td>1.025428</td>\n",
       "      <td>-0.532677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>69</td>\n",
       "      <td>0.146449</td>\n",
       "      <td>0.896601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>0.929872</td>\n",
       "      <td>2.894725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>56</td>\n",
       "      <td>0.027442</td>\n",
       "      <td>1.519521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>42</td>\n",
       "      <td>2.861919</td>\n",
       "      <td>1.055548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45</td>\n",
       "      <td>0.851560</td>\n",
       "      <td>-0.482976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>65</td>\n",
       "      <td>1.257400</td>\n",
       "      <td>0.800394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>51</td>\n",
       "      <td>-1.204015</td>\n",
       "      <td>2.029052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>39</td>\n",
       "      <td>0.199912</td>\n",
       "      <td>0.406030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>54</td>\n",
       "      <td>1.639112</td>\n",
       "      <td>-0.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>74</td>\n",
       "      <td>1.416991</td>\n",
       "      <td>0.725006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>88</td>\n",
       "      <td>1.762699</td>\n",
       "      <td>0.613780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>47</td>\n",
       "      <td>2.087120</td>\n",
       "      <td>1.654582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>13</td>\n",
       "      <td>0.993131</td>\n",
       "      <td>0.339586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>71</td>\n",
       "      <td>-0.020830</td>\n",
       "      <td>-0.030885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>78</td>\n",
       "      <td>1.632613</td>\n",
       "      <td>0.958406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>46</td>\n",
       "      <td>-1.226725</td>\n",
       "      <td>-0.152244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>22</td>\n",
       "      <td>0.034114</td>\n",
       "      <td>0.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>2.350821</td>\n",
       "      <td>1.771666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>66</td>\n",
       "      <td>0.505959</td>\n",
       "      <td>0.069411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>25</td>\n",
       "      <td>0.451918</td>\n",
       "      <td>0.854401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>48</td>\n",
       "      <td>2.531823</td>\n",
       "      <td>2.637015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.499437</td>\n",
       "      <td>1.555248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>52</td>\n",
       "      <td>1.233251</td>\n",
       "      <td>-0.632895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>14</td>\n",
       "      <td>0.813392</td>\n",
       "      <td>1.837534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>40</td>\n",
       "      <td>0.891917</td>\n",
       "      <td>0.164199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>44</td>\n",
       "      <td>0.377863</td>\n",
       "      <td>2.360609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20</td>\n",
       "      <td>2.564918</td>\n",
       "      <td>2.498060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>57</td>\n",
       "      <td>-0.155383</td>\n",
       "      <td>0.833151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>34</td>\n",
       "      <td>-0.466264</td>\n",
       "      <td>0.356116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>99</td>\n",
       "      <td>0.504383</td>\n",
       "      <td>2.387736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>63</td>\n",
       "      <td>1.147589</td>\n",
       "      <td>1.797165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>81</td>\n",
       "      <td>2.131442</td>\n",
       "      <td>1.111330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>75</td>\n",
       "      <td>0.617817</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6</td>\n",
       "      <td>0.967873</td>\n",
       "      <td>2.314183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>24</td>\n",
       "      <td>2.936860</td>\n",
       "      <td>0.393374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>27</td>\n",
       "      <td>1.797962</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>53</td>\n",
       "      <td>0.838922</td>\n",
       "      <td>0.488226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>58</td>\n",
       "      <td>0.302343</td>\n",
       "      <td>0.822130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>4</td>\n",
       "      <td>1.431921</td>\n",
       "      <td>2.641574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id        v1        v2\n",
       "0         8  2.672519  2.710697\n",
       "1        29  0.156198  1.211929\n",
       "2        23  3.579980 -0.429931\n",
       "3        49  0.135647  3.574934\n",
       "4        50 -0.515371  0.407560\n",
       "5        61  2.040257 -0.485274\n",
       "6        21  0.230367  1.841019\n",
       "7        35  3.205260 -0.386073\n",
       "8        93  1.900613  0.735791\n",
       "9        83  0.137273  1.274005\n",
       "10       41 -0.431000  0.668752\n",
       "11       95  1.836884  1.117188\n",
       "12       43 -0.477873  2.412459\n",
       "13       18  2.639947  1.266837\n",
       "14        5 -0.009381  3.642123\n",
       "15       79  0.607514  0.461494\n",
       "16       33  0.327233  2.524449\n",
       "17       72 -0.298992  2.389939\n",
       "18       67  0.549574 -0.934782\n",
       "19       89  1.109672  0.545934\n",
       "20       96  1.025428 -0.532677\n",
       "21       69  0.146449  0.896601\n",
       "22        3  0.929872  2.894725\n",
       "23       56  0.027442  1.519521\n",
       "24       42  2.861919  1.055548\n",
       "25       45  0.851560 -0.482976\n",
       "26       65  1.257400  0.800394\n",
       "27       51 -1.204015  2.029052\n",
       "28       39  0.199912  0.406030\n",
       "29       54  1.639112 -0.478000\n",
       "..      ...       ...       ...\n",
       "61       74  1.416991  0.725006\n",
       "62       88  1.762699  0.613780\n",
       "63       47  2.087120  1.654582\n",
       "64       13  0.993131  0.339586\n",
       "65       71 -0.020830 -0.030885\n",
       "66       78  1.632613  0.958406\n",
       "67       46 -1.226725 -0.152244\n",
       "68       22  0.034114  0.169200\n",
       "69       70  2.350821  1.771666\n",
       "70       66  0.505959  0.069411\n",
       "71       25  0.451918  0.854401\n",
       "72       48  2.531823  2.637015\n",
       "73        9 -0.499437  1.555248\n",
       "74       52  1.233251 -0.632895\n",
       "75       14  0.813392  1.837534\n",
       "76       40  0.891917  0.164199\n",
       "77       44  0.377863  2.360609\n",
       "78       20  2.564918  2.498060\n",
       "79       57 -0.155383  0.833151\n",
       "80       34 -0.466264  0.356116\n",
       "81       99  0.504383  2.387736\n",
       "82       63  1.147589  1.797165\n",
       "83       81  2.131442  1.111330\n",
       "84       75  0.617817  0.002653\n",
       "85        6  0.967873  2.314183\n",
       "86       24  2.936860  0.393374\n",
       "87       27  1.797962  0.005900\n",
       "88       53  0.838922  0.488226\n",
       "89       58  0.302343  0.822130\n",
       "90        4  1.431921  2.641574\n",
       "\n",
       "[91 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't have all the metadata from all the users\n",
    "users_content['user_id'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df[df[\"rewards_history\"] > 0].reset_index(drop=True)\n",
    "neg_df = df[df[\"rewards_history\"] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>468.002443</td>\n",
       "      <td>[[49, 0, 45.63728801221079, 0.1356474524899955...</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>976.646282</td>\n",
       "      <td>[[21, 0, 45.63728801221079, 0.2303667552366982...</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>154.396372</td>\n",
       "      <td>[[35, 0, 45.63728801221079, 3.2052596765490757...</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>990.309864</td>\n",
       "      <td>[[5, 0, 45.63728801221079, -0.0093813677387590...</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>662.539654</td>\n",
       "      <td>[[49, 0, 45.63728801221079, 0.1356474524899955...</td>\n",
       "      <td>49</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0               7       468.002443   \n",
       "1               2       976.646282   \n",
       "2              25       154.396372   \n",
       "3              13       990.309864   \n",
       "4              15       662.539654   \n",
       "\n",
       "                                       state_history  user_id  item_id_action  \n",
       "0  [[49, 0, 45.63728801221079, 0.1356474524899955...       49               7  \n",
       "1  [[21, 0, 45.63728801221079, 0.2303667552366982...       21               2  \n",
       "2  [[35, 0, 45.63728801221079, 3.2052596765490757...       35              25  \n",
       "3  [[5, 0, 45.63728801221079, -0.0093813677387590...        5              13  \n",
       "4  [[49, 0, 45.63728801221079, 0.1356474524899955...       49              16  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df[\"user_id\"] = [pos_df[\"state_history\"][i][0][0] for i in range(pos_df.shape[0])]\n",
    "pos_df[\"item_id_action\"] = [pos_df[\"state_history\"][i][pos_df[\"action_history\"][i]][1] for i in range(pos_df.shape[0])]\n",
    "pos_df.head()## There are some users that have more than one positive item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[8, 0, 45.63728801221079, 2.6725193925993143,...</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[29, 0, 45.63728801221079, 0.1561980677443061...</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[23, 0, 45.63728801221079, 3.5799797302261704...</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[50, 0, 45.63728801221079, -0.515370624544243...</td>\n",
       "      <td>50</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[61, 0, 45.63728801221079, 2.040256686435062,...</td>\n",
       "      <td>61</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0              20              0.0   \n",
       "1               2              0.0   \n",
       "2              26              0.0   \n",
       "3              24              0.0   \n",
       "4              16              0.0   \n",
       "\n",
       "                                       state_history  user_id  item_id_action  \n",
       "0  [[8, 0, 45.63728801221079, 2.6725193925993143,...        8              20  \n",
       "1  [[29, 0, 45.63728801221079, 0.1561980677443061...       29               2  \n",
       "2  [[23, 0, 45.63728801221079, 3.5799797302261704...       23              26  \n",
       "3  [[50, 0, 45.63728801221079, -0.515370624544243...       50              24  \n",
       "4  [[61, 0, 45.63728801221079, 2.040256686435062,...       61              16  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df[\"user_id\"] = [neg_df[\"state_history\"][i][0][0] for i in range(neg_df.shape[0])]\n",
    "neg_df[\"item_id_action\"] = [neg_df[\"state_history\"][i][neg_df[\"action_history\"][i]][1] for i in range(neg_df.shape[0])]\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"Ignore y_true and return the mean of y_pred\n",
    "    \n",
    "    This is a hack to work-around the design of the Keras API that is\n",
    "    not really suited to train networks with a triplet loss by default.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(y_pred + 0 * y_true)\n",
    "\n",
    "\n",
    "def margin_comparator_loss(inputs, margin=1.):\n",
    "    \"\"\"Comparator loss for a pair of precomputed similarities\n",
    "    \n",
    "    If the inputs are cosine similarities, they each have range in\n",
    "    (-1, 1), therefore their difference have range in (-2, 2). Using\n",
    "    a margin of 1. can therefore make sense.\n",
    "\n",
    "    If the input similarities are not normalized, it can be beneficial\n",
    "    to use larger values for the margin of the comparator loss.\n",
    "    \"\"\"\n",
    "    positive_pair_sim, negative_pair_sim = inputs\n",
    "    return tf.maximum(negative_pair_sim - positive_pair_sim + margin, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model : Recommand the most expensive item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model_expensive(state):\n",
    "    greater_index = 0\n",
    "    greater_value = 0\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if(state[i][2] > greater_value):\n",
    "            greater_value = state[i][2]\n",
    "            greater_index = i\n",
    "    return greater_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ID = 'Y6EKWA0GK1D0VCTN0RT7'\n",
    "BASE_URL = \"http://35.180.178.243\"\n",
    "url_reset = BASE_URL + \"/reset\"\n",
    "url_predict = BASE_URL + \"/predict\"\n",
    "params = {\"user_id\" : USER_ID}\n",
    "\n",
    "def load_data(url_reset,params) :\n",
    "    r = requests.get(url=url_reset, params=params)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "def build_dataframe(data) : \n",
    "    df = pd.DataFrame({\"action\":data[\"action_history\"], \"reward\":data[\"rewards_history\"], \"item\":data[\"state_history\"]})\n",
    "    return df\n",
    "\n",
    "def prepare_data(df) : \n",
    "    x, y = [], []\n",
    "    for action, reward, item in zip(df[\"action\"], df[\"reward\"], df[\"item\"]) : \n",
    "        x.append(item[action][2:]) # We do not take care of the item_id and user_id\n",
    "        y.append(reward > 0) # True if reward > 0 else False\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward at run 1 : 221586 with conversion rate 0.235\n",
      "Total reward at run 2 : 213815 with conversion rate 0.242\n",
      "Total reward at run 3 : 210671 with conversion rate 0.232\n",
      "Mean of total reward over 3 independant runs : 215357\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 1000\n",
    "mean = 0\n",
    "\n",
    "for k in range(1, number_run+1):\n",
    "\n",
    "    # Load the data\n",
    "    params = {\"user_id\" : USER_ID}\n",
    "    data = load_data(url_reset, params)\n",
    "    # Build the dataframe and prepare the data\n",
    "    df = build_dataframe(data)\n",
    "    x, y = prepare_data(df)\n",
    "    next_state = data[\"next_state\"]\n",
    "\n",
    "    total_reward = 0\n",
    "    purchase = 0\n",
    "    \n",
    "    for i in range(generations):\n",
    "\n",
    "        predicted_item = naive_model_expensive(next_state)\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        if reward > 0 :\n",
    "            purchase += 1\n",
    "\n",
    "    mean += total_reward\n",
    "\n",
    "    print(\"Total reward at run %d : %d with conversion rate %.3f\" % (k, total_reward, purchase/generations))\n",
    "print(\"Mean of total reward over 3 independant runs : %d\" % (mean/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model : Recommand The cheapest item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model_cheapest(state):\n",
    "    cheapest_index = 0\n",
    "    cheapest_value =  state[0][2]\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if(state[i][2] < cheapest_value):\n",
    "            cheapest_value = state[i][2]\n",
    "            cheapest_index = i\n",
    "            \n",
    "    return cheapest_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward at run 1 : 10406 with conversion rate 0.299\n",
      "Total reward at run 2 : 32463 with conversion rate 0.262\n",
      "Total reward at run 3 : 14882 with conversion rate 0.226\n",
      "Mean of total reward over 3 independant runs : 19250\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 1000\n",
    "mean = 0\n",
    "\n",
    "for k in range(1, number_run+1):\n",
    "\n",
    "    # Load the data\n",
    "    params = {\"user_id\" : USER_ID}\n",
    "    data = load_data(url_reset, params)\n",
    "    # Build the dataframe and prepare the data\n",
    "    df = build_dataframe(data)\n",
    "    x, y = prepare_data(df)\n",
    "    next_state = data[\"next_state\"]\n",
    "\n",
    "    total_reward = 0\n",
    "    purchase = 0\n",
    "\n",
    "    for i in range(generations):\n",
    "\n",
    "        predicted_item = naive_model_cheapest(next_state)\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        if reward > 0 : \n",
    "            purchase += 1\n",
    "\n",
    "    mean += total_reward\n",
    "\n",
    "    print(\"Total reward at run %d : %d with conversion rate %.3f\" % (k, total_reward, purchase/generations))\n",
    "print(\"Mean of total reward over 3 independant runs : %d\" % (mean/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    As naive models, the strategy of always recommending the most expensive item (in order to get the biggest reward) always works best. Indeed, the conversion rate is about the same for these two strategies but \"Most Expensive Model\" bring more rewards. We will now try the deep models seen in class.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (without covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, Dense\n",
    "from keras.layers import Lambda, Dot\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import dot, concatenate\n",
    "\n",
    "\n",
    "def build_models(n_users, n_items, latent_dim=64, l2_reg=0):\n",
    "    \"\"\"Build a triplet model and its companion similarity model\n",
    "    \n",
    "    The triplet model is used to train the weights of the companion\n",
    "    similarity model. The triplet model takes 1 user, 1 positive item\n",
    "    (relative to the selected user) and one negative item and is\n",
    "    trained with comparator loss.\n",
    "    \n",
    "    The similarity model takes one user and one item as input and return\n",
    "    compatibility score (aka the match score).\n",
    "    \"\"\"\n",
    "    # Common architectural components for the two models:\n",
    "    # - symbolic input placeholders\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "\n",
    "    # - embeddings\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(input_dim=n_users + 1, output_dim=latent_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "    \n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(input_dim=n_items + 1, output_dim=latent_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # - similarity computation between embeddings\n",
    "    positive_similarity = Dot(name=\"positive_similarity\",\n",
    "                              axes=1, normalize=True)(\n",
    "        [user_embedding, positive_item_embedding])\n",
    "    negative_similarity = Dot(name=\"negative_similarity\",\n",
    "                              axes=1, normalize=True)(\n",
    "        [user_embedding, negative_item_embedding])\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss,\n",
    "                          name='comparator_loss',\n",
    "                          output_shape=(1,))([positive_similarity, negative_similarity])\n",
    "\n",
    "    triplet_model = Model(inputs=[user_input,\n",
    "                                  positive_item_input,\n",
    "                                  negative_item_input],\n",
    "                          outputs=triplet_loss)\n",
    "    \n",
    "    # The match-score model, only use at inference to rank items for a given\n",
    "    # model: the model weights are shared with the triplet_model therefore\n",
    "    # we do not need to train it and therefore we do not need to plug a loss\n",
    "    # and an optimizer.\n",
    "    match_model = Model(inputs=[user_input, positive_item_input],\n",
    "                        outputs=positive_similarity)\n",
    "    \n",
    "    return triplet_model, match_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_df,max_items):\n",
    "    \"\"\"\n",
    "    Sample negatives at random\n",
    "    \n",
    "    \"\"\"\n",
    "    range_itens = np.arange(max_items)\n",
    "    user_ids = pos_df['user_id'].values\n",
    "    pos_item_ids = pos_df['item_id_action'].values\n",
    "    \n",
    "    \n",
    "    user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "    neg_item_ids = np.array([])\n",
    "    for i in user_ids:\n",
    "        if i in set(user_pos_items.index):\n",
    "            number = np.random.choice([item for item in range_itens \n",
    "                                       if not item in user_pos_items[user_pos_items.index == i]])\n",
    "        else:\n",
    "            number = np.random.choice(range_itens,1)\n",
    "        neg_item_ids = np.append(neg_item_ids,number)\n",
    "    \n",
    "  \n",
    "    return [user_ids, pos_item_ids, neg_item_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.1009\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2157\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2438\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2358\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4087\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4495\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8381\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6618\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6220\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9928\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5312\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1299\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0142\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0745\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7267\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3275\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9002\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2617\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5562\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6481\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6797\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7902\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8834\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6319\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6958\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1384\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5886\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5978\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3994\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9281\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3317\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7441\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6873\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5615\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6716\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3384\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8661\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6140\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7555\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6735\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4778\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9571\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1657\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6837e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2921\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8414\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2994\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9193\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0793\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4315\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9413\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9700\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3544\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2355\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2760\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3043\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4825\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0989\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9920\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3267\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5129\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5864\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6201\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4787\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6544e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1362\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7607\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6215\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2902\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9760\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6494e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2329\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5804\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4889\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0944\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2833\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1589\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0525\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4525\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1904\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5374\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6557\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7359\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8017\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7611\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8907\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9017\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6498\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2133\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8816\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7501\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0728\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9091\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2882\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5398\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7315\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0530\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6454\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3785\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6720\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9358\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4517\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6341e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2506\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2525\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4108\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5459\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4955\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9359\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7288\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3282\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0863\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7999\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9424\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7050\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3083\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3848\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0700\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5488\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6404e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2083\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8122\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4519\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2228\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2157\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6458\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1323\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5560\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0869\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3734\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4599\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9655\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6579\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0292\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2630\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0422\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0748\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3029\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5711\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0155\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8684\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8647\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7894\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9921\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8945\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7888\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2750\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5188\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2700\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6165\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5336\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5043\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1921\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3879\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3662\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4649\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1750\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6736e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9527\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2679\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9875\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0922\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6589\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7707\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6449\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0677\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9603\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3873\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0373\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7902\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8540\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4289\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6734\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7651\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5022\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5076\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1614\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8696\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8794\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7795\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1855\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5305\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0108\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9487\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9702\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5744\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4405\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0853\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4973\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2165\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6167\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4737\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1770\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0425\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1138\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4885\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3988\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9368\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1001\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7703\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1458\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2354\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6466\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8950\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3872\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3216\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4993\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5022\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8280\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1621\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7431\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8452\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9991\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3979\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8714\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0482\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6948\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2415\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5524\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5533\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8538\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0761\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1350\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9768\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6148\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7542\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5469\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3396\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7231\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6196\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8720\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3889\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5089\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7891\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5185\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6714\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3460\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4570\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5673\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1435\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2292\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7969\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7995\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2617\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9011\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1113\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8054\n",
      "Total reward at run 0 : 118653 with conversion rate 0.254\n",
      "Epoch 1/1\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.0275\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2242\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8625\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3908\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7261\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6155\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4305\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6166\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8516\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1805\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7994\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6886\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7541\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1693\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2059\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7614\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4850\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2879\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3280\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5136\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4060\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7016e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0909\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1762\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8178\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2408\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3181\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5952\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8589\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5757\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7176\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5589\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1551\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1718\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3498\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1165\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1810\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0735\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4673\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9878\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5946\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1647\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6588\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9793\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4187\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7256\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1903\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2444\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9842\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8988\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9957\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2298\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6387e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5752\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3498\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5681\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9795\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7901\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2635\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4740\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2299\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9026\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6658\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1917\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5063\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0701\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2258\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6329\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5869\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3405\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7392\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3407\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4447\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7976\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3825\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2382\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3595\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3385\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4786\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9804\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6087\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2065\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6973\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7046\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7068\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6050\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7595\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1570\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0651\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1674\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5711\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7623\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1166\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6228\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2906\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8383\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4499\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7353\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5053\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5362\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6085\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3991\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3015\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5430\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9992\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3743\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9831\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5298\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3783\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5613\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9861\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2084\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1465\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6256\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1926\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3498\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3527\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5919\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0592\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7016\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5143\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5934\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9658\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7924\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7714\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7196\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7092\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6049e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0012\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4200\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7776\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3612\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7163\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2997\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1206\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3051\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7802\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6523\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1803\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6293\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3770\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8300\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6166e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1874\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9720\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8904\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6196e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8415\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5611\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6428\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2634\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3535\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1040\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2758\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9596\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3015\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0272\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3026\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2664\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3983\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2959\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6691\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5787\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9142\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3484\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1258\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5140\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7311\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4456\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3753\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1990\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3913\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4386\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8805\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3134\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4522\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6075\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7638\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4217\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0144\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3518\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6190\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9640\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0075\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3742\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4292\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3951\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8808\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2143\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7224\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6448\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2233\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8555\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7435\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2453\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3940\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8164\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0128\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9105\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6103\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9479\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5705\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5226\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0594\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5626\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1849\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0726\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2717\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2249\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3356\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6310\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7717\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7312\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1673\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2939\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4554\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5450\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2529\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8272\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9979\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9417\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0080\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5913\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7659\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1985\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3752\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7258\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4505\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7557\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1261\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5512\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9222\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4001\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1585\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4870\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8179\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8004\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2278\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7012\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8611\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2408\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5410\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1835\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2566\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9363\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2421\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9235\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1525\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0697\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1809\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2852\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9082\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5778\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2809\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4905\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0892\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0695\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0756\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7624\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2212\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2809\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1513\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9399\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5327\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9879\n",
      "Total reward at run 1 : 128638 with conversion rate 0.272\n",
      "Epoch 1/1\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.9570\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4257\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3347\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7825\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8003e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2131\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8937\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7820\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6360\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2532\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8428\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4934\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0674\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7948\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0021\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7608\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6420\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5305\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4392\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3878\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8989\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7465\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1299\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7463\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0533\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6753\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2614\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5989\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7315\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1819\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6335\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8295\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4095\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0688\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4618\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8180\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9805\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8624\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6933\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6665\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3743\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9411\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1430\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3757\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5093\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2964\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3076\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0240\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2517\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5675\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3908\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5138\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5543\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2941\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4945\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0365\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3516\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6777\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7963\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2270\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7724\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6975\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8117\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7604\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7925\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2568\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8254\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3135\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6626e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1650\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3325\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8806\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7226\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5758\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7407\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5606\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6748\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3857\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7503\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8071\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3752\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8327\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5531\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8681\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7455\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8475\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1815\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6404\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3653\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0808\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6124\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5623\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9978\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8907\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1977\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6233\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4857\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2833\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0035\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7125\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6287\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9782\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3967\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0769\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5510\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3265\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6906\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5610\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7140\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0338\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9958\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4735\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9329\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5057\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2095\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4683\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6232\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4767\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6205\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0676\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3801\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6225\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3274\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5092\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2682\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6670\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3611\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1690\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0057\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2762\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2578\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1469\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6945\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4556\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9936\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4185\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5394\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8471\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3237\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3971\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2585\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2956\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8479\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6795e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2811\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5835\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6839e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7165\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9135\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0848\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1852\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6329\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2473\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8984\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6161\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4776\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5188\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7200\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2234\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1288\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0094\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3766\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7424\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7282\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9885\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6348\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3253\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0838\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4298\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5847\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3875\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9110\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5913\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0602\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7598\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3742\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1552\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7291\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5800\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3461\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8875\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9488\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1248\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7321\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9066\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0545\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6595\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6201\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7773\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6488\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6231\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7167\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2255\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7457e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0663\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9387\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2477\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5539\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5543\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6320\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8443\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8330\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0567\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2010\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1980\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4062\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1275\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6248\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2439\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5746\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4997\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8907\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1270\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0392\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5675\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2840\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9533\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8377\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9013\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5810\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6593\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1109\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9199\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1951\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4273\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5504\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5603\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5730\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1501\n",
      "Total reward at run 2 : 111897 with conversion rate 0.242\n",
      "Mean of total reward over 3 independant runs : 119729\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "n_epochs = 1\n",
    "total_reward = 0\n",
    "generations = 1000\n",
    "max_items = 30\n",
    "mean = 0\n",
    "\n",
    "for k in range(number_run):\n",
    "    \n",
    "    # Load the data\n",
    "    params = {\"user_id\" : USER_ID}\n",
    "    data = load_data(url_reset, params)\n",
    "    # Build the dataframe and prepare the data\n",
    "    \n",
    "    pos_df = df[df[\"reward\"] > 0].reset_index(drop=True)\n",
    "    pos_df[\"user_id\"] = [pos_df[\"item\"][i][0][0] for i in range(pos_df.shape[0])]\n",
    "    pos_df[\"item_id_action\"] = [pos_df[\"item\"][i][pos_df[\"action\"][i]][1] for i in range(pos_df.shape[0])]\n",
    "\n",
    "    total_reward = 0\n",
    "    purchase = 0\n",
    "    \n",
    "    triplet_model, match_model = build_models(nb_users, nb_items, latent_dim=16, l2_reg=1e-6)\n",
    "    # we plug the identity loss and the a fake target variable ignored by\n",
    "    # the model to be able to use the Keras API to train the triplet model\n",
    "    triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "    fake_y = np.ones_like(pos_df['user_id'])\n",
    "    params = {'user_id': USER_ID}\n",
    "    \n",
    "    positive_data = pos_df.copy()\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        # Sample new negatives to build different triplets at each epoch\n",
    "        triplet_inputs = sample_triplets(positive_data, max_items=nb_items)\n",
    "\n",
    "        # Fit the model incrementally by doing a single pass over the\n",
    "        # sampled triplets.\n",
    "        triplet_model.fit(triplet_inputs, fake_y, shuffle=True, epochs=1)\n",
    "\n",
    "    for j in range(generations):\n",
    "\n",
    "\n",
    "        # The user that we need to recommend a product\n",
    "        new_user_id = next_state[0][0]\n",
    "        # Take all the items in the new state\n",
    "        items_ids = np.array([next_state[i][1]\n",
    "                              for i in range(len(next_state))])\n",
    "\n",
    "        # Create an array of the same size of 'items_ids' of all values equal to 'new_user_id'\n",
    "        repeated_user_id = np.empty_like(items_ids)\n",
    "        repeated_user_id.fill(new_user_id)\n",
    "\n",
    "        # predict the product using our model\n",
    "        predicted = match_model.predict([repeated_user_id, items_ids])\n",
    "        predicted_item = np.argmax(predicted)\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        # take the reward from the API\n",
    "        r = requests.get(url=url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "\n",
    "        # (Online learning)Check if the reward is greater than 0 , if yes train again \n",
    "        # the model the sample point\n",
    "        if(reward > 0):\n",
    "            purchase += 1\n",
    "            range_itens = np.arange(max_items)\n",
    "            user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "            if new_user_id in set(user_pos_items.index):\n",
    "                neg_item_id = np.random.choice([item for item in range_itens \n",
    "                                           if not item in user_pos_items[user_pos_items.index == new_user_id]])\n",
    "            else:\n",
    "                neg_item_id = np.random.choice(range_itens,1)\n",
    "            \n",
    "           \n",
    "            \n",
    "            triplet_inputs = [[new_user_id], [predicted_item], [neg_item_id]]\n",
    "    \n",
    "            fake_y = np.ones_like([1])\n",
    "            triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size=64, epochs=1)\n",
    "    \n",
    "            \n",
    "        # Take the nexr_state\n",
    "        next_state = r['state']\n",
    "    \n",
    "    mean += total_reward\n",
    "        \n",
    "    print(\"Total reward at run %d : %d with conversion rate %.3f\" % (k, total_reward, purchase/generations))\n",
    "print(\"Mean of total reward over 3 independant runs : %d\" % (mean/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for this model are :\n",
    "- Total reward at run 0 : 118653 with conversion rate 0.254\n",
    "- Total reward at run 1 : 128638 with conversion rate 0.272\n",
    "- Total reward at run 2 : 111897 with conversion rate 0.242\n",
    "- Mean of total reward over 3 independant runs : 119729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    The conversion rate is almost the same with the Dot Model (25% in average of conversion rate). But the most important thing is that The cumulative reward is much lower than with the naive model (Mean of total reward for the most expensive model : 215357). This may be due to the small amount of database in the learning data. Let's try a deeper model and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(triplet_model, to_file='triplet_model.png')\n",
    "Image(filename='triplet_model.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2(with deep learning and covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dense, Dropout\n",
    "from keras.layers import Concatenate, Lambda\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def make_interaction_mlp(input_dim, n_hidden=1, hidden_size=64,\n",
    "                         dropout=0, l2_reg=None):\n",
    "    \"\"\"\n",
    "    Build the shared multi layer perceptron\n",
    "\n",
    "    \"\"\"\n",
    "    mlp = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        # Plug the output unit directly: this is a simple\n",
    "        # linear regression model. Not dropout required.\n",
    "        mlp.add(Dense(1, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "    else:\n",
    "        mlp.add(Dense(hidden_size, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "        mlp.add(Dropout(dropout))\n",
    "        for i in range(n_hidden - 1):\n",
    "            mlp.add(Dense(hidden_size, activation='relu',\n",
    "                          kernel_regularizer=l2_reg))\n",
    "            mlp.add(Dropout(dropout))\n",
    "        mlp.add(Dense(1, activation='relu', kernel_regularizer=l2_reg))\n",
    "    return mlp\n",
    "\n",
    "\n",
    "def build_models(n_users, n_items, user_dim=32, item_dim=64,\n",
    "                 n_hidden=1, hidden_size=64, dropout=0, l2_reg=0):\n",
    "    \"\"\"\n",
    "    Build models to train a deep triplet network\n",
    "\n",
    "    \"\"\"\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "    positive_meta_input = Input((2,), name='positive_meta_input')\n",
    "    negative_meta_input = Input((2,), name='negative_meta_input')\n",
    "\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(n_users + 1, user_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "\n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(n_items + 1, item_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # Similarity computation between embeddings using a MLP similarity\n",
    "    positive_embeddings_pair = Concatenate(name=\"positive_embeddings_pair\")(\n",
    "        [user_embedding, positive_item_embedding, positive_meta_input])\n",
    "    positive_embeddings_pair = Dropout(dropout)(positive_embeddings_pair)\n",
    "\n",
    "    negative_embeddings_pair = Concatenate(name=\"negative_embeddings_pair\")(\n",
    "        [user_embedding, negative_item_embedding, negative_meta_input])\n",
    "    negative_embeddings_pair = Dropout(dropout)(negative_embeddings_pair)\n",
    "\n",
    "    # Instanciate the shared similarity architecture\n",
    "    interaction_layers = make_interaction_mlp(\n",
    "        user_dim + item_dim + 2, n_hidden=n_hidden, hidden_size=hidden_size,\n",
    "        dropout=dropout, l2_reg=l2_reg)\n",
    "\n",
    "    positive_similarity = interaction_layers(positive_embeddings_pair)\n",
    "    negative_similarity = interaction_layers(negative_embeddings_pair)\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss, output_shape=(1,),\n",
    "                          name='comparator_loss')(\n",
    "        [positive_similarity, negative_similarity])\n",
    "\n",
    "    deep_triplet_model = Model(inputs=[user_input,\n",
    "                                       positive_item_input,\n",
    "                                       negative_item_input,\n",
    "                                       positive_meta_input,\n",
    "                                       negative_meta_input\n",
    "                                       ],\n",
    "                               outputs=[triplet_loss])\n",
    "\n",
    "    # The match-score model, only used at inference\n",
    "    deep_match_model = Model(inputs=[user_input, positive_item_input, positive_meta_input],\n",
    "                             outputs=[positive_similarity])\n",
    "\n",
    "    return deep_match_model, deep_triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_df, items_content, max_items):\n",
    "    \"\"\"\n",
    "    Sample the data and put in the format \n",
    "    [user_ids, pos_item_ids, neg_item_ids,pos_item_metadata,neg_item_metadata]\n",
    "\n",
    "    \"\"\"\n",
    "    range_itens = np.arange(max_items)\n",
    "    user_ids = pos_df['user_id'].values\n",
    "    \n",
    "\n",
    "    pos_item_ids = pos_df['item_id_action'].values\n",
    "    pos_item_metadata = items_content.loc[pos_item_ids][['v3', 'v4']].values\n",
    "\n",
    "    \n",
    "    user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "    neg_item_ids = np.array([])\n",
    "    for i in user_ids:\n",
    "        if i in set(user_pos_items.index):\n",
    "            number = np.random.choice([item for item in range_itens \n",
    "                                       if not item in user_pos_items[user_pos_items.index == i]])\n",
    "        else:\n",
    "            number = np.random.choice(range_itens, 1)\n",
    "        neg_item_ids = np.append(neg_item_ids, number)\n",
    "\n",
    "    neg_item_metadata = items_content.loc[neg_item_ids][['v3', 'v4']].values\n",
    "\n",
    "    return [user_ids, pos_item_ids, neg_item_ids, pos_item_metadata, neg_item_metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = dict(\n",
    "    user_dim=50,\n",
    "    item_dim=15,\n",
    "    n_hidden=1,\n",
    "    hidden_size=16,\n",
    "    dropout=0.5,\n",
    "    l2_reg=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.9754\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0068\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0107\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0577\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2578\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9656\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8946\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0277\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0727\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0906\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0368\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0737\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8446\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9830\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3908\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0139\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0112\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7185\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5027\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8360\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5827\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8528\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9615\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3178\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0561\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0576\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8106\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8174\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3899\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0442\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8783\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0830\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9447\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3172\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5541\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9463\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5170\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0746\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5627\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4826\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2384\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5985\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5620\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0684\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9398\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5641\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3217\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8954\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1754\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6416\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7036\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0386\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6454\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2165\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3813\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9145\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5350\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9780\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9975\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9886\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0352\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0718\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0081\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9752\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6401\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7604\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7027\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8054\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2218\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2524\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8049\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5729\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0241\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6705\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0837\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8414\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6026\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4903\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9763\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0462\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0303\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9384\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8148\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6171\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5716\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0934\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3711\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6397\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9366\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8818\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0271\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9451\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7905\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3889\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5120\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9062\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8936\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9403\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7883\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8034\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4428\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7379\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2048\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5891\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2387\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7097\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1305\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1359\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0316\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2932\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1936\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9984\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0309\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7633\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2431\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4014\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2024\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0359\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0705\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5875\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6638\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0782\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8172\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5890\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5372\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1361\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8418\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1154\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7314\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0792\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9007\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0229\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8742\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8597\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9371\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8953\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6728\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4616\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9951\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0313\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0244\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2383\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0226\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1134\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4004\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4426\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0891\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0178\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7305\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7642\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0465\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9257\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1048\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0980\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1573\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8719\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5105\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4350\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3661\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4090\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5182\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2650\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0901\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6790\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0011\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1699\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2885\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1196\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.9138\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4658\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6598\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8420\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8234\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8738\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9704\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0074\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9745\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9985\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7130\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0065\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0223\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1280\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1087\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3374\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5287\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5239\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8191\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0028\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7149\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8880\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1433\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9598\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1746\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4260\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1016\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8684\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8670\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5208\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6575\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8377\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7452\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1982\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7910\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5438\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4208\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6174\n",
      "Total reward at run 0 : 118306 with conversion rate 0.256\n",
      "Epoch 1/1\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.0893\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2240\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1396\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4200\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9568\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9681\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9425\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3624\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0345\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3903\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9774\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0316\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0961\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8401\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0430\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2072\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1159\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0286\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0712\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0027\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3821\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9180\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1415\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0516\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0417\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9867\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0121\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8616\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1485\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0901\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2881\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5921\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9958\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6369\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3926\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8904\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0596\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1435\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5203\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6163\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1299\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9897\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8303\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4591\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2236\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0465\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9437\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0282\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7519\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0703\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9610\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9188\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6032\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0901\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9611\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2806\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0236\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9985\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0545\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2298\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1116\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9751\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0129\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1594\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9354\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9997\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1577\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0601\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0119\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1388\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2621\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0214\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0180\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9132\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0056\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9886\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7845\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2272\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0247\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9482\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9342\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1216\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3655\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8470\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0746\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0679\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4022\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9081\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8661\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0141\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0437\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9112\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1499\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1034\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7219\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7682\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0097\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9604\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9214\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9426\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9731\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2832\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9112\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2201\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9451\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2258\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9257\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6283\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9845\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0884\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2047\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0133\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9426\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8996\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9268\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0497\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9630\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0197\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0239\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1962\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9597\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0083\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0030\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9484\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0309\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1820\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9266\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7264\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6323\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9625\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7974\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0597\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3820\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8312\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0210\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0437\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9189\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9363\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8950\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9910\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6120\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2175\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0412\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7213\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0241\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0226\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7406\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7699\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0748\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9019\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0531\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0133\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0277\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0129\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0356\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7298\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1307\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1181\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9947\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0087\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0048\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4242\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8805\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2570\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2541\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9736\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1321\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1205\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9275\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1374\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6509\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2295\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7867\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4373\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9982\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0787\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9019\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9337\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0877\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9738\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0083\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0059\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1689\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9974\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0290\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1075\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7825\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1004\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8133\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1331\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4194\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8502\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0145\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8759\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0063\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9593\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7858\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8932\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7258\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8647\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0688\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8120\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4394\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0388\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0698\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9725\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9354\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6271\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0064\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8779\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0079\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6940\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1039\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9931\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0060\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1013\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9321\n",
      "Total reward at run 1 : 153219 with conversion rate 0.285\n",
      "Epoch 1/1\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 1.0095\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8172\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0353\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0960\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0391\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8807\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8031\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5550\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6519\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9981\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0272\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0689\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9059\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9421\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9517\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7228\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9399\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9115\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8801\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8201\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9093\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9686\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8957\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0858\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0765\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0380\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9106\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0147\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0306\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1037\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9668\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0939\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9173\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9828\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0268\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9085\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8104\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1809\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2265\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9559\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8133\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9626\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0293\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0697\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0345\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4096\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1443\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4979\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9530\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9124\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2281\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9997\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0497\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8656\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6699\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9997\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9664\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5659\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9742\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8795\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9365\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3268\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0202\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9681\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1183\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4467\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5697\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0097\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0603\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8727\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0763\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1980\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5952\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8266\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8187\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9853\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9286\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6506\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0543\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9551\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2851\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1632\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0628\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7672\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8005\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5673\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0301\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0287\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8837\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8042\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1303\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7791\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8976\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4892\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2193\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2274\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5394\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8869\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0206\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0220\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1147\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4695\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0427\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2165\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1399\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9770\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1198\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9652\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3077\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9318\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9109\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1847\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4121\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2806\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9169\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7718\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8873\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2514\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9107\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0529\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7999\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8245\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0802\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9443\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7967\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8522\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5420\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8524\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2673\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9131\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9791\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2070\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3742\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7665\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1236\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0664\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3246\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0139\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0651\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8291\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0384\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8684\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9840\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1170\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9577\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8137\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1776\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0630\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0293\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7178\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9083\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2776\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7384\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2753\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Total reward at run 2 : 121467 with conversion rate 0.237\n",
      "Mean of total reward over 3 independant runs : 130997\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "n_epochs = 1\n",
    "total_reward = 0\n",
    "generations = 1000\n",
    "max_items = 30 \n",
    "mean = 0\n",
    "\n",
    "for k in range(number_run):\n",
    "    \n",
    "    # Load the data\n",
    "    params = {\"user_id\" : USER_ID}\n",
    "    data = load_data(url_reset, params)\n",
    "    # Build the dataframe and prepare the data\n",
    "    \n",
    "    pos_df = df[df[\"reward\"] > 0].reset_index(drop=True)\n",
    "    pos_df[\"user_id\"] = [pos_df[\"item\"][i][0][0] for i in range(pos_df.shape[0])]\n",
    "    pos_df[\"item_id_action\"] = [pos_df[\"item\"][i][pos_df[\"action\"][i]][1] for i in range(pos_df.shape[0])]\n",
    "\n",
    "    total_reward = 0\n",
    "    purchase = 0\n",
    "\n",
    "    deep_match_model, deep_triplet_model = build_models(nb_users, nb_items,**hyper_parameters)\n",
    "    # we plug the identity loss and the a fake target variable ignored by\n",
    "    # the model to be able to use the Keras API to train the triplet model\n",
    "    deep_triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "    fake_y = np.ones_like(pos_df['user_id'])\n",
    "    \n",
    "    positive_data = pos_df.copy()\n",
    "    \n",
    "    # Training the model \n",
    "    for i in range(n_epochs):\n",
    "        # Sample new negatives to build different triplets at each epoch\n",
    "        triplet_inputs = sample_triplets(positive_data,items_content, max_items=nb_items)\n",
    "\n",
    "        # Fit the model incrementally by doing a single pass over the\n",
    "        # sampled triplets.\n",
    "        deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size=64, epochs=1)\n",
    "\n",
    "\n",
    "    for j in range(generations):\n",
    "        \n",
    "        # Predicting\n",
    "        new_user_id = next_state[0][0]\n",
    "        items_ids = np.array([next_state[i][1] for i in range(len(next_state))])\n",
    "        repeated_user_id = np.empty_like(items_ids)\n",
    "        repeated_user_id.fill(new_user_id)\n",
    "\n",
    "        pos_metadata = items_content.loc[items_ids][['v3', 'v4']].values\n",
    "        predicted = deep_match_model.predict([repeated_user_id, items_ids, pos_metadata])\n",
    "        predicted_item = np.argmax(predicted)\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url=url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "\n",
    "        # Online learning\n",
    "        if(reward > 0):\n",
    "            purchase += 1\n",
    "            range_itens = np.arange(max_items)\n",
    "            user_pos_items = positive_data.groupby('user_id')['item_id_action'].apply(list)\n",
    "            if new_user_id in set(user_pos_items.index):\n",
    "                neg_item_id = np.random.choice([item for item in range_itens \n",
    "                                           if not item in user_pos_items[user_pos_items.index == new_user_id]])\n",
    "            else:\n",
    "                neg_item_id = np.random.choice(range_itens,1)\n",
    "            \n",
    "            pos_item_metadata = items_content.set_index('item_id').loc[predicted_item].values\n",
    "            neg_item_metadata = items_content.set_index('item_id').loc[neg_item_id].values\n",
    "            \n",
    "            triplet_inputs = [[new_user_id], [predicted_item], [neg_item_id], \n",
    "                              pos_item_metadata.reshape((1,2)), neg_item_metadata.reshape((1,2))]\n",
    "    \n",
    "            fake_y = np.ones_like([1])\n",
    "            deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size = 8, epochs=1)\n",
    "\n",
    "        next_state = r['state']\n",
    "        \n",
    "    mean += total_reward\n",
    "    \n",
    "    print(\"Total reward at run %d : %d with conversion rate %.3f\" % (k, total_reward, purchase/generations))\n",
    "    \n",
    "print(\"Mean of total reward over 3 independant runs : %d\" % (mean/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIcAAAKECAYAAACdNVoPAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdeVxWZf7/8ffN4r7lmo57ue+iuKC53eASmKlA40KbYuaU2aZ9p8XGJnWmppweVmI5jeUGpiaWmpgbqaUo4tbkVqNoIi6oKAh4/f7oxz2RKDcKHG7u1/Px6DFzn3O4z9vr3Pd1Lj6ccx2bMcYIAAAAAAAA7ijKw+oEAAAAAAAAsA7FIQAAAAAAADdGcQgAAAAAAMCNURwCAAAAAABwY16/X7Bt2zb94x//sCILgGLk2WefVbdu3QrlvelnAOSmMPsduBfOM3BHUVFRVkcA4MJuuHLo+PHjWrp0qRVZABQTS5cu1fHjxwvt/elnAPxeYfc7cC+cZ+BOTpw4wecdwB274cqhbFSeAfdls9mKZD/0MwCyFVW/A/fCeQbuIDIyUqGhoVbHAODimHMIAAAAAADAjVEcAgAAAAAAcGMUhwAAAAAAANwYxSEAAAAAAAA3RnEIAAAAAADAjVEcAgAAAAAAcGMUhwAAAAAAANwYxSEAAAAAAAA3RnEIAAAAAADAjVEcAgAAAAAAcGMUhwAAAAAAANwYxSEAAAAAAAA3RnEIAAAAAADAjVEcAgAAAAAAcGMuUxxasWKF6tWrp4MHDxbIdoXByn3nh6vk/L2YmBiNGTNGNptNNptN/fv314IFC6yOpaioKHXt2tWRa+LEiYqPj7c6FooQ/U7eXCXn79HvACgO6EMLFn0oANzIy+oAzipfvrxq1qypMmXK5Fh+6tQp1a5dO8/tioKzGa1mZRtlu502sdvtstvtWrlypc6cOaN58+bpD3/4QyElvLXf5g8ODla9evXUrVs3tW/fXrNmzbIkE4oO/U7+0e/cOfodwH3Rh945+lAAuDWXuXLI399fcXFxatSokWPZ+fPnNWrUqDy3KyrOZrSalW0k3XmbVKpUSZJUuXLlgoqUL7nlr1KliiTrMqHo0O/cHvqdO0O/A7g3+tA7Qx8KAHlzmeLQ7125ckUPPfSQjh49anWUm3KFjEWtINrEZrPl+N+idLP8VmZC0XGF77QrZCxq9DsAcPvoQwHAPdxxcejAgQP685//rJYtW+rkyZMaMmSIqlatKl9fX23fvj3Htp9//rn+9Kc/6fnnn9fAgQP18ssvKz093bE+Pj5ejz76qGbOnKkHHnhA/v7+kn6t9n/88cfy9/fXihUrJEnLly/XwYMHlZycrLFjx+qtt97KdbvNmzerRo0astlsevnllx37Wr9+vSpVqqTXXntNkmSM0Ycffqjx48erS5cuCggI0KFDh5xuB2cz5rWv/fv36//+7//UrFkzJSYmatq0aWrQoIFatWqlDRs2KC0tTZMmTdI999yj+vXra+3atfk6XrnljI+P1wsvvKDGjRsrNTVVY8aMUfXq1eXr6+s4kTpznBctWqRKlSqpXr16kqSUlBRNmzZNnp6e6tat2y3b5Ntvv1W9evW0evXqfP17ikv+/Dh9+rTGjh2radOmaezYsXrwwQd19uxZSdIXX3yhihUrymaz6d1339W1a9ckSdu2bVPt2rX15ptvSrr1ZygxMVEzZsxQ69atde7cOfXv318NGjRw7MMVOXOMJee+x3FxcQoPD9eIESPk6+urOXPmKDMz07H+VseHfod+J1txyJ8f9DvArTnzXZRuPlaVCu87Qh9KHwoARcL8zpIlS0wui29qypQppkqVKsbT09NMmjTJbNiwwXz++eemevXqply5cubkyZPGGGPeeecd0717d3Pt2jVjjDHJycmmSZMmplevXub69evGGGOaNm1qYmNjjTHGXLlyxfTo0cMYY8yBAwfMpEmTjCSzdOlSx74DAwNNw4YNHa9vtt1bb71lJJlly5Y5lmVkZJiePXs69j19+nTzySefGGOMyczMNC1btjR33323SU1NdaodnM2Y176SkpLM6NGjjSQTHh5u4uLizMWLF02XLl1M48aNzYQJE8yBAwfMpUuXTPfu3U3jxo2dynernKdOnTJ2u91IMhMmTDD79+83u3fvNqVLlzYPPfSQMcb54xwQEGDq1q2bY59t2rQxXbt2vWWbfPnll6Zs2bJmwYIFef4b7r33XiPJXL58udjk/+GHH4wk07t37zzz9+7d24SGhjpet2vXzowaNcrxesqUKUaS2bFjh2NZenq66dKli+P1rT5Dq1evNs2bNzeenp7mtddeMxEREcbX19ckJibmmS2bJLNkyRKnt8+v/PYzzhxjY/L+Hv/888+mfPny5tixY8YYY8LCwowk4+PjY5555hljTN7Hh36Hfqe45KffAW4uv+cZY5z7Lt5srGpM4X1H6EPpQ/NyO593APidyDsuDhljzIgRI4y3t7ej8GOMMVFRUUaSefXVV83p06dN+fLlzfz583P83L/+9S8jyXz66afm2rVrxmazmVmzZjnWL1++3PH/N27c6NQvQLltd/nyZVO1alUzbNgwx7JVq1aZ2bNnG2OMSUxMNLVq1TJZWVmO9a+++qqRZBYvXux0OziT0Zl9zZ4920gyCQkJjm1ee+01I8ns3r3bseyVV14xkkxSUpLTGW+W86WXXjKSTHJysmNZjx49TJMmTRyv8zrOxhgzZMiQG07QXbt2zfMEbcyvJ0pn/H6AURzy52eA0adPH/Pmm286Xo8cOdK0bdvW8fr48ePGy8vLjBkzxrFs1apVZtq0acYY5z5Djz/+uJFkDh06lGee3BS34pAxeR9jZ9rlhRdeMPXq1XOszz5uc+bMcSzL6/jQ79DvFJf89DvAzd3OeSav7+KtxqqF/R2hD6UPvRWKQwAKQGSBPK2sXLly8vT0lLe3t2PZkCFDVLp0ae3du1fbt29Xamqq6tevn+PnAgMDJUkbNmzQqFGj1L9/fz3zzDPat2+fZsyYoSFDhji29fJyLmpu25UvX15hYWGaPXu2kpOTVb16dS1ZssTxZIKtW7cqIyND48aNy/FzY8aMUdmyZZ1rBCczOrMvT09PSZKHx//u+qtbt64k5Wjj7PZMTk5WjRo17ihn9j5/u65u3bo6fPiw43Vex/lOZWe4k5+1Mr+zvvnmG0lSWlqaFixYoO+//17GmBy5g4OD9dlnn2n69OmqXr26IiMjHbciOfMZ8vb2lpeXl+69994i+lcVvryOsTPtkpiYqCtXrjjWNWvWTNWqVdPx48cdy/I6Prmh37m9nMXhe0u/87/c9DvArXl7e990rFrY3xH6UPpQAChshfYoey8vL9WpU0eZmZn6+eefJUnnzp3LsU316tVVrlw5nTx5UtKvcxKNHTtWc+fO1fLlyxUZGak+ffoUSJ7w8HC9++67+uyzz/TII4/I09NTd911lyTp4MGDKl++vObOnVsg+7qV291XbpPlZS+7fv16gWS7Hb89zq7IivxZWVn629/+pp07d+rpp59Wly5dbpifa9KkSVq0aJEiIiL0/PPPKzk5WY0bN5ZUtJ9XV+JMuwwaNEgLFy7U+vXr1a9fP124cEGpqakaMGCAYxtnjo+z6HcKB/1O/tHvAAXjZmNVV/qO0IfmH30oAHdQqE8ru3Llipo3b+547ObNnnLQvHlzSb929gsWLNCCBQvk5eWlAQMG6ODBgwWSpUWLFurZs6fmzZunJUuWaOTIkY515cqV04kTJ3TixIkbfu7MmTMFsn8r9lVUso+zqyqq/IcOHdKVK1c0aNAgHThwQJ9//rl69eqV67adO3eWn5+fZs+erVWrVikoKMixriR+hgqCM+0ycuRIzZ07V2FhYXrllVf07LPPatGiRfLz85P0a8HDmePjLPqdwkO/4xz6HaBg3Wys6mrfEfpQ59CHAnAnhVYcOnXqlM6cOaPhw4erW7duqlSpkuMJC9lOnDihK1euaPDgwUpPT1dERIQkacSIEdq+fbuMMdqwYcPNw3t46PLly05nCg8P1969ezV//nz17dvXsbxNmzYyxmjy5Mk5tj9y5Ijef/99p9/fmYyFuS8r/PY4S78Omi5fvqysrCzHNpcvX85xlcHNjpuzVyJkX8ab160+ziio/HllMcboiSee0O7du/X111+rd+/ejnUZGRm5/vxzzz2nkydP6rnnnlNwcLBjeUn7DBUUZ9olIyNDhw4d0p49ezRt2jTNmzcvx+2r33//fZ7Hh37HevQ7OTPdKjP9DuC8vL6LtxqrutJ3hD40Z6ZbZaYPBeBOCuy2svT0dO3Zs0ft2rWTJL3xxht6+OGH5evrK0maOXOmnnzyScftHJL0z3/+Uw8//LD69Omj9PR0zZs3T+PHj5enp6fq1KmjypUrq2PHjpJ+PRFIOavrderUUXJysuLi4nTp0iX5+vrmul224cOH6+mnn5a/v3+OeTX8/f3VuXNnLVy4UGlpaXrwwQd18eJFLVu2TIsXL3a6DZzJ6Ofnl+e+Ll68KEk5LpfNXpacnOxYdunSJUfb50duOVNSUm7YZ1JSUo75WbL3davj3KZNGy1dulTTp09XSEiIIiMjlZ6eruPHj2v37t3q0KFDrsdt69atGjZsmD7++GPHyf5mstsiJSVFFSpUKBb5s/d/4cKFG/KmpKTo6aef1l133eW4r/7f//63fH19tWPHDu3fv1+nT59WQkKCatWqpVq1akmSBg8erPr166tdu3aqVq2a4/2c+bxmD5IuXLigKlWq3LI9XUVex9iZdpk5c6Y2bdqk9u3bq3bt2qpQoYKqVavmuLox+5apWx0f+h36Hfod9+l34F7y+i62bNnypmPVLl26FOp3hD6UPhQACt3vp6i+ndnux4wZY0qVKmUmTZpkgoODzeOPP26mTZvmeFxzthUrVpiAgADzpz/9ybzyyivm7bffdmyTlpZmOnfubPr3729mzJhhwsPDzdy5c40xxqxfv97cd999RpLp1KmT+frrr40xxuzZs8fUrVvXNG3a1ERFRd10u9+aNm2aOXXq1A3Lz549a0aOHGlq1qxpatSoYcLCwvL1CF5nM+a1r/Xr15u2bdsaSWbkyJHm8OHDZuPGjaZDhw5GkhkwYIBJSEgwsbGxpmPHjkaSGTVqlDly5Mht54yJiTENGzY0ksyTTz5pkpKSzPz5802FChWMJDN16lSTmZnp1HFOSUkxQUFBpkKFCqZr165mx44d5pFHHjGjRo0yK1euvGmbfPPNN6Z27dpmxYoVN82+YcMG8+STTxpJRpIZOHCgWbx4seX5V6xYYXr06OHI1a5dOxMQEGD8/f1N8+bNTalSpXI8EeuJJ54wFStWNF27djUxMTHmq6++MtWrVzfDhw/P8RQPY4wZN26co41+61afoYiICFOjRg0jyYwePdrs2rXLqc/Gb6mYPa3M2WOc1/c4OjraVKxY0XGssv9r1aqVY7u8jg/9Dv0O/Y5r9jtwL7czns3ru3irsaoxhfcdoQ+lD80LTysDUAAibcbkvCYyMjJSoaGh+brsc+zYsfrss8909epVp38GrsfVj7Or5TfGyNfXV1u2bFGZMmWKdN82m01LlixRSEhIobz/7fQzBeGLL75QRkaG7Ha7zpw5ozNnzujEiRNKSEiQMUZ//etfizQP8uZq39vfc7X8JbnfgXux6jxT3LhaH/R7rpbfqj6UzzuAAhBVaE8rK0mceVzzvHnzckw8ZwVXyQnnrF+/Xn379i3yX9BKqoSEBE2YMMExGWSVKlXUpEkTSVJAQIDmz59vZbwbuMr32VVywjn0O0DRog8tWehDAbiyAikOXb582TExW26PPnZ1rvIUgcLO6erH2RXyx8bGaty4cWrVqpX27dunzZs3Wx2pxIiPj1diYqLeeOMNDRo0SC1atNDly5f13Xffad26dZo+fbrVEXOg3/mVK3xvb8UV8tPvANahD701V8hPHwqgpLjjp5V98MEHWrdunbKyshQeHq7Y2NiCyIVixtWPs6vkr1atmtLS0rRr1y7NmTNH1atXtzpSiTFy5Ei9+uqrmj17tnx8fFSzZk0FBgYqOTlZ77zzjsqVK2d1RPyOq3xvb8ZV8tPvACWTq/RBN+Mq+elDAZQUBTLnEICSpaTOOZTtypUrKlu2bLH9KyTgjphzCAXJ6vMMUJT4vAMoAMw5BMD9cJUQAAAAAPzPHd9WBgAAAAAAANdFcQgAAAAAAMCNURwCAAAAAABwYxSHAAAAAAAA3BjFIQAAAAAAADdGcQgAAAAAAMCNURwCAAAAAABwYxSHAAAAAAAA3BjFIQAAAAAAADdGcQgAAAAAAMCNURwCAAAAAABwYxSHAAAAAAAA3JjXzVYEBwcXZQ7gBunp6UpPT1elSpWsjoJCQj9TtC5evChvb2+VLVvW6igAUCQ4z8AdnDhxwuoIAEoAz6lTp0797YKLFy8qJSXFojjA//zwww/auXOnSpUqpapVq1odx620bNlSAwYMUL169Qrl/elnitb169f1ww8/6Pvvv5ck1apVy+JEwI0Ku9+Be+E8Y42TJ09qw4YNatasmdVR3EqlSpXUsmVLhYSEWB0FgOs6YDPGGKtTALnJzMzU22+/rVdeeUV9+/bVJ598orvvvtvqWIBL2bt3rx577DHt27dPU6dO1fPPPy9PT0+rYwEASqDIyEiFhoaKXy8AwOVEMecQii0vLy9NnjxZmzdv1uHDh9WuXTt99dVXVscCXEJGRoZmzpypTp06ydvbW7t379bkyZMpDAEAAAC4AcUhFHtdu3bVrl27FBAQoMDAQE2cOFHp6elWxwKKrfj4eHXp0kWvv/66/vKXv2jLli1q3ry51bEAAAAAFFMUh+ASKlWqpE8//VRLlizR/Pnz5ePjo71791odCyhW0tLSNHXqVPn6+qp8+fKKj4/naiEAAAAAeaI4BJcSHBys3bt3q0qVKvL19dWsWbO4rx2QtG3bNnXo0EF///vfNW3aNG3atElNmza1OhYAAAAAF0BxCC6nYcOG2rhxoyZPnqznnntOQ4cOVXJystWxAEtcvXpVU6ZMUc+ePdWgQQMdPHhQkydPlocH3TsAAAAA5/DbA1ySl5eXpk6dqtjYWCUkJKh169ZavXq11bGAIhUbG6v27dvrww8/1Pvvv6/Vq1erfv36VscCAAAA4GIoDsGlZU9W3a9fP91///2aOHGirl27ZnUsoFBduXJFU6ZMUa9evXTvvfdq3759Cg8Pl81mszoaAAAAABdEcQgur3LlylqwYIE++eQTzZs3Tz4+Ptq3b5/VsYBCsXbtWrVo0UIRERH64IMP9OWXX6pu3bpWxwIAAADgwigOocQICwvT3r17ValSJcdk1UBJkZKSonHjxmngwIFq27at42ohAAAAALhTFIdQojRs2FCbNm3Siy++6Jis+uzZs1bHAu7IV199pdatW+uLL75QZGSkoqOjVadOHatjAQAAACghKA6hxMmerHrdunXasWOHWrdurTVr1lgdC8i3CxcuaNy4cbr//vvVrVs37du3T8OHD7c6FgAAAIAShuIQSqw+ffpo37596tOnjwYNGsRk1XAp0dHRatWqlVauXKlly5YpMjJS1atXtzoWAAAAgBKI4hBKtMqVK2vhwoWOyar9/Pz0448/Wh0LuKmkpCSFhIRo8ODB8vPz0/79+/Xggw9aHQsAAABACUZxCG4hLCxMCQkJ8vb2Vvv27ZmsGsVSVFSUWrVqpe+//15r165VZGSkqlatanUsAAAAACUcxSG4jUaNGmnz5s168cUX9eyzz2rYsGE6d+6c1bEA/fLLLxo6dKhCQ0M1dOhQ7d27VwEBAVbHAgAAAOAmKA7Brfx2survvvtO7du318aNG62OBTeWfbVQfHy8YmJiNGfOHFWsWNHqWAAAAADcCMUhuKW+fftq37598vPzU9++fZmsGkXu5MmTeuCBBxQaGqrhw4crISFBffv2tToWAAAAADdEcQhuq0qVKlq0aJE++eQTffzxx+rRo4cOHTpkdSyUcMYYRUREqHnz5tq/f782bNigOXPmqEKFClZHAwAAAOCmKA7B7YWFhWnnzp3KysqSj4+PIiIirI6EEurYsWPy9/fXhAkT9OijjyohIUG9evWyOhYAAAAAN0dxCJDUvHlzbd++Xc8++6zGjx+v4cOHM1k1Ckz21UJt27bV6dOntXXrVs2aNUvlypWzOhoAAAAAUBwCsnl7e2vq1Kn6+uuvtX37dnXo0EGbN2+2OhZc3JEjR9S3b19NmDBBEyZM0M6dO9W5c2erYwEAAACAA8Uh4Hf69eun+Ph4tW/fXn369NHEiROVkZFhdSy4mMzMTM2aNUvt2rXTuXPntH37ds2YMUOlS5e2OhoAAAAA5EBxCMhF9erV9cUXX+hf//qXPv74Y/n5+enw4cNWx4KL2L9/v/z8/PTCCy/oT3/6k3bs2CEfHx+rYwEAAABArigOAbeQPVl1ZmamOnbsyGTVuKXMzEzNnDlTPj4+8vDwUHx8vGbMmKFSpUpZHQ0AAAAAboriEJCH5s2ba9u2bXryySc1fvx4BQcH6/z581bHQjGTkJCgrl276vXXX9frr7+u2NhYtWzZ0upYAAAAAJAnikOAE0qXLq0ZM2Zo7dq12rp1q9q3b68tW7ZYHQvFQEZGhmbOnKnOnTurdOnS2rVrlyZPnixPT0+rowEAAACAUygOAflgt9sVHx+vdu3aqU+fPpoyZQqTVbux+Ph4+fr66i9/+Yv+8pe/aMuWLWrevLnVsQAAAAAgXygOAflUo0YNrVy5UvPmzdN7772nHj16MFm1m0lLS9OUKVPUqVMnVaxYUfHx8Zo8ebI8POhSAQAAALgefpMBblP2ZNXXrl2Tj4+PPv30U6sjoQhs3bpVHTp00AcffKC3335bGzduVJMmTayOBQAAAAC3jeIQcAdatGihbdu2afz48XrkkUcUEhLCZNUl1NWrVzVlyhTdd999atiwofbu3auJEydytRAAAAAAl8dvNcAdKlOmjGbMmKE1a9YoNjZWHTp0UGxsrNWxUIC2bNmidu3aac6cOXr//ff11VdfqX79+lbHAgAAAIACQXEIKCD+/v7as2eP2rRpw2TVJcTFixc1ceJE9e7dW02bNtXevXsVHh4um81mdTQAAAAAKDAUh4AClD1Z9ezZs/Xee++pZ8+eOnLkiNWxcBvWrl2rNm3a6NNPP9UHH3ygVatWqW7dulbHAgAAAIACZzPGGKtDACXRgQMHNGLECB07dkyzZ8/WqFGjrI4EJ6SkpOjFF1/U3LlzNXz4cM2ePVs1atSwOhYAAMVKYmKigoKCclwlnZqaqjNnzqhhw4Y5tm3fvj0P7gCA4i3Ky+oEQEnVsmVLbd++XZMnT1ZYWJhWrlypiIgIValSJdftr127pmPHjqlZs2ZFnNQ9ZGRkyNvb+5bbfPXVVxo3bpwyMjIUFRWlYcOGFVE6AABcyx/+8AelpaXp4MGDN6zbt29fjtehoaFFFQsAcJu4rQwoRGXKlNGsWbNyTFb97bff5rrtK6+8osGDB+vq1atFnLLk27Ztm4KCgnSzCyXPnz+vcePG6f7771e3bt20b98+CkMAAOQhLCxMXl55/62Z4hAAFH8Uh4AiEBAQoPj4eLVq1Uq9e/fW1KlTlZWV5Vi/fv16/f3vf9fhw4f1wgsvWJi05ElKStKQIUO0du1azZs374b10dHRat26taKjo7V8+XJFRkaqevXqFiQFAMC1jBgxIsd45vdsNps6duyoJk2aFGEqAMDtoDgEFJGaNWsqOjpas2fP1t/+9jf16NFDR48e1fnz5zV69Gh5eHjo+vXrev/99/Xll19aHbdEyMrKUmhoqM6fPy9JmjhxohITEyVJp0+fVnBwsAYPHiw/Pz/t27dPQ4YMsTIuAAAupX79+urcubM8PHL/lcLT01NhYWFFnAoAcDsoDgFFyGazKTw8XNu2bdPFixfVqVMnhYSEKDk52fGXN5vNptGjR+v06dMWp3V9U6dO1ebNmx2TZV67dk2PP/64oqKi1Lp1a+3cuVNff/21IiMjVbVqVYvTAgDgesLCwmSz2XJdl5WVpeDg4CJOBAC4HTytDLDI1atXNWrUKC1fvvyGuXC8vb3Vv39/RUdHW5TO9a1fv14BAQG6fv16juU2m02dO3dW+/bt9dZbb6lixYoWJQQAwPWdOXNGtWvXvuH2Mk9PT/Xs2VMbNmywKBkAIB+iuHIIsEhiYqJWr16d6yTJGRkZ+vLLL/XRRx9ZkMz1HT9+XMOHD7/p+h9++EGvv/46hSEAAO5QjRo11Lt3b3l6et6wbvTo0RYkAgDcDopDgAXS09M1dOhQZWZm3nQbY4yeeuop/fjjj0WYzPVlZGQoODhYqampN1w1JP3arlevXtWYMWMsSAcAQMkzevToG/7Y5eHhoaFDh1qUCACQXxSHAAv8+c9/1v79+x1z4dxMZmam/vjHP96yiIScnnvuOe3cufOWbZt9ZVZUVFQRJgMAoGQaOnRojkfae3l5aeDAgapSpYqFqQAA+UFxCChiiYmJWrZsma5fvy5vb++bTuIo/Voc2rNnj954440iTOi6oqKi9N57793ysbrZbDabxo8fr+Tk5CJIBgBAyVWxYkUFBgbK29tb0q8TUY8aNcriVACA/GBCasAiR48eVXR0tL744gtt2bJFWVlZ8vLyyvWKFw8PD8XGxqpbt24WJHUNP/74ozp06KCrV6/mOo+TzWaTp6enMjMzVaFCBfXq1Ut2u10hISGqU6eOBYkBACg5li1bpuHDh8sYo7Jly+rs2bMqW7as1bEAAM6JojgEFAOpqan65ptvHMWipKQkeXt7OwpFnp6eqlOnjvbv388kyrlIS0tT586d9Z///MfRZr8tBpUtW1bdunVTQECA7Ha7OnToIA8PLpwEAKCgpKWlqXr16kpNTdWoUaP06aefWh0JAOA8ikMoHJGRkVZHcFnGGB09elS7du3Sjh079PPPPzvW9e7dW+PHj7cwXfE0e/Zsbd68WTabTcYYlSlTRi1btlSbNm3UsmVLNWjQ4Ja37yGnkJAQqyMAKOFOnDihrVu3Wh0DBQp3a4QAACAASURBVOyDDz7Qxo0b9dJLL6l9+/ZWx0EBY3wAlGgUh1A4+EUccF2cFgAUtsjISIWGhlodA0A+MD4ASrQor7y3AW7PkiVL+AtDAcvIyNDBgwfVtm1bq6MUGz/88IOaNm3KbWIFgF/WABQ1ftksWbKysvTmm2/qlVdesToKChDjA8A9UBwCXIi3tzeFod9p3ry51REAAIB+nSPxpZdesjoGAOA28Kd2AAAAAAXCy4u/PQOAK6I4BAAAAAAA4MYoDgEAAAAAALgxikMAAAAAAABujOIQAAAAAACAG6M4BAAAAAAA4MYoDgEAAAAAALgxikMAAAAAAABujOIQAAAAAACAG6M4BAAAAAAA4MYoDgEAAAAAALgxikMAAAAAAABujOIQUIxcunTJsvctrH27AtodAFCccZ6yBu0OwJ1QHEKxsGLFCtWrV08HDx60OoolZs+erZ49e6pr164F+r5z5sxRr1691KJFi3xtY/XxiIqKUteuXWWz2VS6dGnZ7XYNHDhQAwYMUK9evVSrVi3ZbDYdOnTojvZDuwMAijPOUzkxPmB8AKDwUBxCsVC+fHnVrFlTZcqUybH81KlTFiUqWuPGjVNKSoquX79eoO87ZswYXb9+XVlZWfna5mbHo6gEBwfr3XfflSR17txZMTExWr16tdasWaNNmzYpMTFR9913nzIzM+9oP7Q7AKA44zyVE+MDxgcACg/FIRQL/v7+iouLU6NGjRzLzp8/r1GjRlmYquh4eXnpD3/4Q4G/r6enp+rWrZvvbXI7HkWtSpUqkiRvb+8b1nl5eemJJ56QzWa7o33Q7gCA4ozz1I0YHwBA4fCyOgCQmytXruihhx7S0aNHrY4Ci+Q1sPvjH/9YREkAAEBxwfgAAAoHVw7BcufPn9fHH38sf39/rVixQpK0fPlyHTx4UMnJyRo7dqzeeustSZIxRh9++KHGjx+vLl26KCAgwHFf+f79+/V///d/atasmRITEzVt2jQ1aNBArVq10oYNG5SWlqZJkybpnnvuUf369bV27dp8Zy2K/W/cuFEDBgxQ1apV1b9//xwFslvtP9sXX3yh8PBwTZ48WU899VSut+bdapvcjkd8fLxeeOEFNW7cWKmpqRozZoyqV68uX1/fGwp4cXFxCg8P14gRI+Tr66s5c+bkuLz722+/Vb169bR69ep8tv7/vPvuuzp58qQk2j1bXu0OAK5s0aJFqlSpkurVqydJSklJ0bRp0+Tp6alu3bo5touPj9ejjz6qmTNn6oEHHpC/v79j3a368sTERM2YMUOtW7fWuXPn1L9/fzVo0EBnz551Kh/jA8YHrtruAOBggEIgySxZssSpbQ8cOGAmTZpkJJmlS5c6lgcGBpqGDRvm2Hb69Onmk08+McYYk5mZaVq2bGnuvvtuk5qaapKSkszo0aONJBMeHm7i4uLMxYsXTZcuXUzjxo3NhAkTzIEDB8ylS5dM9+7dTePGjfP97yrM/Q8YMMBUq1bNPPbYY2b16tXm7bffNqVKlTJ16tQxqampee7fGGMWLFhgunTpYq5evWqMMebMmTOmevXq5u6773bsJ69tcjsep06dMna73UgyEyZMMPv37ze7d+82pUuXNg899JDjvX/++WdTvnx5c+zYMWOMMWFhYUaS8fHxMc8884wxxpgvv/zSlC1b1ixYsOCWbf3DDz8YSaZ3796OZVlZWebo0aOmXbt25sSJE8YYQ7s72e7OWLJkieG0AKAo3E5/ExAQYOrWrZtjWZs2bUzXrl0dr5s2bWpiY2ONMcZcuXLF9OjRw7HuVn356tWrTfPmzY2np6d57bXXTEREhPH19TWJiYlOZWN8wPjAVdvdGYwPALcQybcchSI/xSFjjNm4cWOexaHExERTq1Ytk5WV5Vj26quvGklm8eLFxhhjZs+ebSSZhIQExzavvfaakWR2797tWPbKK68YSSYpKcnpjIW9/wEDBpg6derk2Of06dONJDNr1qw895+ammpq165tFi5cmOM9HnzwQccAw5ltjMn9eLz00ktGkklOTnYs69Gjh2nSpInj9QsvvGDq1avneJ09gJszZ06O/WVmZpq8ZP9spUqVTNeuXU3Xrl2Nr6+vadCggZHkGPwZQ7s72+55YfAHoKjcTn8zZMiQG4pD2ecHY4y5du2asdlsZtasWY71y5cvN8Y4dw5//PHHjSRz6NChfOVifMD4IJurtnteGB8AbiGSOYdQLHh55f1R3Lp1qzIyMjRu3Lgcy8eMGaOyZctK+nUSP0ny8PjfHZPZk/r9duLC+vXrS5KSk5NVo0YNpzIWxf4rVaqU473DwsL00ksvKS4uTnXq1Lnl/rds2aJTp06pTZs2OdaXLl3a8f+d2UbK/Xhk/9t+u65u3bo6fPiw43ViYqKuXLnieN2sWTNVq1ZNx48fz/W9nNGxY0dt2LAhx7LfzydAuzvX7gBQknl7e6t///565plntG/fPs2YMUNDhgyR5Nw53NvbW15eXrr33nvztV/GB4wPfssV2x0AJCakhgs5ePCgypcvr7lz5+br53KbuDB7WX4eUWrF/uvUqaOyZcvq6tWree5/1qxZkqRSpUrd9P1++OGHPLe5E4MGDdLChQu1fv169evXTxcuXFBqaqoGDBhQoPt58sknVa5cuVtuQ7sXfLsDQHH3+eefa+zYsZo7d66WL1+uyMhI9enT57bP4c5gfJA3xge5KyntDqBkoDgEl1GuXDmdOHFCJ06cuOERn2fOnHH6CiBX27/NZlPr1q3z3H/2wOLnn39W06ZNc30vZ7a5EyNHjtTVq1cVFhamxx57TImJiVq0aJH8/PwKdD89e/aUJCUlJemuu+4q0PfORrsDgOvx8vLSggULdP/99+u5557TgAEDFB8fX6jncMYHeWN8ULLbHUDJwNPKUGx5eHjo8uXLjtdt2rSRMUaTJ0/Osd2RI0f0/vvvF3oeK/b/008/KSMjQyEhIXnuv23btpKkJUuW5Fh//fp1ZWVlSZJT29yJjIwMHTp0SHv27NG0adM0b948xyX9v99fXowxeW7z6KOP5rhUvKCU1HYHAFfm5eWly5cv5+g3L1++7DinpKenKyIiQpI0YsQIbd++XcYYbdiwoVDP4YwP8sb44H+KY7sDgMSVQygmsh/ZeebMGceyOnXqKDk5WXFxcbp06ZL8/PzUuXNnLVy4UGlpaXrwwQd18eJFLVu2TIsXL5YkXbx4UZJyPKIze1lycrJj2aVLlyT9OpB0lr+/f6Hu39PTU+fPn1dqaqrKly8vY4ymTZum1157Tc2bN1ezZs1uuf/q1aurT58++uSTT+Tj46OHH35Y+/fvV2xsrM6cOaNFixbpgQcecGqb3I5HSkrKDf+2pKSkHPeyz5w5U5s2bVL79u1Vu3ZtVahQQdWqVVOjRo0c28TExGjYsGH6+OOPNXz48Ju294ULF25oo2xpaWmaMmWKvL295enpSbs70e4A4OratGmjpUuXavr06QoJCVFkZKTS09N1/Phx7d69Wy1bttS8efM0fvx4eXp6qk6dOqpcubI6duyoLl265HkOzy48XbhwQVWqVHE6F+MDxgeu3O4A4GDBLNhwA8rH08rWr19v7rvvPiPJdOrUyXz99dfGGGP27Nlj6tata5o2bWqioqKMMcacPXvWjBw50tSsWdPUqFHDhIWFOR4zu379etO2bVsjyYwcOdIcPnzYbNy40XTo0MFIMgMGDDAJCQkmNjbWdOzY0Ugyo0aNMkeOHHH631WY+09ISDAPPfSQ6d+/vwkPDzcTJ07M8VSKvPZvjDEpKSnm0UcfNbVq1TL169c3U6dONeHh4ebRRx81MTExJisrK89t1q1bd8PxiImJMQ0bNjSSzJNPPmmSkpLM/PnzTYUKFYwkM3XqVJOZmWmio6NNxYoVjaQc/7Vq1cqR85tvvjG1a9c2K1asuGk7L1++3HTv3t1IMp6enqZVq1ZmwIAB5v777zd+fn6O/f7zn/+k3Z1sd2fwNBIAReV2+puUlBQTFBRkKlSoYLp27Wp27NhhHnnkETNq1CizcuVKk5aWZjp37mz69+9vZsyYYcLDw83cuXMdP3+rvjwiIsLUqFHDSDKjR482u3btylc2xgeMD1y13Z3B+ABwC5E2Y5y4NhPIJ5vNpiVLligkJMTqKChCX3zxhTIyMmS323XmzBmdOXNGJ06cUEJCgowx+utf/2p1xBKpoNo9MjJSoaGhTl2yDwB3gv7GvTA+sAbjAwD5EMVtZYDk1GSR8+bNU1BQUBGkcU0JCQmaMGGCTpw4IUmqUqWKmjRpIkkKCAjQ/PnzrYxXYtHuAFB4GB/cOc5T1qDdAeQXxSFAOe/hxu2Jj49XYmKi3njjDQ0aNEgtWrTQ5cuX9d1332ndunWaPn261RFLJNodAAoP44M7x3nKGrQ7gPziaWUACsTIkSP16quvavbs2fLx8VHNmjUVGBio5ORkvfPOOypXrpzVEUsk2h0AUJxxnrIG7Q4gv5hzCIWCOYfc25UrV1S2bFnZbDaro7iVO2135hQAUFTob9wT4wNrMD4A4ATmHAJQ8PhrlDVodwBAccZ5yhq0OwBncFsZAAAAAACAG6M4BAAAAAAA4MYoDgEAAAAAALgxikMAAAAAAABujOIQAAAAAACAG6M4BAAAAAAA4MYoDgEAAAAAALgxikMAAAAAAABujOIQAAAAAACAG6M4BAAAAAAA4MYoDgEAAAAAALgxikMAAAAAAABuzMvqACi5tm3bZnWEEisrK0uenp5Wx0ABuXbtmtLS0lSpUiVLc/CdBVDUIiMjrY4AIA+MDwD3YDPGGKtDoOSx2WxWRwBwmzgtAChskZGRCg0NtToGgHxgfACUaFEUhwAX8u233yo4OFgVKlTQsmXL1Lp1a6sjoQBcunRJ69ev15o1a7R69Wr997//VbVq1RQQEKABAwaof//+qlWrltUxAQC4peyiH79eAIDLiWLOIcBFREREqG/fvvLx8dH3339PYagEqVixooYMGaIPP/xQP//8s44cOaI333xTly5dUnh4uO6++261atVKU6ZMUUxMjDIyMqyODAAAAKAE4cohoJhLS0vTk08+qfnz5+vll1/Wq6++Kg8P6rru4sqVK9q6datiYmK0cuVKHTx4UBUqVFDv3r0VFBSkQYMGqW7dulbHBACAK4cAwHVxWxlQnB0+fFjDhg1TYmKiFi5cqICAAKsjwWJHjx5VTEyMYmJitHr1al2+fFmNGzdWYGCggoKCdN9996lUqVJWxwQAuCGKQwDgsritDCiuvvrqK/n6+srT01M7duygMARJUuPGjRUeHq7IyEglJSVp3bp1Cg4OVkxMjPz9/VW1alUFBQUpIiJC//3vf62OCwAAAMAFUBwCihljjGbOnKmgoCAFBgYqNjZWjRo1sjoWiqGyZcvKbrdrxowZ2r9/v44cOaJ//OMfKlu2rJ5//nk1aNBA99xzjyZOnKiYmBilp6dbHRkAAABAMcRtZUAxcvHiRT388MNavXq1Zs6cqYkTJ1odCS4qLS1NsbGxjlvQ4uLiVK5cOXXv3l2BgYEaMmSIGjRoYHVMAEAJwm1lAOCymHMIKC727NmjoUOH6tq1a4qKilLXrl2tjoQS5NixY1q3bp1iYmK0du1aXbx4UY0bN5bdbldgYKACAgJUunRpq2MCAFwYxSEAcFnMOQQUBwsWLFD37t1Vt25d7dy5k8IQClyjRo0ccxWdPXtWW7ZsUXBwsOLi4vTAAw+oatWq8vf318yZM3Xw4EGr4wIAAAAoQhSHAAtlZmZqypQpGj16tMaMGaOYmBjVqlXL6lgo4by8vNSjRw/NmDFDO3fu1KlTp/TBBx/orrvu0vTp09WyZUvdc889GjdunKKionT58mWrIwMAAAAoRNxWBljk5MmTCg4OVkJCgubNm6fg4GCrIwHKyspSfHy8oqOjtWrVKu3atUtlypSRn5+f7Ha77Ha7fHx8rI4JACiGuK0MAFwWcw4BVtiyZYtCQ0NVsWJFLVu2TK1atbI6EpCrpKQkbdq0SdHR0YqOjtaFCxcccxXZ7XYNGDBAFStWtDomAKAYoDgEAC6LOYeAohYREaF+/fqpc+fO+v777ykMoVirWbOmgoODNX/+fCUnJ2vnzp0aPXq04uLiFBoaqpo1azrmKoqLi7M6LgAAAIDbwJVDQBG5evWqxo8fr88++0wvv/yyXnvtNdlsNqtjAbftzJkz2rhxo+MWtPPnz6tRo0by9/eX3W5X//79ValSJatjAgCKCFcOAYDL4rYyoCgcPnxYQ4cO1alTp7Rw4UL5+/tbHQkoUNlzFcXExCg6Olrbtm2Th4eHunTpoqCgINntdnXs2JGCKACUYBSHAMBlcVsZUNhWrVqlzp07y9vbWzt27KAwhBLJ09NTPj4+mjx5smJjY3X69GktXLhQrVq10qxZs9SpUyfVrl1bYWFhioqKUkpKitWRAQAAAPx/FIeAQmKM0cyZM/XAAw8oKChIsbGxatiwodWxgCJRvXp1BQcHa86cOTpx4oR27typSZMm6dSpUxoxYoSqVaumTp06aerUqYqLi+OvzAAAAICFuK0MKARnz57VyJEjtXHjRr333nsaO3as1ZGAYuPs2bP65ptvFBMTo1WrVunkyZOqWbOmevXqpcDAQA0ePFhVqlSxOiYAIJ+4rQwAXBZzDgEFLT4+XkOHDlVGRoaWLl2qLl26WB0JKNb279+vVatWKSYmRps2bdL169fVvn17BQYGKigoiLmKAMBFUBwCAJfFnENAQfrss8/k5+enBg0aaOfOnRSGACe0atVKkydP1rp16/TLL79o0aJF8vHx0UcffaROnTqpVq1aCgkJ0fz583Xu3Dmr4wIAAAAlDlcOAQUgPT1dL774ot577z099dRTevvtt+Xl5WV1LMDl/faqos2bNysrK0vt27eX3W5XYGCgunfvLg8P/s4BAMUBVw4BgMvitjLgTiUmJio4OFj79u3TvHnzNHz4cKsjASVSamqqvvnmG61atUqrV6/W8ePHVb16dfXp00d2u11BQUGqXbu21TEBwG1RHAIAl0VxCLgTmzdvVmhoqCpXrqxly5apZcuWVkcC3MbRo0cVHR2tVatWafPmzcrMzFSHDh1kt9tlt9vVu3dvruADgCJEcQgAXBZzDgG3KyIiQna7XV26dNF3331HYQgoYo0bN9bEiRO1bt06nTt3TmvXrpWfn58WLVokf39/3X333QoJCVFERIROnjxpdVwAAACg2OLKISCfLl++rMcff1yff/65/vrXv+rFF1/kSUpAMXP06FHFxMQoOjpa69atU3p6ulq2bKmgoCDZ7Xb16tVL3t7eVscEgBKFK4cAwGVxWxmQH4cOHdLQoUMdT1Sy2+1WRwKQhytXrmjr1q2Kjo7WypUr9dNPP6lq1arq16+f7Ha7Bg0apLp161odEwBcHsUhAHBZ3FYGOCs6Olq+vr4qXbq0du7cSWEIcBHlypWT3W7XrFmzdOzYMR05ckTTp0/X1atXNXHiRNWrV0+tWrXSlClTFBMTo2vXrlkdGQAAAChSFIeAPGRlZWnq1KkaMmSIQkJCtHXrVjVo0MDqWABuU+PGjRUeHq7o6GidO3dO69atU1BQkFauXCl/f39VrVpVQUFBioiI0PHjx62OCwAAABQ6bisDbuHs2bMaMWKENm/erPfee09jxoyxOhKAQpQ9V1FMTIzWrFmjS5cuqXHjxgoMDFRQUJDuu+8+lSpVyuqYAFAscVsZALgs5hwCbmb37t0aNmyYMjMztXTpUvn6+lodCUARunr1qr799ltHsSguLk7ly5dXt27dFBgYqAcffFD169e3OiYAFBsUhwDAZVEcAnIzf/58PfHEE+ratasWL16smjVrWh0JgMWOHTumdevWKSYmRmvXrtXFixfVuHFj2e12BQYGKiAgQKVLl7Y6JgBYhuIQALgsJqQGfis9PV0TJ07UI488oqefflrr1q2jMARAktSoUSOFh4crMjJSZ8+e1ZYtWxQcHKy4uDgNHjxYVatWlb+/v2bNmqWffvrJ6rgAAACA07hyCPj/Tpw4oeDgYO3fv1//+te/NGzYMKsjAXARP/30k77++mvFxMTo66+/VkpKSo6rivz9/VWmTBmrYwJAoeLKIQBwWdxWBkjSpk2bFBoaqrvuukvLli1TixYtrI4EwEVlZmZq+/btWrVqlWJiYrRr1y6VKVNGfn5+stvtCgoKUsuWLa2OCQAFjuIQALgsikNwb8YY/fOf/9QLL7ygYcOG6aOPPlL58uWtjgWgBDl9+rTWrl2rVatWad26dbpw4YLjqiK73a4BAwaoYsWKVscEgHw5ffq0PvnkkxzLEhIStHDhQs2YMSPH8rvuukvh4eFFmA4AkE8Uh1CyJSQkqG3btrmuu3z5sh577DEtX75cb7zxhiZPnlzE6QC4m6ysLMXHxys6OlqrVq264aoiu90uHx8fq2MCQJ4yMzNVq1YtpaSkyMvLy7HcGCObzeZ4nZ6errFjxyoiIsKKmAAA51AcQsn16aef6qmnntKuXbvUuHHjHOt+/PFHDR06VElJSVq0aJH69etnUUoA7iwpKUmbNm1yFIvOnz+vRo0ayd/fX3a7Xf3791elSpWsjgkAuZowYYLmzp2rjIyMW263YcMG9e7du2hCAQBuB8UhlExnzpxR06ZNlZKSohYtWmjHjh0qV66cJGnlypUKCwtTkyZN9Pnnn6t+/foWpwWA/11VFBMTo+joaG3btk0eHh7q0qWLgoKCuKoIQLETGxurnj173nKbGjVq6NSpU/L09CyiVACA28Cj7FEyPfPMM0pNTZUxRj/++KMef/xxZWVlaerUqRoyZIhCQ0P17bffUhgCUGx4enrKx8dHkydPVmxsrH755RctXLhQjRs31t/+9jd16tRJtWvXVlhYmKKiopSSkmJ1ZABuzs/PT3Xq1Lnp+lKlSiksLIzCEAC4AK4cQomzZs0aDRw4MMcym80mHx8f7d+/X3PmzNHo0aMtSgcA+efMVUUdO3bMMc8HABSFyZMn65133rnprWU7d+7kqkcAKP64rQwly5UrV9S8eXOdPHlSWVlZOdZ5eHho7ty5euyxxyxKBwAFIzk5WRs2bHAUi06dOqVatWopICBAQUFB8vf3V5UqVayOCcANxMfHq0OHDrmua9CggX766aeiDQQAuB0Uh1CyPPPMM5o9e7YyMzNvWOfp6akqVaooISHhlpdAA4AruX79unbv3q2YmBjFxMRo06ZNun79utq3b6/AwEAFBQXd0VVFP/74o5KSktSjR48CTg6gpGjatKkOHTqUY1mpUqX00ksvaerUqdaEAgDkB3MOoeTYsWOH3nvvvVwLQ9Kvt2VcvHhRQ4YM0bVr14o4HQAUDg8PD8dcRevWrdOpU6e0aNEi+fj46KOPPlKnTp109913KyQkRPPnz9f58+fz9f7Lli1Tr1699Oc//znPJxIBcE9hYWHy9vbOsezatWsKDQ21KBEAIL+4cgglQkZGhtq3b68ff/zxpsWh33r66ac1a9asIkgGANbav3+/Vq1adcNVRXa7XYGBgerevbs8PG7+t6IePXpo69at8vDwUOvWrbVkyRI1a9asCP8FAIq7I0eOqEmTJsr+tcJms6lNmzbas2ePxckAAE7iyiGUDG+99Zb+85//3LIwlP0XrZo1a8rLy0tXr14tqngAYJlWrVo5rio6d+6cli9fLh8fHy1YsEA9e/Z0XFUUERGhX375JcfPXrx4Udu3b5cxRllZWTpw4IDatm2rWbNmib8tAch2zz33qH379o5Cs5eXl8LCwixOBQDID64cgss7dOiQWrduneutYqVKldK1a9dUuXJlDR48WGFhYerbt+8t/0oOAO7i6NGjio6O1qpVq7R582ZlZWXluKrol19+UUhIyA2FIJvNpn79+mn+/PmqXbu2RekBFCfvvvuuXnjhBWVmZspms+m///2v6tata3UsAIBzmJAars0Yoz59+mjr1q2OuTC8vLyUmZmpChUqaMiQIQoJCdHAgQPl5eVlcVoAKL5SUlIUExOj1atXa82aNUpMTFT58uWVkZGRa/Hd29tb5cuX17///W8NHjzYgsQAipNTp06pbt26un79urp3765vv/3W6kgAAOdRHIJr++ijjzR27Fh5eHjIGKNy5cpp2LBheuihh2S322+YHBEA4JyEhAT16tVLFy5cuOk2Hh4eun79ukaNGqUPP/xQ5cuXL8KEAIqb3r17a9OmTZozZ47Cw8OtjgMAcB7Fod+73Uf9ArejsL5+kZGRPCEEwC3l1f/s3btXbdu2deq9vLy8VLt2bS1evFgnTpyg/0GRGT58uKKiogrlvYODg7V06dJCeW/g95YsWaKQkJBCeW/GhQB+L5dxYBT32eTimWeeUbdu3ayOgTzExsbKy8tLHTt2VKlSpayOky/btm3Tu+++W+j7WbJkSaHvA4Brcbb/WbNmjby9vZ16fH1mZqZOnTql++67T8HBwZLof1D43nnnnULfR9euXTVp0qRC309JkZqaqoiICNosn4qqcEO/DOBW40CKQ7no1q1boVXuUXBc/RgVRXHI1dsIQOFwpv+Jjo6+6RMgbTabvLy8HJP7X79+XZmZmTLGaPHixZIkX19fNWzYsMAyA79XWFcM/VbdunU5l+aTv78/E1HnU1EVh/gsA5BuPg6kOAQAAHK4dOmS4xH2ZcuWVcWKFVWpUiVVqVJFVatWVdWqVVW5cmVVqVJFd911lypXruz4Ly4uTq+++qrKli1r9T8DgAUoDAGAa6I4BAAAcihdurROnz6typUrO64Octbly5clSbVq1SqMaAAAACgEFIcAAEAOpUqVcrm53AAAAHD78vfnQAAAAAAAAJQoFIcAAAAAAADcGMUhAAAAAAAAN0ZxCAAAAAAAwI1RHAIAAAAAAHBjFIcAAAAAAADct33EmAAAIABJREFUGMUhAAAAAAAAN0ZxCAAAAAAAwI1RHAIAAAAAAHBjFIcAAAAAAADcGMUhAAAAAAAAN0ZxqAAYY/TOO+9oxowZatKkif74xz9q48aN+vOf/6y1a9daHe+OLFy4UJ06dVKlSpXUpUsXffXVV7lud+HCBb388st66aWXbms/UVFR8vX1lc1mU+nSpWW32zVw4EANGDBAvXr1Uq1atWSz2XTw4EFt2bKlRLQtnLdixf9j777Dorj2/4G/FxYUEIIKCigWEhSx4xVEsYBgC3awUDZqBBua2G6iX2MJJmpi1CRgwdhoitg1epVmATuKBTQWlCtFehFpC5zfH/6YK9J2FZhd9vN6Hp9HZmdnPnPm7Jmzn5055wQMDQ3x6NEjvkORC/JaXqGhoZg9ezYEAgEEAgFGjhyJgIAAvsNCcHAwBgwYwMX1zTffICYmhu+wmhxFv5bevn0bkydPxrJly+Du7o4DBw5IvR+6lsqGplyXgbr7fMeOHcOSJUuwZMkSTJ8+HVeuXJF6H1SXFYOk/RV57dfwRV7Li/qB/BPyHUBT8OOPPyIlJQU7d+6ElZUVJk6cCIFAgIMHD8LIyEiqbaWkpEBfX7/OZY1h69atCAkJgaurK168eIHdu3fD3t4eFy5cgK2tLbfe6dOn4e/vj8OHD8PDw+Oj9uXo6Ih27dph0KBB6N+/P0JDQyu9XlpaiuHDh+PatWuIjIzEvn375LpsSe0+PC8aGhpo06YNmjdvzmNUDaMh6qAslNfHHJetrS1sbW1x6tQppKenY+/evWjXrl0DRVi79+N3dHSEoaEhLC0t0adPH/z++++8xNTUKfK19N69exg2bBhCQkJgaWmJwsJC9OnTB4WFhZg7d67E+6JrqWxoqnUZqLvPt2/fPmzduhUxMTFQUlLC/fv3MXToUAQFBWHEiBES74fqsmKoqb9C/cBPIwvlRf1A+UR3DtWD7du3o1OnTgAAKysrpKenY/ny5VJvJzs7Gy4uLnUuawz5+fk4c+YM/v77b3zzzTfYtm0bQkNDIRAI8Ouvv1Zad+zYsdi9e/cn77NVq1YAABUVlSqvCYVCzJ07FwMHDsTChQul3rYslS2pXXXnxc7ODtHR0ejcuTNPUTWMhqqDfJfXpx6XlpYWAOCzzz6rr5CkUl382traAPiLSREo8rV06dKlsLCwgKWlJQBATU0N33zzDZYvX443b95ItU+6lvKvKdblCrX1+fLz8/Hdd9/ByckJSkrvvmL06tULw4YNw9KlS8EYk2pfVJebvur6K9QP/HR8lxf1A+UXJYc+UVFREdLS0iAQCCotV1VVlWo7BQUFmDZtGuLj42td1lhu3LiBjRs3VjouS0tL9O3bF8+ePauyfrNmzT55nx+W4YemT58OExMTuS9bUjNFOi9N9Vjr47gq2oK62oSGUFP8fMakCBT5WpqSkoKwsDAMHTq00nsHDx6M/Px8+Pv7S7VPupbyq6nW5ffV1Oe7efMm0tPT8cUXX1RabmNjg4cPHyIyMlKq/VBdVjyKdB6a6rFSP1C+UXLoExw4cABubm4A3j2L6Obmhk2bNtW4fmpqKtzc3ODp6Qk3NzdMnDgRmZmZAIDjx4/j0aNHyMjIgJubGzZv3lztMuDds+w7d+7EvHnzYGFhgREjRuDp06cAgJiYGCxfvhxGRkZ4+/YtZs+eDR0dHZibm0v1IR0+fDj69+9fZflnn33G/RomqaioKBgaGuLcuXNSve99a9eurfV1eSpbWSTpsdVWPhWio6Ph7u4OJycnmJubY9euXSgtLeVel/ZcZWdnY8+ePbCzs8OJEycAAJcvX4auri4EAgFWrVrFbTssLAxaWlpYs2aNxPHWJjY2FitXrkTXrl2RlJQET09PdOzYEd27d0dERASKioqwePFifP755+jQoUOV8Q5q239NdbC28pFEdeUlyfmNi4vD//3f/8HU1BTJycmYMGECWrVqBXNzc1y/fh0AcPDgQWhpacHQ0BAAkJubC09PTygrK3N3PNR0XJ/SDshC/NKo7RyePHkSmpqaEAgE2LZtG0pKSgAA165dg76+Pn7++WcAtdedpKQkbNy4ET169EBWVhZGjhyJjh07SlVPZImiX0vj4uIAoMoXamNjYwDA1atXAdC1VB405bosiYp9fpis0dPTAwBu/BOqy7JPkmtqhaNHj8LDwwPLli3D6NGjsWrVKhQXF3Ovx8TEYObMmdi0aRPGjx8POzs7ANX3V6gfSP3A6shC/NKQ634gI5UAYEFBQRKvn5GRwQCw9evXV1r+8OFDBoD99ddf3LJhw4axqVOncn/37t2bubi4cH/b29uzTp06VdpOdcs2bNjA9u/fzxhjrLS0lJmamjI9PT329u1blpKSwmxtbRkAtmDBAhYbG8vu3r3LmjVrxqZNmybxcVWntLSU6erqsr1791Z5raioiAFgHh4eVV77+++/mZqaGgsICKh1+48fP2YA2LBhw7hlZWVlLC4ujpmYmHDLmkLZBgUFsYb8+Em7fUmPrbbyYYyxhIQEpqGhwV68eMEYY0wkEjEArF+/fuzbb79ljEl/ruLi4tjixYsZAHbkyBFu+ebNmxkAduzYMW6ZWCxmgwcPZuXl5RLFW5e0tDTm6urKADB3d3cWHR3N8vLymIWFBTMyMmILFixgcXFx7M2bN2zgwIHMyMio0vvr2n91dbCu8qlLdeUlyfn9/vvvmba2NlNWVmaLFy9mERER7OjRo0xHR4epq6uz5ORkxhhjI0aMYO3bt6+0z549e7IBAwZwf1d3XJK2A4wx9sUXXzAALD8/X2bir659qkld5/D7779nANitW7e4ZcXFxczCwoL7u7a6c+7cOWZiYsKUlZXZmjVrmI+PDzM3N2dJSUl1xsaY7LU/jCn2tdTLy4sBYGfOnKmybrNmzdjQoUMZY3QtrY6DgwNzcHCQ6j0NvX1FqMs19fkOHjzIADBvb+9Ky0NCQhgAtmLFCsYY1eXqSPv9Q1rStsuSXlO3bt3KBg4cyEpKShhj7+q/sbExGzp0KNcX69KlC4uMjGSMMVZQUMCsrKwYYzX376gfSP1A6gfy1g88THcONSKBQIDevXtzf/fo0QP379+XahvJycnYtm0bXF1dAQDKyspwcHDA69evcfr0aejp6XG/Uq5btw6mpqbo06cP+vfvj+jo6E+K/+TJk+jTpw9mzJgh1fvGjBmDN2/ewMnJSaL179y5A0tLS1haWsLCwgJDhw7F69eva32PvJct3yQ5trrKBwC8vLzQqlUr7hfxlStXAgDc3d2xdetWANKfq27dumH8+PFVls+dOxetWrWqNIvB+fPnMW3aNAgEAonirYuuri4GDBgAAPDw8ICZmRk0NTUxatQoxMfHY/bs2ejWrRtatGiB4cOHIz4+Hunp6RKXV3U+tS5XV16SnN8NGzZgzJgxUFJSwqZNmzBs2DBMmjQJO3bsQEFBAXbu3AkAUFdXr7JPDQ2NOuOSth2QtfilUdc5XLBgAYRCIXbt2sUtCwkJgb29PYC6686oUaMwaNAglJWVwcXFBW5ubrhx4wYMDAzq9Thklby39x9eS5OSkgAALVq0qLJuixYtkJqaCoCupU1RUytvMzMzCAQC7m6FCkVFRQD+N44I1WXZJ8k1NS0tDatWrcLcuXO5caFat26NlStX4tKlSwgICIBYLMbTp0+58lBTU8PSpUsB1Ny/+xD1A6kfKAvxS0Oe+4E0W1kjCg8PB/DuIhkQEICbN29KPTjf1atXIRaLMWfOnErLZ8+eDTU1NQDvKhDwbrC+Cu3bt692rCBJZWdnY/369Th37txHPWtZEZMkzMzMEBERwf0tFou5W1BrIs9lKyvqOjZJyicpKQkFBQXca127dkXr1q3x6tUrbtnHnKv3Y6qgoaEBkUgEb29vZGRkQEdHB0FBQdwMApLEK4mKcqkYXBN4Vy5A5UEyO3ToAADIyMiArq7uR++/PupydeUlSd1VV1eHsrJypeOaMGECmjVrhgcPHkgVQ3WkaQdqei+f8UuqrnPYvn17ODo6wt/fHxs2bICOjg4OHz7M3QYvSd1RUVGBUCis8iiSIpDn9r66a2nF7e3vt50VCgoKuLbl/ZgkQddS2dfUyrtLly6YNWsW9uzZg02bNmHOnDl4+vQp91hGx44duXWpLsu+uq6p169fx9u3byu1UQC4L7gRERFwcXHByJEj8e233+Lhw4fYuHEjJkyYwK1bXX+lOtQPpH6gLMQvKXnuB1JyqBGVlZXhl19+we3bt7Fo0SJYWFhUeW63Lo8ePYKGhka9zA4mjcWLF2Pbtm1o27Zto+4XeFf5//3vf9e6jjyXrbyQpHzGjBmDwMBAhIWFYfjw4cjJycHbt28xatQobp36OFcV3N3dsW3bNvj7+2PGjBlQVlZGy5YtJY73Y1WXIK1YVl5e/kn7r8/yqQ9CoRAGBgaVxo2SJ3zEL8k5XLx4MQ4ePAgfHx8sW7YMGRkZ3DTM1BbVTp7b++qupRUdu9zc3ErrlpSUoLCwEF27dq2XfdO1VPY0xfL28fFB9+7dce7cOVy6dAkjR45Ely5dEBUVJdVU9rWhusyf96+pCQkJAICsrKxK6+jo6EBdXR3JyckA3o1J5Obmht27d+P48eM4fPgwrK2t6yUe6gc2POoHSk+e+4H0WFkjKS8vx5gxYxAXF4ejR49WmZVEUurq6khMTERiYmKV1ypuY6xv3t7emDBhAoYMGdIg25fEmDFjanxNnstWnkhSPs7Ozti9ezdEIhF++OEHLFmyBAcPHsSgQYMA1N+5qtCtWzcMHjwYe/fuRVBQEJydnaWKtyF9zP7ru3zqS0FBAUxMTPgO46M1VvxPnz5FQUGBROewf//+GDRoELy9vXHmzBmMHTuWe43vuivL5Lm9r+la2qNHDygrK+Ply5eVlr948QIA6rXu0rVUdjTV8lZSUsLixYtx4cIFnD17Fm5ubggODsakSZPQunXretsP1WX+VFxTK6ZJr2nw7Yq2SygUIiAgAAEBARAKhRg1ahQ3OPmnon5g46B+oGSaQj+QkkOfSNLb/G7evIkLFy5g2LBh3DKxWFzp/UpKSsjPz6/0vg+X9ezZE4wxfPfdd5XWe/78ObZv3/4RR1C7wMBAqKmpVboFFABCQ0Ol2k5FFr02FWUh7a2T8lq28kaS8ql4tvzevXvw9PTE3r17K9Wdjz1XtXF3d8eDBw/g6+sLGxsbqeJtSJLs/8NjlaR8GltKSgrS09Ph4OAA4F0nLz8/H2VlZdw6+fn5lT7jNZ1DSdoB4OPbgurUV/x1xcIYw9y5c3H37l2Jz+HSpUuRnJyMpUuXwtHRkVvOd93lgyJfS/X19TF16lRcunSp0muXLl2CqqoqJk+ezC2ja6nsa+p1WVqLFi0CYwxbtmyptJzqsnx6/5pqaWkJLS2tKmNMJSYmoqCgAOPGjUNxcTF8fHwAAE5OTrh+/ToYY5UeE/wQ9QOpH0j9QP7qLj1W9okqMnofjhWQl5cHAHj79i2A/91qeODAAZibm+PWrVuIjY1Famoq7t+/j7Zt28LAwAAZGRmIjo7GmzdvYG5uXmXZoEGD0L9/fwQGBqKoqAgTJ05EXl4ejh07hkOHDgH4363p798+l5aWVu14BrU5e/Ys/vzzT8yYMYMbMIsxhvv378PU1BS2trbcuhXH+f4HrUJoaCgmT56MPXv2cB/M6uTk5ABAnReEplC2sqiuY7Ozs6uzfDZt2oRLly6hT58+0NfXR4sWLdC6dWvu16WPPVcpKSkAqs+WOzg4YNGiRbCzs6v0PLgk8Uqior69Xy4VyzIyMrhlb968AQBu+lZJ9v/hsVZcOGorH0ke7ayuvCStu8XFxbh37x43kN769evx1VdfwdzcHMC7C9aRI0ewYcMGTJkyBYcPH0ZxcTFevXqFu3fvom/fvtWew6tXr0rUDrxfvrm5udwgvXzHX7H/inbqfbm5uVi0aBFatmzJPQsvyTkcN24cOnTogN69e1f6RV2SulPRscnJyYG2tnat5SkPFP1aumLFClhaWiImJgZ9+vRBSUkJvLy8sGrVKq6+0LVUPjTlulyhtj7f+3799VccPXoUZ8+eRbt27bjlVJflR13X1E2bNmH+/PnccAIA8Mcff+Crr76CtbU1iouLsXfvXsybNw/KysowMDDAZ599BjMzMwDV91eoH0j9QOoH8tgPlGi+MwUCKaaSjI6OZtOnT2cAWOfOnVlAQADLyclhN27cYKNHj2YAmJmZGfv7778ZY4zNnTuXaWpqsgEDBrDQ0FB29uxZpqOjwxwcHFh+fj67d+8ea9++PevSpQsLDg5mjLFql2VmZjJnZ2fWpk0bpqury0QiETd1XWhoKOvUqRMDwObPn8/S0tKYr68va9GiBQPA1q5dy0pLS+s8tps3bzI1NTUGoMq/Zs2asczMTG7dCxcuMBcXFwaAGRkZsV27dnHTBTLGWHh4ONPX12cnTpyocX8nTpxgQ4YMYQCYQCBgK1asYLGxsVXWawply5jsTSUt6bHVVj6MMXb69GmmqalZpc50796dW0/acxUWFsbVjX/961/swoULVeL39PRkKSkpVZbXFW9dwsLCWK9evRgA5uzszJ49e8YuXrzI+vbtywCwUaNGsfv377PIyEhmZmbGADAXFxf2/PlzifZfXR2sq3wkifnD8pL0/M6ePZupqqqyxYsXM0dHR/b1118zT09PbkpYxhjLzc1lY8eOZS1atGADBgxgt27dYjNmzGAuLi7s1KlTNR6XJO1AREQEmz9/PldvRo8ezQ4dOsR7/CdOnGBWVlZcXL1792YjRoxgdnZ2zMTEhKmqqjIAbNeuXVKfwzlz5nBl9L7a6o6Pjw/T1dVlAJirqyu7c+dOnfXifbLW/tC19J1bt26xqVOnshUrVrDp06czLy+vSnWXrqVVydpU9k25Lleoq8/HGGN37txhX375JXN0dGRPnz6tsg2qy1VJ8/3jY3xMuy/JNZWxd+dqxIgRzMPDg/3www/st99+49YpKipi/fv3ZyNHjmQbN25k7u7ubPfu3Yyx6vsrjFU9D9QPpH4g3/ErUD/wsIAxHu9Tk0ECgQBBQUGYMmUK36GQJuzw4cOYOnVqg90m2tDbr8nJkychFotha2uL9PR0pKenIzExEffv3wdjDD/99FOjxkOk4+bmBn9/fxQWFvIdykeRt/gZYzA3N8eVK1fQvHnzRttvU21/iOKpuA0/ODhYLrffFO3cuRMaGhqwtLRUyNkUP1ZDf//4mHZZ3q6p5NPJ+zmXt/hlsB8YTI+VKSBdXd0619m7d2+lgbEIqcv9+/exYMEC7pZ6bW1tGBsbAwBGjBgBX19fPsOrQh4/B/IYM6lZWFgYbGxsGrVDQOoPfR5JU1GfdXnu3Ln1ERIhDU4e23B5jJnUTBb7gZQcUkCKPMMCaTgxMTFISkrC+vXrMWbMGHTr1g35+fm4ceMGQkJCsGHDBr5DrEQePwcNHXN+fj43aF5107TKOnmIPzIyEnPmzEH37t3x8OFDXL58me+QyEeSxzaEkOpQXSbVkYdr6qeQx3pP/cDayUP8st4PpNnKCCH1wtnZGatXr4a3tzf69euHNm3awN7eHhkZGdi6dSvU1dX5DpHUYseOHQgJCUFZWRnc3d0RGRnJd0hSkZf4W7dujaKiIty5cwe7du2Cjo4O3yERQgghlcjLNZXUH3k/5/ISv6z3A+nOIUJIvVBWVsa6deuwbt06FBQUQE1NTWaz9qSqefPmYd68eXyH8dHkJf5u3brh+fPnfIdBCCGE1Eherqmk/sj7OZeX+GW9H0jJIUJIvaO7hAghhBBCCCFEftBjZYQQQgghhBBCCCEKjJJDhBBCCCGEEEIIIQqMkkOEEEIIIYQQQgghCoySQ4QQQgghhBBCCCEKjJJDhBBCCCGEEEIIIQqMkkOEEEIIIYQQQgghCoySQ4QQQgghhBBCCCEKjJJDhBBCCCGEEEIIIQqMkkOEEEIIIYQQQgghCoySQ4QQQgghhLzn0aNHSE5O5jsMQgghpNEI+Q6AEEIIIYQQPr1+/RqhoaHcv6SkJKipqUFfX5/v0AghhJBGQckhQgghhBCiUAoKCnD16lUuGXTnzh0oKyujd+/ecHFxga2tLXbs2AGBQMB3qIQQQkijoORQNaZOnYqpU6fyHQYhn4w6tYQQvlD7QxqDg4ODROuVlpbi5s2bCAsLQ2hoKK5du4bS0lL06NEDtra28PT0xJAhQ6ChocG9Z9euXThy5AjVZdJkUF0mhNSGkkMfCAoK4jsEIsdiYmJw7NgxPHnyBFpaWhg4cCAGDx6Mzz//vFHjGDhwINVlIpUXL17g+++/h6enJ7p06cJ3OESOUfvT9JSVlWHbtm24efMmBgwYAJFIhNatW/MdFgDA0NCw2uWMMTx8+BBhYWEICwvDpUuX8ObNG7Rv3x62trZwd3eHra0t2rZtW+O2lyxZAkdHx4YKXe5lZmZi/vz5WLNmDUxNTfkOR+4NHDiwQbetyO1yZmYmLl26hIsXLyI1NRXGxsaYNm0aevTowXdohMgUAWOM8R0EIU3Nq1evEBgYiL179+LJkycwMTHB1KlT4eLigi+++ILv8AipVu/evWFpaYmdO3fyHQohRAaFh4djwYIF+O9//4vly5dj5cqVUFVV5TssTnJyMqKiohAaGoqzZ88iMTERmpqasLCwgK2tLWxtbWFmZkZ3T9STU6dOYcKECcjKyoK2tjbf4RBSSVlZGSIiIuDj44Pjx4+jRYsWmDJlCubPn4/evXvzHR4hsiiYkkOENLDY2Fj4+flh//79SE1NRb9+/eDq6gpnZ2fo6OjwHR4hnM2bN2P9+vVISUmBmpoa3+EQQmRQUVERNm7ciE2bNqFjx47w8vKCra0tL7Gkp6fj4sWL3LhB8fHxUFNTw6BBg2Bra4tBgwZhwIABEArpRvmGsG7dOvj5+eHZs2d8h0II559//sG+ffuwf/9+pKenw8bGBq6urnB0dKS+DSG1o+QQIY2l4hcMX19fHDt2DKWlpbCzs4OjoyNdsIhMSE1NRfv27REYGEiPUhBCavX8+XMsXLgQ//nPf+Di4oLNmzejTZs2DbrPt2/f4tq1a1wy6O7duxAIBOjTpw93Z5CVlRWaN2/eoHGQdyZMmAAVFRUEBwfzHQpRcEVFRTh9+jR8fHwQFhYGAwMDuLi4YM6cOejcuTPf4REiLyg5RAgf8vLycOLECQQHB+PcuXPQ1NTE2LFjIRKJMHz4cLrlnfBm9OjRUFZWxpkzZ/gOhRAiB06fPo2FCxciNzcXa9euhYeHB5SVletl26Wlpbh37x6XDLp8+TJKSkpgZGTEJYPs7OzokSaedOzYEXPmzMHKlSv5DoUoqOjoaPj6+sLf3x/5+fkYMWIERCIRJk2aVG/tECEKhJJDhPAtKSkJR44cwYEDB3D37l0YGhrCyckJs2bNooGBSaM7dOgQXF1d8erVK+jp6fEdDiFEDhQUFOCXX37Bhg0b0L17d+zYsQMWFhZSb6e8vByPHj3ixg06f/488vLyoK+vDysrK9ja2uLLL79Eu3btGuAoiDQyMzOho6ODs2fPYvTo0XyHQxRITk4ODh8+jB07diAmJgYmJiaYMWMGZs2aBV1dXb7DI0SeUXKIEFkSGxuL4OBg7N+/HwkJCTA1NYVIJMKMGTNqnVGFkPpSVFQEfX19rF69GosXL+Y7HEKIHPnnn3/g4eGB8PBwODs7Y+vWrXXOahYfH8/dGRQeHo7MzEzo6upi2LBh3N1BRkZGjXQERFJhYWGwtbVFSkoK/ZBAGlx5eTnCw8Ph6+uLI0eOQCgUYsKECRCJRLyNeUZIE0TJIUJkUXl5Oa5evQo/Pz8cPHgQBQUFsLa2hqurKyZPngwNDQ2+QyRNmJubG65fv44HDx7wHQohRM4wxuDn54d///vfEIvF2LBhA9zc3LjHpVNTU3H58mXuzqCEhARoaGjA0tKSZhSTI5s3b8bmzZvx+vVrvkMhTVhSUhL8/f3h4+OD+Ph49OvXD+7u7nByckKLFi34Do+QpoaSQ4TIuopB9nx9fXH+/Hmoqalh/PjxcHR0xOjRo2kWFlLvIiMjMXjwYMTExNB0r4SQj5KTk4M1a9bA29sbpqam6N+/P+7du4c7d+5AWVkZvXv35pJBQ4YMgaqqKt8hEyk4OzsjOzsbZ8+e5TsU0sSUlJTg/Pnz8PPzw/Hjx6GpqQlHR0csWLAAvXr14js8QpoySg4RIk8yMzNx9OhR+Pr6IioqCu3atcPkyZPx1VdfwczMjO/wSBPBGIOxsTHGjx+P3377je9wCCFypLCwEFFRUYiMjERUVBQuXryI0tJSCAQC9OrVC0uWLMGECROgpaXFd6jkE3Tr1g2TJk3CTz/9xHcopIl4/Pgx9u/fj3379iEjIwM2NjZwd3fH+PHjKXlMSOOg5BAh8urRo0cICgqCn58f4uPjYWpqCkdHR8ycORMdO3bkOzwi59atW4ft27cjMTERKioqfIdDCJFRZWVliImJ4cYNioyMRFFRUaUZxaytrXH27FksXboUqqqq2LBhA0QiEd+hk49UUFAALS0tHDp0CA4ODnyHQ+RYYWEhzpw5w01B365dOzg7O2Pu3Lno1KkT3+ERomgoOUSIvKsYnyg4OBiBgYHIysqCpaUlRCIRpk+fDk1NTb5DJHLo5cuXMDIywunTp/Hll1/yHQ4hREYwxvDgwQOEh4cjPDwcly5dQl5eHtq1a4fhw4dz/6qbUSwrKwsrVqzA7t27YW1tDW9vb5iYmPBwFORTXLt2DQMHDsSzZ8/w+eef8x0OkUPR0dHw8fHBwYMHUVJSgnHjxsHV1RVjxoyhKegJ4Q8lhwhpSoqLi3HhwgX4+fnh5MmTUFJSwtixY+Hq6opRo0bRHSBEKkOHDoWenh6CgoL4DoUQwqMnT55wyaCLFy8iPT07QhMbAAAgAElEQVQdrVq1wrBhwzB8+HDY2NhIleS5cuUKFixYgH/++QeLFy/G2rVr0bx58wY8AlKftm/fjhUrViAnJ4cGDicSy87ORnBwMLZv34579+5xM/J+/fXX0NHR4Ts8QgglhwhpurKzs3H69Gn4+fkhLCwMrVq1wuTJk+Hq6gorKyu+wyNyYM+ePfDw8EBycjJatmzJdziEkEby+vVrXLlypdYZxfr27QslJaWP3kdpaSm8vb3xww8/oE2bNvjjjz8wZsyYejwK0lDc3Nzw9OlTXLx4ke9QiIz7cAp6FRUVTJs2jfqihMgmSg4Rogj++9//4uDBg9izZw+ePn2Kbt26YcqUKRCJRDAyMuI7PCKj8vLyoK+vjy1btmDOnDl8h0MIaSDp6em4ePEiN4h0dHQ0hEJho8wolpycjO+//x5+fn6wt7eHl5cXjZsn4/71r39h8ODB2Lp1K9+hEBlVMQX9rl278OLFC24KemdnZ2hoaPAdHiGkepQcIkTRREdHw9fXF4cOHUJGRgYsLS3h6OgIZ2dnuq2XVOHs7IyXL18iKiqK71AIIfUkPz8f169f5waRvnPnDpSUlNCnTx8uGWRlZdWoj3pFRERgwYIFSEhIwPLly7Fy5UqaoUgGlZaWQlNTE7t27aJBxUklxcXFOHXqFHx9fXHu3Dno6upiypQpmD17Nnr27Ml3eISQulFyiBBFVVZWhoiICPj6+uLo0aMoKyuDnZ0dRCIRTRtKOOfPn8eoUaPw+PFjdO3ale9wCCEfoaCgAFevXuVmE7t58ybEYnGlGcXs7Oygra3Na5xisRhbtmzB2rVr0aFDB3h5ecHOzo7XmEhl9+/fR+/evXH//n36wk8AvJs998CBA9i7dy8yMzO5KegnTJhAY10SIl8oOUQIAXJzc3Hy5ElufCJtbW3Y29tDJBJh+PDhNOCkAisvL0fHjh0xc+ZM/Pjjj3yHQwiRQGlpKe7du8fdGXTlyhUUFxdXSgbZ2NigdevWfIdarfj4eCxcuBBnz56Fo6Mj/vzzT7Rt25bvsAiAAwcOYM6cOXjz5g198VdgeXl5OHHiBPz8/BAaGgpDQ0M4OTlh3rx59FgoIfKLkkOEkMoSExNx9OhR7N+/HzExMejQoQOmT5+Or7/+GsbGxnyHR3jw3XffISgoCPHx8Z80AC0hpGGUlZUhJiaGuzPo8uXL3JhhVlZWsLW1xejRo2FoaMh3qFI5ffo0Fi5ciNzcXKxduxYeHh40zTXPvv32W0RFReHWrVt8h0J4UDEFfWBgIMRiMcaNGwd3d3f6IZGQpoGSQ4SQmsXGxsLPzw8HDhzA69ev0a9fP7i6umL69Olo06YN3+GRRhIXF4fu3bsjIiICw4YN4zscQgje3V1TcWdQaGgosrOz0bZtWwwZMoS7O6gpTDhQUFCAX375BRs3boSpqSm2b9+OAQMG8B2WwhoyZAhMTEzg4+PDdyikkVRMQe/l5YUHDx7QFPSENF2UHCKE1K28vBxXr16Fn58fAgMDUVhYCGtra7i6usLBwQHq6up8h0ga2L/+9S/07NkT+/bt4zsUQhRSRTIoMjIS4eHhSEpKgqamJiwsLLhkkJmZWZP99f7Jkyfw8PBAaGgoXFxcsGXLFvpi2sgYY2jZsiU2bNiAefPm8R0OaUAVU9D7+PjgxIkTUFdXx9SpU+Hu7o5+/frxHR4hpGFQcogQIp3CwkKcOXMGvr6++M9//gMNDQ2MGzcOIpEINjY29NhRE/Xnn39ixYoVeP36NVq0aMF3OIQ0eSkpKYiMjERoaCjOnz+PhIQEaGhowNLSkksG9e3bV+Ha3ODgYCxcuBBisRirV6/GwoULFa4M+PLs2TMYGxvj2rVrdPdWE5WYmIiAgADs2LEDCQkJNAU9IYqFkkOEkI+XnJyM4OBgBAcHIyoqCu3bt8ekSZMwc+ZM9OnTh+/wSD3KzMyEgYEB9uzZAxcXF77DIaTJSUtLw6VLl7i7g+Li4qCmpgYzMzNu3KAhQ4bQTJJ4N4nC6tWr4e3tDUtLS2zfvp1mzmoEwcHBmD59OvLy8uiO4SakYgp6Hx8fhIWFQU9PDyKRCLNnz8YXX3zBd3iEkMZDySFCSP2Ii4vD4cOH4evrixcvXnDPpH/11VfQ09PjOzxSDyZMmIC3b98iJCSE71AIkXsZGRm4dOkSLl68iIiICMTGxkJFRQXm5uawsbGBtbU1LC0t0bx5c75DlVkxMTGYP38+bt26hfnz58PT0xNaWlp8h9VkrVy5EidPnkRsbCzfoZB6EBcXB19fX+zZswfZ2dmwtramKegJUWyUHCKE1K/3xyc6dOgQ8vPzYWlpCZFIBCcnJ3okSY4dP34cDg4OePnypdzNekQI37Kzs3H58mVEREQgIiICDx48gJKSEszMzDBs2DBYW1tj8ODB1EZKiTEGPz8/LF26FCoqKti4cSNEIhHfYTVJo0ePRuvWreHv7893KOQj5eXl4dChQ/D19UVUVBSMjY3h5OSEWbNmoUOHDnyHRwjhFyWHCCENp6ioCCEhIfDz88OJEycgFAphb28PV1dXjB49GkKhkO8QiRRKSkrQrl07LF26FN9//z3f4RAi0/Lz83H9+nXuMbGbN29CLBbDyMiIGzPI1tYWLVu25DvUJiErKwvr1q2Dl5cXhg4dCm9vb3Tr1o3vsJoUfX19LFu2DEuXLuU7FCKliinoAwICUFZWhrFjx9IU9ISQD1FyiBDSOLKysnDkyBH4+vri6tWr0NfXh4ODAxwdHWFlZcV3eERCHh4eCAkJwT///MN3KITIlLdv3+LatWuIjIxEVFQULl26VCUZZGNjg9atW/MdapMWGRmJBQsW4NGjR5g3bx5+/vlnGki3HqSkpMDAwABhYWGwsbHhOxwigdevXyMoKAh//fUXHj58yD3uP3v2bGqHCCHVoeQQIaTxJSQk4NChQ/jrr7/w7NkzmJqawtHREV999RU6d+7Md3ikFrdu3YK5uTlu3LgBc3NzvsMhhDcFBQW4c+cOoqKiEBoaisuXL6OkpIRLBg0aNAjDhw9Hu3bt+A5V4ZSWlsLb2xurV6+Gjo4O/vjjD3z55Zd8hyXXzp49iy+//BKZmZlo1aoV3+GQGtQ0Bf3cuXPRt29fvsMjhMg2Sg4RQvgVHR0NX19fHDx4EJmZmdz4RNOmTaOBRWVUz549MXToUHh5efEdCiGNprS0FPfu3UNoaChCQ0Nx5coVFBcXw8jICIMGDYKVlRVGjx5N43HJkJSUFHz33Xfw8/ODvb09/vzzT3Tq1InvsOTS+vXr8ddff+Hly5d8h0Kq8fTpUwQEBGDfvn1ITEzk+lIuLi40sxwhRFKUHCKEyIbi4mJcuHABwcHBOHr0KMrLy2FrawuRSEQzZ8iYjRs34tdff0VycjKaNWvGdziENIgPk0GRkZEoKiqCvr4+N7X8yJEj0bFjR75DJXWIiIjAggULkJCQgOXLl2PFihXUdklp8uTJKC8vx/Hjx/kOhfx/H05Br6+vD1dXV7i5ueHzzz/nOzxCiPyh5BAhRPbk5OTg1KlT8PPzQ1hYGLS1teHo6AhXV1cMGjSIBk/kWVJSEjp27IjDhw9j0qRJfIdDSL0oKytDTEwMlwi6fPky8vLyKiWD7Ozs6NFXOSUWi7F9+3asWrUK+vr68PLywogRI/gOS24YGRlhxowZWL16Nd+hKLzY2Fj4+fnhr7/+Qk5ODjcF/cSJE2miD0LIp6DkECFEtr169QqBgYHYt28f/vnnH3Ts2BHTpk3D7Nmz8cUXX/AdnsIaMWIE1NXVceLECb5DIeSjVCSDKgaQvnDhAnJzc6Gnp4fBgwdz4wZ1796d71BJPYqPj8eiRYvw999/w97eHjt27ED79u35Dkum5eTkoFWrVjh16hTs7e35Dkch5ebmIigoCLt27cKdO3fQtWtXzJw5EzNmzEDbtm35Do8Q0jRQcogQIj8qfi3bv38/UlNT0a9fP7i6usLJyQm6urp8h6dQ/P39MXPmTCQmJlLHlMiFsrIyPH78mBtAOiQkBDk5OWjTpg2GDh3KjRtkZmZGdycqgNOnT2PRokXIzs7GunXr4OHhAWVlZb7DkkkXL16EtbU1EhMTaYD1RlYxBb2/vz/Ky8tpCnpCSEOi5BAhRP6UlZUhIiICvr6+OHbsGEpLS2FnZwdHR0c4ODjQ4IuNoKCgAHp6eli/fj0WLVpU6bW8vDwaTJzIhPj4eG7MoLCwMGRlZUFXVxcWFhbco2KUDFJchYWF2LRpEzZu3Ihu3bph+/btsLS05DssmbN161b8/PPPSE9P5zsUhZCSkgJfX19uRteKH8JcXV1ppjhCSEOi5BAhRL4VFhbizJkz8PX1xX/+8x+0aNECY8eOhUgkol/WGtisWbNw//593L59G0VFRTh58iT27duH5ORk3L9/n+/wiAJ6PxkUHh6OzMxMaGpqwsLCAra2trC1tUXfvn2hpKTEd6hEhjx9+hQeHh4ICQmBi4sLfvvtN4W9G/X333/H2bNnYWZmhr59+6Jv377w9PREamoqzp8/z3d4TVbFj14+Pj44fvw4WrRogSlTpmDevHno06cP3+ERQhQDJYcIIU1HUlISjhw5Al9fX9y5cweGhoZwcnLCzJkz0bVrV77Da3IiIiJgY2ODyZMn4/z58ygoKABjDB07dsSLFy/4Do8ogIpkUGRkJMLDw5GUlIQWLVpgwIABlAwiUjt9+jTmz5+PgoICrF69GgsXLlS4uuPn5weRSAShUIiysjIwxqCqqorOnTtj5MiR6NOnD/r27Yvu3bvTLKL14MmTJ9i7dy/279+P9PR02NjYwNXVFY6OjlBTU+M7PEKIYqHkECGkaYqNjUVwcDAOHDiAly9fwtTUFCKRiAZvrAeJiYkICAjAjh07kJCQABUVFYjFYu719u3b49WrVzxGSJqq+Ph4bgDpc+fO4dWrV9DQ0IClpSU3gLSFhQV9aSUfLTc3F6tXr4a3tzf69u2L7du3o3///rW+p7S0FEpKSk0ikXTjxg0MGDCg2tdUVVVRWlqK8vJyqKio4Pbt2+jVq1cjRyj/ioqKcPr0aW4KegMDA7i4uMDd3R1GRkZ8h0cIUVyUHCKENG3l5eW4evUq/Pz8cPDgQRQUFMDa2hqurq6YNGkSWrRowXeIciM0NBRr167F1atXoaKigpKSkmrX09PTQ0pKSiNHR5qi5ORkbgDp8+fPIyEhAerq6hg4cCA3gPSQIUOgqqrKd6ikibl37x7mz5+PmzdvYv78+fD09KxxLLUtW7YgPT0dGzZsaOQo6192dnad49oIhULY29vj+PHjjRRV0xAdHQ1fX1/4+/sjPz8fI0aMgEgkoinoCSGygpJDhBDFUfFrna+vL86fP891cF1dXTF69OiP6pyJxWKFuUshISEBZmZmyMnJQXl5eY3r6ejo0MCl5KOkpKQgMjKSm03sxYsXUFNTg5mZGTeA9ODBg9GsWTO+QyUKgDEGPz8/LFu2DEKhEBs3boSrq2ulseySkpJgbGyMoqIi+Pv7w8nJiceI64e2tjZyc3NrfF1VVRX//PMPOnXq1HhByYD4+Hip7+zJycnB4cOHsXPnTty9excmJiaYMWMGZs6ciTZt2jRQpIQQ8lEoOUQIUUxZWVnc+ERRUVFo164dJk+ejK+++gpmZmYSbaOsrAwWFhb4888/FWaGm6tXr2LYsGGVHiP7UMuWLZGVldWIURF59fr1a1y5coUbNyguLg5CoRC9e/fmxgyysrJC8+bN+Q6VKLDs7GysXbsWXl5eGDJkCLy9vWFqagoAcHR0xMmTJyEWi6GqqorIyMg6H0OTdf3798ft27erfU0oFGLlypVYt25dI0fFr5MnT8LZ2Rm3bt1Ct27dal33/TuW/f39wRiDvb09TUFPCJF1lBwihJDHjx/j0KFD8Pf3x/Pnz2FqagpHR0fMmDGj1l9GQ0JCMGLECAiFQuzatQuzZs1qvKB5dODAAcyYMaPG1zU1NZGXl9d4AZEGd+PGDYSEhGDVqlWftJ20tDRcunSJGzfozp07UFZWrpQMGjRoEA3ESmTS7du3MX/+fMTExGDevHkYPnw4xo8fz70uFAqhra2NmJgYtGvXjsdIP83MmTPh7++P0tLSSssFAgHatGmD58+fQ0NDg6foGt+WLVuwbNkyAMCSJUuwefPmatdLTk6Gn58ffHx8EB8fj379+sHd3R3Tp0+HpqZmY4ZMCCEfg5JDhBDyvooxAQIDA5GVlQVLS0uIRKJqO3cikQiHDh3i7qJZuHAhtmzZohBjByxZsgR//PEHysrKqrympqaGgoICHqIi9a2wsBCrV6/Gli1boKOjg9TUVKnen56ejuvXr3PjBt25cwdKSkro06cPlwgaOnRojWO5ECJrSktL4e3tjY0bN0JVVRVJSUmV2kEVFRWYmpri2rVrcpvk/Pnnn/Hjjz+iuLi40nKBQIDAwEBMmzaNp8gaV1lZGRYtWoTt27dzy7S1tfH69Wvu0dYPp6DX1NSEo6Mj5s+fj969e/MVOiGEfAxKDhFCSHWKi4tx4cIF+Pn54eTJk1BSUsLYsWPh6uqKUaNGQSwWQ0dHB4WFhdx7lJWVYWFhgePHjzf5sQTKy8sxZswYhIWFVfl1WVVVtcqXCiJ/IiMjIRKJ8OrVK+4cP3nyBMbGxjW+582bN7hx4wZCQ0MRGhqKu3fvQiAQoE+fPtwA0nZ2dtDW1m6swyCkQaxZswY//fRTtQlyoVCIqVOnwt/fn4fIPt2RI0cwZcoUvP8VQSgUwszMDNevX1eIx6Ly8/Ph4OCAkJCQSmPsCQQCHDlyBN27d8e+ffuwb98+ZGRkwMbGBu7u7hg/fjwNkE8IkVeUHCKEkLpkZmYiKCgI/v7+uHbtGtq2bQtra2sEBQXhwyZURUUFenp6OHv2LHr06MFTxI0jLy8P/fv3R3x8fKUEkbKycpWEEZEfhYWFWLduHX799VcoKSlx51JZWRk7duyAm5sbt25+fj6uX79eKRkEACYmJtwA0ra2tmjZsiUvx0JIQ0hISEDXrl1rTYIrKSlh48aNWL58eSNGVj/u379f5a4XgUCAGzduyP14SpJISkrCyJEj8eTJkyrj6ykrK6NNmzZISUlBp06dMHPmTMycOROGhoY8RUsIIfWGkkOEECKNZ8+eISAgAEFBQXj69Gm1SRChUAihUIiAgABMmjSJhygbz4sXL9C3b1+8efOm0q+r5eXlCvHrclNT3d1CFYRCISZOnAh3d3duAOmbN29CLBbDyMiISwTZ2NigdevWPB0BIQ1vzJgxCA0NrXVgfuBdQuXUqVOwt7dvpMjqR2FhITQ0NLgfP1RUVCASifDXX3/xHFnDu3nzJr788kvk5ubWeH4FAgH8/Pwwffp0KCkpNXKEhBDSYCg5RAgh0kpLS4OBgUG1jxNUqEiM/Pvf/8bPP//cpDuQoaGhGDVqVKXyKCkpgYqKCo9REWkUFBRUuluoprqtoqICsVgMU1NTWFtbw9raGkOHDoWOjk4jR0wIP06cOIGJEydKtK6SkhLU1NQkmuFK1ujr6+P169cAAHV1dTx//hx6eno8R9Wwjh07BicnJ5SVldV696uKigpWr179yQP0E0KIjAluut9WCCGkgQQGBtZ5VwxjDIwx/Prrr7C3t2/Ss3fZ2tpi27Ztlcqkrl/Uiew4f/48jI2NsWXLFjDGak16isViREVFITY2Fl5eXpg8eTIlhohCycvLw/jx47lx5QQCAZo1a1btNaG8vBzFxcUYM2YMsrOzGzvUT2JiYgLg3WNUnp6eTT4x9Pvvv8PBwQElJSV1PhYtFouxa9euSnfLEkJIU0DJIUIIkdK+fftq/QL9vvLycoSEhMDCwgIvXrxo4Mj44+HhAXd3d+5vGnNI9uXm5sLNzQ2jR4/G69evJTpnysrKePjwYSNER4hsEolEOHHiBFJTU/Hq1SscOXIE33zzDQYNGsTNTqasrMzdOVlaWor//ve/mDx5sly1i6ampgCA9u3bw8PDg+doGo5YLMbXX3+NxYsXcz/qSCIxMRHh4eENHB0hhDQueqyMEAV27do1bNmyhe8w5EpeXh4uXLjA/Uosybg6FR1OVVVVWFpaQldXt6HD5EV5eTkuX76MjIwMjBs3jmZskWEpKSm4ffu21LPKCQQCtG/fHhYWFg0UWcMLDg7mOwSJODo68h0CkRJjDG/evEFmZiays7ORnp6O/Px8LuFgbGwsN9ObP336FPfu3YOVlVWTvWtILBbj2rVrSEtLAwDu8W9Jk0SGhoZy3RbybcmSJbC0tOQ7DELI/wQL+Y6AEMKfil89HRwc+A5FrpiZmXH/r+nxqfLy8mp/JU5LS4OWlhaaNWvWYPHxRUlJCQMHDkR4eDjdbi/D3rx5g/T0dLRt2xZFRUUoLCxEcXExSkpKKq0nEAi4f+Xl5dwXptTUVJ4i/zSJiYm4fv0632FI7MiRIxgwYADat2/PdyhEQgKBAFpaWtDS0kLnzp0BvLtzKCcnB1lZWcjKykJqairatm3Lc6R109TUhL6+fpNNDAHAy5cvoa2tjZYtW0JFRQUCgYC74+v9v2tbTj7OkSNH4OjoSMkhQmQMJYcIIXLzSzqRD7GxsdDT06MZq+RMWVkZ0tLSkJ6ejuTkZKSlpSE1NRUpKSlIT0/Hq1evkJycjOzsbKxfvx5du3blO2SpHD58GFOnTuU7DKksXrwYU6ZM4TsMooASEhJQXFyMLl268B0KaYIosUaIbKLkECGEkHrVvXt3vkMgH0FZWRn6+vrQ19dHr169+A6HEMKjjh078h0CIYSQRkYDUhNCCCGEEEIIIYQoMEoOEUIIIYQQQgghhCgwSg4RQgghhBBCCCGEKDBKDhFCCCGEEEIIIYQoMEoOEUIIIYQQQgghhCgwSg4RQgghhBBCCCGEKDBKDhFCCCGEEEIIIYQoMEoOEUIIIYQQQgghhCgwSg4RQgghhBBCCCGEKDBKDhFCCCGEEEIIIYQoMEoOEUIIIYQQQgghhCgwSg4RQgghhBBCCCGEKDBKDhFCiBx48+YNb9ttqH0TQoi8obaYEEJIU0XJIUJIk3HixAkYGhri0aNHfIdSb7y9vTF48GAMGDCgXre7a9cuDB06FN26dZNqnaZYxu9ryOPLzc3FqlWrMGTIEPTo0QP29vYYN24cvvvuO6xcuRJeXl71vk9C6pOkn4+m2E5QW9y4qC0mhJDGR8khQojcSklJqfS3hoYG2rRpg+bNm/MUUf2bM2cOcnNzUV5eXq/bnT17NsrLy1FWVibVOk2xjN/XUMd3+vRpmJiY4OLFizhw4AAePnyIM2fO4MCBA0hJScGGDRtQUFBQr/usDx9+xpr6fkntavp8UFv88agtrh61xZVRW0wIaQyUHCKEyKXs7Gy4uLhUWmZnZ4fo6Gh07tyZp6jqn1AoRLt27ep9u8rKymjfvr3U6zTFMn5fQxxfZGQkJk+ejA4dOiA8PLzStlu2bAlfX19MnTpV5r6QVPcZa8r7JXWr7vNBbfGnoba4etQW/w+1xYSQxkLJIUKI3CkoKMC0adMQHx/PdyiE1GnhwoUQi8Xw9PSEqqpqtev8+OOPMvWFhK/PGH225QudLyJPqC2W/f0SQvhFySFCiMRiYmKwfPlyGBkZ4e3bt5g9ezZ0dHRgbm5eqQPBGMPOnTsxb948WFhYYMSIEXj69GmlbUVHR8Pd3R1OTk4wNzfHrl27UFpayr2empoKNzc3eHp6ws3NDRMnTkRmZiYA4Pjx43j06BEyMjLg5uaGzZs3Izs7G3v27IGdnR1OnDgBALh8+TJ0dXUhEAiwatUqbtthYWHQ0tLCmjVrJI63LrVtIzY2FitXrkTXrl2RlJQET09PdOzYEd27d0dERASKioqwePFifP755+jQoQPOnz9f7T4uXryIUaNGoVWrVhg5cqTUZX7y5Em4u7vju+++w8KFC6u9Xby2daorY0nrBFD3OY+JicHMmTOxadMmjB8/HnZ2dhKXf1xcHP7v//4PpqamSE5OxoQJE9CqVSuYm5vj+vXr3Hq11avqji8pKQkbN25Ejx49kJWVhZEjR6Jjx47IzMxEVFQUDA0Nce7cuRrjevjwIWJiYqCtrY0RI0bUuF6XLl0wf/587u+jR4/Cw8MDy5Ytw+jRo7Fq1SoUFxdLXeZnz57F/Pnz8c0338DS0hK7d++WqCyq+4wBtdczSeOq7/0Syes/UHvdAmr+HFb3+aC2mNriD1FbTG0xIUSOMUKIwgoKCmLSNAMpKSnM1taWAWALFixgsbGx7O7du6xZs2Zs2rRp3HobNmxg+/fvZ4wxVlpaykxNTZmenh57+/YtY4yxhIQEpqGhwV68eMEYY0wkEjEArF+/fuzbb79ljDE2bNgwNnXqVG6bvXv3Zi4uLtzf9vb2rFOnTtzfcXFxbPHixQwAO3LkCLd88+bNDAA7duwYt0wsFrPBgwez8vJyieKVRG3bSEtLY66urgwAc3d3Z9HR0SwvL49ZWFgwIyMjtmDBAhYXF8fevHnDBg4cyIyMjCpte9SoUax169Zs1qxZ7Ny5c+y3335jqqqqzMDAgIuxrmMICAhgFhYWrLCwkDHGWHp6OtPR0WF6enrcfupap7oylrROSHLOu3TpwiIjIxljjBUUFDArKyuJy//7779n2traTFlZmS1evJhFRESwo0ePMh0dHaaurs6Sk5MZY7XXq+qO79y5c8zExIQpKyuzNWvWMB8fH2Zubs6SkpLY33//zdTU1FhAQECNce3Zs4c7Tklt3bqVDRw4kJWUlDDGGMvIyGDGxsZs6NChrLy8XOIy9/X1ZdOmTWNlZWWMMcZ++uknBoCFhYXVWRaMVf2MMVZ7PZM0rvrer6Skbe/4BoAFBQVJtK6k9b+uusVYzZ/DmtpYaoupLX4ftcXUFktCmvaNENJoDstPL4kQUu8+5svSihUrGACWkW5+nBoAACAASURBVJHBLbOysmLGxsaMMcaSkpJY27ZtuU4QY4ytXr2aAWCHDh1ijDG2fPlyZmhoyL3++PFjBoDt2rWLW2Ztbc1+/vln7m9nZ2fWq1cv7u/qOi0XL16s8oUkPz+ftWrVik2ePJlbdubMGebt7S1xvHWRZBve3t4MALt//z63zpo1axgAdvfuXW7ZDz/8wACwtLQ0btmoUaOYgYFBpX1u2LCBAWC///57nft/+/Yt09fXZ4GBgZW2MXHiRO7LhiTrMFZ9GddVJxir+5yXlJQwgUDAfv/9d26d48ePM2k4OTkxFRUVriPPGGPBwcEMAFu9ejVjrO56Vd3xff311wwAe/r0aZV9lpaW1hrTL7/8wgCwESNGSHQMqampTENDg/n6+lZavm/fPgaA+fn5McbqLvO0tDT22Wefsfj4eO719PR0NmnSJBYXF8cYk/4zJkk9l6QuNMR+JdGUk0OM1V3/JalbdX0Oq/t8UFtMbfGHqC2mtrgulBwiRCYdFn7EzUaEEAWmrKwM4N3gnBXat2+PZ8+eAQCuXr0KsViMOXPmVHrf7NmzoaamBuDd7eHvP9PftWtXtG7dGq9eveKWhYeHAwCKiooQEBCAmzdvgjFWa2zvx1RBQ0MDIpEI3t7eyMjIgI6ODoKCgvD7779LHG9dJNlGRbkpKf3vad6KAUZVVFS4ZR06dAAAZGRkQFdXl1uupaVVadsikQgrVqxAdHQ0DAwMat3/lStXkJKSgp49e1Z6vVmzZtz/JVkHqL6M66oTQN3nXEVFBSNHjsS3336Lhw8fYuPGjZgwYUKVfdVGXV0dysrKlcpzwoQJaNasGR48eACg7npV3fGpqKhAKBTiiy++qPHYa2JoaAgAePnypUTHcP36dbx9+5arBxXs7e0BABEREXBxcamzzCMjI1FeXl5pwFUdHR0cPXqU+1vaz5g09by2utAQ+yV1139J61Ztn8PqPh/VobaY2mJqi6ktJoTIH0oOEULq1aNHj6ChoVHpefoPjRkzBoGBgQgLC8Pw4cORk5ODt2/fYtSoUdw6ZWVl+OWXX3D79m0sWrQIFhYWVcbOkJS7uzu2bdsGf39/zJgxA8rKymjZsqXE8dblY7chEAhqXFbXdMkGBgZQU1NDYWFhnfuv+PJV0wCcAPD48eM61/kUkpzzo0ePws3NDbt378bx48dx+PBhWFtbf9J+hUIhDAwMuPE06rNeSaJbt24AgPj4eJSWltb55TohIQEAkJWVVWm5jo4O1NXVkZycLNF+Hz58CLFYDMZYtfUMkL4s6uOzwud+FdH79V/SutUQn8MK1BZTW0xtcVXUFhNCZAUNSE0IqVfq6upITExEYmJildfS09MBAM7Ozti9ezdEIhF++OEHLFmyBAcPHsSgQYMAvOuMjxkzBnFxcTh69CiGDh36STF169YNgwcPxt69exEUFARnZ2ep4q1LfWzjYwgEAvTo0aPO/Vd8yajo7FZHknU+RV3nHHj35SEgIAABAQEQCoUYNWoUHj169Mn7LigogImJSb3XK0l0794dXbt2RWlpKSIjI+tcv+LX5ZpmiDExMZFov1paWigqKkJcXFyV14qLiz+qLOqjnvO1X0VWUf8lrVsN9TkEqC2mtpja4vdRW0wIkTWUHCKE1KuePXuCMYbvvvuu0vLnz59j+/btAACxWIynT5/i3r178PT0xN69eyvdtn7z5k1cuHABw4YN45ZV/PJWQUlJCfn5+RLH5e7ujgcPHsDX1xc2NjZSxVsfx1zfXr58CbFYjClTptS5/169egEAgoKCKr1eXl6OsrIyAJBonU9R1zkvLi6Gj48PAMDJyQnXr18HYwwRERGftN+UlBSkp6fDwcFBonolrbruKhAKhdwsLytWrEBJSUm1671+/RoHDhyApaUltLS0uBl6KiQmJqKgoADjxo2TKK7+/fsDAFatWlUpxujoaPz9998f9Rmrj3rO134V1fv1X5K69TGfQ2qLqS2WBLXF1BYTQmQfPVZGCJFKbm4uAFSa9jYtLY0bw8DOzg79+/dHYGAgioqKMHHiROTl5eHYsWM4dOgQAGDTpk24dOkS+vTpA319fbRo0QKtW7fmfqmruPX6wIEDMDc3x61btxAbG4vU1FTcv38fbdu2hYGBATIyMhAdHY03b97A3Nycm+q3ul+wHBwcsGjR/2Pv3uNzrhs/jr+vXdfICLEcJ6o74SZR5lALN2OYU23NYZKyVUpLuVO3EjelJUnd3dxEUbhtziWniSI5jXGLjrppiM1xB2aHz++PbtevNXZwuL7bvq/n49Gjx77H977X9blm730PTyswMDDXvSYKk7cghdnGmTNn8hy3C9OSk5Pd01JSUiQp16OlnU6nTp48qbS0NJUvX17GGI0dO1avvPKKGjRooNtvvz3f/fv6+qp9+/b68MMPddddd2ngwIH65ptvtHHjRiUlJWnevHnq2bNnoZa52DEu6D1RmNdckmbOnKknnnhCTqdTtWrVUqVKldS8efNCvQYXZGRkaNeuXWratKkkady4cRo4cKD8/f21ZcsWSfm/ry72/aWmpio7O1unTp1S5cqV3dPj4uL0wAMPaMaMGQoJCblkpuDgYI0bN04vv/yy2rVrp8mTJ7t/YTh16pRWrlypGTNmaNasWapataqio6M1ZMgQ92UfkvTOO+9o4MCB7ks7Cjrmbdq0UZcuXbRkyRJ16NBBISEhOnDggE6cOKH333+/UMfij2PsnnvuKfB9XlCuyxnbhdkvfpPf+19Sge+tjIyMfMfhxcYHn8V8Fl8Mn8V8FgMoga79Ta8BFFdFfXpPXFycqVevnpFkhgwZYo4dO2Zmz55tKlSoYCSZ0aNHm6ysLHP8+HHTv39/U61aNXPjjTeahx56yBw6dMi9nU8++cRcf/31RlKu//785z+7l3v88cfN9ddfb1q1amXi4uLMZ599Znx9fU1ISIhJTU01u3btMn5+fqZ+/fomNjbWrF271tx3331Gkrn77rvN6tWr8+QfO3asOXLkSJ7pBeUtjPy2sXbtWnPHHXcYSaZ///7mxx9/NOvXrzfNmjUzkkxQUJDZvXu32bhxo2nevLmRZMLDw81PP/1kjDFm9+7dpk+fPqZz584mMjLSREVF5XqCS2G+h9OnT5tBgwaZ6tWrm5tuusmMHj3aREZGmkGDBpm4uDiTnZ1d4DJr1qzJc4wL+54o6DU/d+6cadGihencubN5/fXXTWRkpJk+fXqRXoPBgwebMmXKmGHDhpnQ0FDz6KOPmrFjx7ofk21M/u+rZcuW5fn+pk2bZm688UYjyQwYMMDs2LHDva3PP//c1KxZ0yxZsqRQ+RISEswjjzxi6tata3x9fU2LFi1Mu3btzJQpU0xmZmauZZcsWWI6depknnrqKfPyyy+biRMnur+Pwh7ztLQ088QTT5jatWub6tWrmyeeeMKcOnWqUMfiYmOsoPdZYXNd7f0WVml/Wllh3v/G5P/eym8cXuozls9iPov/iM9iPosLUtTPNwAeEeMw5grO4QRQosXExCgsLOyKTuW+HEuXLlVmZqY6duyopKQkJSUlKTExUbt375YxRq+++qpH8+Da88RrHhERoY8//lhnz569ColR2lj1eXe5HA6H5s+frwcffLBQy/P+R2HwWYzioKifbwA8IpbLygB41O7du/Xkk0+6b2hYuXJl3XbbbZKkTp06afbs2VbGy+P3jzC+lJkzZ6p79+4eSFMyXelrXtjXAEDpxWfxleOzGACQH8ohAB6VkJCgQ4cOady4ceratasaNmyo1NRUbdmyRWvWrNH48eOtjpgLT+C4clf6mhf2NZg7d26BjwwGSqvU1NRS/f7ns/jK8VkMAMgPTysD4FH9+/fXqFGj9N577+muu+5StWrVFBwcrOTkZE2aNEk+Pj5WR8RV5onXfMqUKVqzZo2ys7MVGRlZqEcVA6UF738UBp/FAID8cM8hwMasvgdHenq6ypUrx18WbYTXHFax+vOuqLgnB64lPothJT7fgGKJew4BsA5nCdkPrzkAWI/PYgDAH3FZGQAAAAAAgI1RDgEAAAAAANgY5RAAAAAAAICNUQ4BAAAAAADYGOUQAAAAAACAjVEOAQAAAAAA2BjlEAAAAAAAgI1RDgEAAAAAANgY5RAAAAAAAICNUQ4BAAAAAADYGOUQAAAAAACAjVEOAQAAAAAA2JjL6gAArBcaGmp1BOCyZGRkyOl0yuXixxnyl5iYaHWEIps0aZJiY2OtjoESJCMjQ2XLlrU6BgCgBOLMIcDG6tSpo5CQEKtjAJftyy+/1M6dO62OgRLAz8+vRH3ehYSEyM/Pz+oYKEFOnjyp5cuX6/Tp01ZHAfIVEhKiOnXqWB0DwB84jDHG6hAAAFyOzz77TMHBwZoxY4YGDRpkdRwAsIQxRvfdd59ycnK0ceNGORwOqyMBAEqWWM7DBwCUWF27dtVf//pXDRkyRM2bN1fTpk2tjgQAHjdv3jxt2rRJW7ZsoRgCAFwWzhwCAJRoWVlZ+stf/qKjR49q+/btuv76662OBAAec/bsWTVs2FCBgYGaPn261XEAACVTLPccAgCUaC6XS3PnztXJkycVGRlpdRwA8Kjo6GidOHFCf//7362OAgAowSiHAAAlnp+fn+bNm6fY2Fi9//77VscBAI9ITEzUm2++qZdfflk1a9a0Og4AoATjsjIAQKkxcuRIvfXWW9q0aZOaNWtmdRwAuKb69u2rrVu3au/evTzCHgBwJWIphwAApUZOTo46deqkAwcOKD4+XhUrVrQ6EgBcE19//bXuueceLV68WD179rQ6DgCgZKMcAgCULkePHlWzZs0UEBCg+fPnWx0HAK66nJwctW7dWhUqVNDatWutjgMAKPm4ITUAoHSpXr265syZo4ULF2rq1KlWxwGAq27WrFmKj4/X22+/bXUUAEApwZlDAIBSadSoUXrjjTe0adMmNW/e3Oo4AHBVpKamqn79+urdu7fee+89q+MAAEoHLisDAJROOTk5CgoK0s8//6zt27erUqVKVkcCgCv24osv6l//+pe+//57+fr6Wh0HAFA6cFkZAKB08vLy0kcffaS0tDRFRERYHQcArtj+/fv19ttva/To0RRDAICrijOHAACl2vr169WxY0dNnjxZTz75pNVxAOCy3X///dq3b592794tb29vq+MAAEoPzhwCAJRu7dq106hRozRs2DBt3rzZ6jgAcFnWrVunxYsX66233qIYAgBcdZw5BAAo9XJyctSlSxd999132rFjh6pUqWJ1JAAotOzsbN11113y8/PTp59+anUcAEDpw5lDAIDS78L9h7KysvTwww+Lv4sAKEmmT5+uvXv3auLEiVZHAQCUUpRDAABbqFatmubOnasVK1bonXfesToOABTKmTNnNHr0aD311FO6/fbbrY4DACilKIcAALZx3333afTo0frrX/+qr7/+2uo4AFCg0aNHKzMzUy+99JLVUQAApRj3HAIA2EpOTo66deumvXv3aseOHapatarVkQDgon788Uc1btxYkydP1mOPPWZ1HABA6RVLOQQAsJ0TJ06oefPmatKkiZYtWyaHw2F1JADIo1u3bjpw4IASEhLkcrmsjgMAKL24ITUAwH6qVKmif//731q1apUmTZpkdRwAyCMuLk6fffaZJk2aRDEEALjmOHMIAGBb0dHReumll7R+/Xrdc889VscBAElSVlaWmjVrpvr162vhwoVWxwEAlH5cVgYAsC9jjHr37q0dO3Zox44d8vX1tToSAGjy5Ml6/vnntWfPHt12221WxwEAlH5cVgYAsC+Hw6EPPvhATqdTAwcOFH8vAWC1EydOaOzYsXr22WcphgAAHkM5BACwtRtuuEHz589XXFycJkyYYHUcADb38ssvy+Vy6cUXX7Q6CgDARiiHAAC25+/vr1dffVUjR47Uhg0brI4DwKb27t2radOm6fXXX1fFihWtjgMAsBHuOQQAgH67/9D999+v7du3a+fOndx/CIDHde7cWUlJSdq+fbu8vPgbLgDAY7jnEAAA0m/3H5o5c6a8vb3Vp08f5eTkWB0JgI0sXbpUq1ev1uTJkymGAAAex08eAAD+58L9hzZs2KDo6Gir4wCwifPnz+v5559X3759FRAQYHUcAIANuawOAABAcdKiRQtFR0dr+PDh8vf3V4cOHayOBKCUe/vtt/XLL79o9erVVkcBANgU9xwCAOAPjDEKCQnRV199pYSEBNWoUcPqSABKqWPHjql+/fp65plnNHr0aKvjAADsKZZyCACAizh16pTuuusu1a1bV2vWrJHT6bQ6EoBSaPDgwVq5cqW+++47lS9f3uo4AAB74obUAABcTOXKlTV//nxt2rRJr732mtVxAJRCCQkJ+vDDD/XGG29QDAEALMWZQwAA5OPdd9/VM888o5UrVyowMDDP/CNHjqhmzZoWJANQUnz++efy9vbOc7Ppdu3a6fz58/rqq6/kcDgsSgcAAGcOAQCQr6FDh6pPnz4aMGCAjhw54p6emZmp5557Tp07d7YwHYCSYNmyZbrvvvsUGhqqAwcOSJJiYmL05Zdf6u2336YYAgBYjjOHAAAoQEpKilq0aKGaNWsqLi5Ohw4d0gMPPKD4+HgZY/TDDz/oT3/6k9UxARRT99xzjzZt2iSXyyUvLy8999xzmjt3rtq3b68PPvjA6ngAAHDmEAAABbn++usVExOjLVu2aMCAAbrjjju0a9cuGWPk7e2txYsXWx0RQDFljNHu3bslSVlZWTp//rwmTJigEydOqGHDhsrJybE4IQAAnDkEAEChZGVlqXfv3lq+fLkcDof7FzqHw6G77rpL27ZtszghgOJo//79uvXWW/NMv3ApWdOmTfWPf/xD99xzj6ejAQBwAWcOAQBQkMTERAUEBGjFihUyxuT6S78xRvHx8UpMTLQwIYDiavfu3Re9p5AxRsYY7dmzRwEBAerfv7+SkpIsSAgAgEQ5BABAPlasWKEmTZooPj5e2dnZF13Gy8tLy5Yt83AyACVBQkKCvL29Lzk/KytL0m9nElWqVMlTsQAAyIVyCACASzh48KAeeughnTp1SpmZmZdczhij2NhYDyYDUFLEx8e7C6BLGTVqlD766COVKVPGQ6kAAMiNcggAgEu46aab9M0336hXr16SdMnHTefk5GjDhg1KTk72ZDwAJcD27dsvetNpLy8vuVwuzZ49W6NHj+Zx9gAAS1EOAQCQj2rVqmnx4sWKiYlRpUqV8r08ZPny5R5MBqC4O3nypI4ePZpnusvlUvny5bVmzRoNGDDAgmQAAORGOQQAQCGEhobqu+++U7du3SRd/CyiBQsWeDoWgGJs586d+uODgb29vVWjRg1t2bJF7dq1syYYAAB/QDkEAEAh5XcWUXZ2tlavXq2UlBQLEwIoTv54M2pvb2/3De4bNmxoYTIAAHKjHAIAoIgudRZRZmamVq1aZWU0AMXIrl273GcOOZ1OdevWTRs3blS1atUsTgYAQG6UQwAAXIYLZxHNnTtXFStWdJ8dsHDhQouTASgutm7d6n5S2bPPPquFCxeqXLlyFqcCACAvh/njhdAAAFyGxMREbdq0yeoYljh16pSmTZum+Ph4lS1bVjNnzpTL5bI6Fn6nTp06at26tdUxPMbO47G4yMrKct9s+tFHH1XHjh0tTlR82G08AkAJEEs5BAC4KmJiYhQWFmZ1DOCiQkJCFBsba3UMj2E8ojiz23gEgBIglj9rAgCuKrv/zeHXX3/V1q1b1aNHD6uj4H9CQ0OtjmAZu49HK8XFxalatWq64447rI5SrNh5PAJAcUY5BADAVVSjRg2KIQBcRgYAKFG4ITUAAAAAAICNUQ4BAAAAAADYGOUQAAAAAACAjVEOAQAAAAAA2BjlEAAAAAAAgI1RDgEAAAAAANgY5RAAAAAAAICNUQ4BAAAAAADYGOUQAAAAAACAjVEOAQAAAAAA2BjlEAAAAAAAgI1RDgEAAAAAANgY5RAAoNhJSUmxOgKA/2E8AgBQ+lEOAQCKjX/9619q27atGjZsaHUUj4qNjVVERIRefPFF9e3bV6NGjVJmZmaRtrFo0SK1b99eDodDDodDbdq00b333qtmzZqpVatWGjFihH766adr9B2gNLLjeOzUqZN7DP3xv08++aTQ22E8AgBKGpfVAQAAuGDw4MH6+OOPlZ2dbXWUy3LkyBHVrFmzSOvExMTojTfe0JYtW+R0OmWMUbdu3fTSSy8pOjq60Nu5//771bJlS/n5+alu3bratGmTe962bds0atQo3X777RoxYoTGjh0rL6+S9/ehyzm+uHx2G48///yzfvnlF40bN041atRwTz948KAmTJigDh06FHpbjEcAQElT8n4SAQBKLafTKT8/P6tjXJaTJ08qPDy8yOtNmzZNrVu3ltPplCQ5HA516dJFS5cuLfK2ypcvL0kqV65cruktWrTQ8uXLFRYWptdee61IpVNxcbnHF5fPbuNxzZo1WrlypUaOHKlHH33U/V+lSpXUqVMn+fj4FGl7jEcAQElCOQQAwBVKT09Xnz59tH///iKvm5KSori4uFyXke3evVu1a9cu8rYcDscl53l5eemf//ynqlWrpnHjxungwYNF3r5VruT4wn4u9/0SGRmpunXr5pm+aNEi9erVq8g5GI8AgJKEcggAYKmlS5cqMjJSI0aM0NChQ3XkyBH3vEOHDun1119X48aNdeLECXXu3Fl169bV8ePHJUkLFy7UU089peHDh6tLly566aWXlJGRIUnau3evRo4cqUaNGunw4cPq1auXqlSpIn9/f23evDlXhvy2M2/ePFWsWFF16tSRJJ0+fVpjx46V0+lU69atJUmLFy/Wvn37lJycrIiICL355puF/v4jIiL07bffqmvXrjp9+rQ2b96sLVu2aNKkSe5lvvrqK9WpU0crVqy4jCP8/ypVqqQHH3xQ6enpiomJscXxRdHYfTz+0a+//qqtW7cqODjYPY3xyHgEgFLJAABwFcyfP98U9cfKnDlzTMuWLc3Zs2eNMcYkJSUZX19fU6NGDWOMMStWrDANGjQwTqfTvPLKK2batGnG39/fHDp0yEyaNMm0adPGnD9/3hhjTHJysrnttttM27ZtTU5OjnnhhRdM5cqVjdPpNMOGDTPr1q0zCxcuNL6+vsbHx8ccPnzYGGMK3I4xxnTq1Mn4+fnlyt6kSRPTqlUr99fBwcGmXr16l3HkjBkzZoyRZOrXr2+Cg4PNyZMnc81fvny5KVeunJkzZ06+2zl16pSRZBo0aHDJZT7++GMjyQwaNMg2xzckJMSEhIRc1rolFePx8t8vvzd16lRz33335ZrGeGQ8AkApFEM5BAC4Kor6y2haWpqpWbOmmTt3bq7pvXv3dv8yaowxjz76qJFkfvjhB/e0o0ePmvLly5vZs2fnWveDDz4wksxHH31kjDGmX79+xtvb2/2LkDHGxMbGGklm1KhRhd5Or1698vyy1KpVq6v6y2ibNm2Mw+EwFSpUMGvXrs0zPysrq8BtFOaX0VWrVhlJpkOHDsYYexxfO/4yyni8OuVQp06dzFtvvZVnOuOR8QgApUwMl5UBACyxYcMGHTlyRE2aNMk1vWzZsrm+9vb2lsvl0p/+9Cf3tM2bNystLU033XRTrmUvXPqxbt06SZKPj4+cTqe8vb3dy/Tq1Utly5bVf/7zn0Jv51rKysrSoEGD9PDDD2vVqlUqW7asunbtmueG1BduWH2lTp8+LUmqX7++pNJ/fFE4jMe8Tp06pXXr1qlnz5555jEeAQClDeUQAMAS3377rSSpTJkyRV73wIEDkqQTJ07kmu7r6ysfHx8dPnz4kuu6XC7VqlVLWVlZV7Sdq+WZZ57RwYMHFRERocDAQH355ZeqWrWqBg4cqDNnzlz1/e3bt0+S1LRp00suU5qOLwqH8ZjXp59+qgYNGuiWW265ZvtgPAIAigvKIQCAJS78EnrhF5aiuPnmmyXpkk/LadCgQb7rp6enq0GDBle8navh3//+t/tGr5LUqFEjjR8/XqdPn9bOnTuv6r6MMVqwYIG8vb0VFBR0yeVK0/FF4TAe87rcp5QVFuMRAFCcUA4BACxxxx13SJLmz5+fa3pOTo6ys7PzXbd169aqWLGilixZkmt6YmKi0tPT1aNHj0uue+TIESUlJSkkJKTQ23G5XEpNTc2VKzU1VTk5Oe6vvby8lJqamm/ui/H19VVKSkquaXfffbckqVq1au5pv9/XpRhj8p0/ceJE/ec//9GIESMu+sjuC0rT8UXhMB5zS09P16pVqy5ZDjEeGY8AUNpQDgEALHHPPfeoffv2+vDDDzVlyhSlp6dr27Zt2rhxo5KSkjRv3jylp6e7f0k5deqUe92qVasqOjpaX331ldauXeue/s4772jgwIFq3769e1pGRoZ27drl/nrcuHEaOHCg/P39C72dJk2a6NSpUxo/fry+//57jRs3ThkZGfruu+/cZ/fUqlVLycnJio+P1/r165Wenl6o4xAZGal58+YpKSnJPW316tUKCAjQ7bffLkmKi4vTDTfcoAULFuS7rQu/rP1x3wcOHNDTTz+t559/XlFRURozZkyudUrz8UXhMB5zW7VqlapWrarmzZvnmcd4ZDwCQGnksjoAAMC+lixZomeeeUZjxozR66+/rkceeUTBwcHKzMxUtWrV9NFHH2nVqlUyxujpp5/WsGHD1KxZM0nS448/rpo1a+qNN97QkiVLdMMNN6hGjRqKjo7OtQ9vb2/NmjVLiYmJqlixourVq6eRI0e65xdmO88884y2b9+u6OhoLV++XO+++65++uknZWVlKTExUc2aNdMTTzyhTz/9VP369dOrr74qHx+fQh2DZ599VhUqVNCAAQPUuHFjOZ1OnTt3TkuWLJGX129/w3E6nSpfvnyuG83+0dKlSzV58mRJ0sGDBxUQEKCyVcskpAAAIABJREFUZcuqbNmyMsaoYcOGSkhIcJ8hIknTp08v9ccXhcd4/H+LFi266I2oJcYj4xEASieHKeicVwAACiEmJkZhYWEFXkrhSREREfr444919uxZq6OUSiXl+IaGhkqSYmNjLU7iOYxH+ykpx9eO4xEASoBYzhwCAOAauPHGGwtcZubMmerevbsH0gD2xngEACB/lEMAgFIrNTVVmZmZMsbI4XB4dN+/v4dQaWXl8UXJw3i8thiPAIArwQ2pAQCl0pQpU7RmzRplZ2crMjJSGzdutDpSqcLxRVHwfrm2OL4AgCvFPYcAAFdFcbzHCSDZ8x4njEcUV3YcjwBQAsRy5hAAAAAAAICNUQ4BAAAAAADYGOUQAAAAAACAjVEOAQAAAAAA2BjlEAAAAAAAgI1RDgEAAAAAANgY5RAAAAAAAICNUQ4BAAAAAADYGOUQAAAAAACAjVEOAQAAAAAA2BjlEAAAAAAAgI1RDgEAAAAAANgY5RAAAAAAAICNuawOAAAoXWJiYqyOgEIwxsjhcFgdwyMSExPl5+dndQxLlIbxmJWVJZeLf7KWFnYejwBQnPGTFgBwVYWFhVkdAcgjJCTE6giWYDyiOLLreASA4sxhjDFWhwAAAJ5z9uxZdevWTXv27NH69evVqFEjqyMBufz3v/9Vv379lJCQoPHjxysqKsrqSAAAlGax3HMIAACbKVeunD799FM1atRIf/nLX/Ttt99aHQlwi42NVbNmzZSRkaGEhASKIQAAPIByCAAAG/Lx8dGyZctUr149BQYGav/+/VZHgs2lpqbqscceU1hYmB566CFt2rRJ9evXtzoWAAC2wGVlAADY2OnTp9WxY0cdO3ZMX3zxherVq2d1JNjQjh071LdvX504cUIffPCBgoODrY4EAICdcFkZAAB2VqlSJa1Zs0a+vr4KDAzU4cOHrY4EGzHGaPLkyWrTpo1q166thIQEiiEAACxAOQQAgM1VrlxZK1euVNmyZdW+fXsdOXLE6kiwgeTkZPXo0UPDhw/XCy+8oDVr1qh27dpWxwIAwJa4rAwAAEiSjh07pnbt2skYo/Xr16t69epWR0IptW7dOg0YMEBOp1Nz5szRvffea3UkAADsjMvKAADAb6pVq6bVq1crMzNTnTt31vHjx62OhFImKytLo0ePVmBgoPz9/bVz506KIQAAigHKIQAA4Obn56d169bpzJkzCgwM1MmTJ62OhFLi4MGDat++vaKjozVx4kQtWrRIVapUsToWAAAQ5RAAAPiDOnXqaM2aNTp27Ji6du2qlJQUqyOhhFu8eLHuvPNOHT9+XFu2bFFUVJTVkQAAwO9QDgEAgDxuvfVWrVu3TgcOHFCXLl2UmppqdSSUQOfOnVNUVJTuv/9+BQcHa/v27brjjjusjgUAAP6AcggAAFzUbbfdpnXr1umnn35Sr169dPbsWasjoQTZu3evWrZsqVmzZmnevHmaPXu2fHx8rI4FAAAugnIIAABc0u23365Vq1YpISFBvXr10rlz56yOhBJg9uzZatGiha677jrt2LFDffr0sToSAADIB+UQAADI1x133KG4uDht375dvXv3VkZGhtWRUEydPn1affr00cMPP6zBgwdr48aNuuWWW6yOBQAACkA5BAAACnTnnXdq+fLl+uqrr9SvXz9lZWVZHQnFzJYtW9SsWTOtX79eK1eu1OTJk+Xt7W11LAAAUAiUQwAAoFBatWqllStXavXq1erbty8FESRJxhhNnjxZAQEBuu2227Rr1y516tTJ6lgAAKAIKIcAAEChtWnTRitWrNCKFSs0ePBg5eTkWB0JFjp69KiCgoI0YsQITZgwQStXrlT16tWtjgUAAIrIZXUAAABQstx7771avHixevToIafTqenTp8vLi7832c2yZcv0yCOPyNfXV19//bWaNWtmdSQAAHCZ+JccAAAossDAQC1evFhz5sxRVFSUjDFWR4KHZGRkKCoqSr169VLXrl21fft2iiEAAEo4zhwCAACXJSgoSPPmzVNYWJhcLpcmTZpkdSRcY99995369u2rH3/8UbNnz1Z4eLjVkQAAwFXAmUMAAOCy9e7dW3PnztU//vEPDR8+3Oo4uIZmz56tu+++W06nUzt27KAYAgCgFOHMIQAAcEVCQkKUnp6uQYMGqWLFiho1apTVkXAVpaSk6IknntDcuXM1dOhQTZgwQWXKlLE6FgAAuIoohwAAwBV76KGHlJ2drcGDB8vlculvf/ub1ZFwFWzfvl19+/bV6dOn9emnn6pr165WRwIAANcA5RAAALgqBg0apOzsbEVGRsrlcun555+3OhIukzFG77zzjp5//nkFBARo9uzZqlWrltWxAADANUI5BAAArprBgwcrLS1Nw4YNU/ny5fXkk09aHQlFdOzYMT388MNas2aNRo4cqVGjRsnLi9tUAgBQmlEOAQCAqyoqKkrZ2dkaOnSonE6nHn/8casjoZDWrl2rAQMGqEyZMvriiy/Upk0bqyMBAAAPoBwCAABX3bPPPquUlBQNGTJELpdLgwcPtjoS8pGVlaVx48Zp7Nix6t27t6ZPn64bbrjB6lgAAMBDKIcAAMA18corrygzM1OPP/64fHx81K9fP6sj4SIOHDigfv36aefOnXrrrbcUFRVldSQAAOBhlEMAAOCaGTdunLKysvTQQw/J6XQqLCzM6kj4nQULFigiIkK1a9fW1q1b1bhxY6sjAQAAC1AOAQCAa2r8+PFKS0vTgAEDVK5cOfXo0cPqSLZ39uxZvfDCC3rnnXc0YMAATZ06VT4+PlbHAgAAFqEcAgAA15TD4dA777yj7OxshYaGatGiRerWrZvVsWzrm2++UZ8+ffTrr79q2bJl6t69u9WRAACAxXguKQAAuOYcDofee+89DRw4UKGhofr888+tjmQ7xhhNmzZNLVq0UNWqVZWQkEAxBAAAJFEOAQAAD3E4HJoyZYruv/9+de/eXevXr7c6km0kJyerZ8+eGjJkiJ5//nmtXbtWtWvXtjoWAAAoJrisDAAAeIzT6dSsWbOUnZ2t4OBgrVixQgEBAVbHKtXWr1+v8PBweXl5ad26dRxvAACQB2cOAQAAj3I6nZo9e7Y6dOig7t27a9u2bRdd7uzZs0pKSvJwutIjOztbo0ePVseOHdWiRQslJCRQDAEAgIuiHAIAAB7n7e2t2NhYBQQEqFOnToqPj881Py0tTUFBQYqOjrYoYfGWlpaW7/xffvlF7du3V3R0tCZOnKhFixapSpUqHkoHAABKGsohAABgiTJlymjBggVq3bq1unTpoj179kiSzpw5o44dO2rDhg365z//qZMnT1qctHjZu3evWrZsqTNnzlx0/pIlS3TnnXcqKSlJmzdvVlRUlBwOh4dTAgCAkoRyCAAAWKZs2bJasGCB/vznP6tDhw7asmWLOnTooPj4eBljlJmZqSlTplgds9jIzMxU37599c033ygyMjLXvHPnzikqKkq9e/dWt27dtH37djVt2tSipAAAoCRxGGOM1SEAAIC9paSkqEOHDtq7d6/Onz+vzMxM97wbbrhBhw4dUrly5SxMWDyMGjVKr732mrKzsyVJM2fO1KBBg7Rv3z717dtXP//8s6ZOnaq+fftanBQAAJQgsZw5BAAALHf27FmlpKQoIyMjVzEk/XaZ2axZsyxKVnzEx8fnKoYkaciQIfr73/+uu+66Sz4+Ptq1axfFEAAAKDLOHAIAAJY6evSo2rZtq/379+cphiTJ4XDIz89P+/fvl8vlsiCh9dLT09W4cWMdPHgwVznk7e2tChUqKCIiQq+++qptjw8AALginDkEAACsk5iYqNatW1+yGJIkY4wSExO1aNEiD6crPoYPH65ffvklVzEk/XYPopSUFJ07d45iCAAAXDbKIQAAYJkZM2bo4MGDBS7n5eWlV1991QOJip+4uDhNnTpVWVlZF52flZWld999V0uXLvVwMgAAUFpwWRkAALDUf//7X7322muaOXOmvLy8LnkGkSStWbNGHTt29GA6a508eVINGzZUcnJynrOGfs/hcKhSpUras2ePateu7cGEAACgFOCyMgAAYK169epp2rRp+vHHH/Xwww/L6XTK29s7z3Iul8t2Zw8NGTJEJ06cyLcYkn47NqdOndKTTz7poWQAAKA04cwhAABQrBw4cEATJ07U1KlTJSnPmUTbt2/XXXfdZUU0j1q0aJEeeOCBS853uVzKyspSzZo11b17dwUHB6tz584qU6aMB1MCAIBSIJZyCAAAFEsXK4lcLpd69uypBQsWWJzu2jp8+LAaNWqkM2fO6MI/1ZxOpyQpJydHTZo0Ue/evdW9e3c1b95cDofDyrgAAKBkoxwCAADF288//6xx48Zp1qxZys7OlpeXl77//nvdeuutVke7JowxCgoK0urVq+V0OpWdna2yZcsqMDBQvXr1UnBwsKpXr251TAAAUHpQDgEAYLXQ0NBSfyYMcLnmz5+vBx980OoYAACUZrEuqxMAAACpVatWGjZsmNUxSoSkpCQtX75coaGhKl++vNVxrqqMjAytWrVKjRs31s0332z7y8XCwsKsjgAAgC1w5hAAABYLDQ2VJMXGxlqcBCheHA4HZw4BAHDt8Sh7AAAAAAAAO6McAgAAAAAAsDHKIQAAAAAAABujHAIAAAAAALAxyiEAAAAAAAAboxwCAAAAAACwMcohAAAAAAAAG6McAgAAAAAAsDHKIQAAAAAAABujHAIAAAAAALAxyiEAAAAAAAAboxwCAAAAAACwMcohAAAAAAAAG6McAgAAuIbOnz+vY8eOWR0DAADgkiiHAABAsbJkyRLVqVNH+/btK9TyWVlZ2rBhg0aOHKlVq1YVeX+xsbFq3ry5KlSooKZNm2rp0qVF3sYFGzduVEBAgJo3b65GjRrpjjvuUGBgoBYuXHjZ2yxuPP36AACAa49yCAAAWOrIkSO5vi5fvryqVaum6667rlDrb9u2TR988IFee+01JSYmFmnfH374ob788kt98MEH+uSTT+R0OhUaGqoffvihSNuRpD179igwMFBRUVHasWOH9u7dqxdffFHx8fH69ddfi7y94sLK1wcAAHgG5RAAALDMyZMnFR4enmtaYGCg4uPjdfPNNxdqG61bt9bQoUOLvO/MzEz9+OOPevfdd9W0aVO1b99e77//vjIzM7Vly5Yib+/DDz+UMUYhISHuaX379tWUKVPyFCwlhZWvDwAA8BzKIQAAYIn09HT16dNH+/fvv+JtlSlTpsjreHl5afTo0bmmVa1aVZLUokWLIm/v6NGjysjI0BdffJFrev/+/eXlVfL+yWX16wMAADyn5P1LBQAAKCEhQYMGDVJ0dLR69uypwMBA9zxjjKZOnaonnnhCLVu2VKdOnfJcJrVy5UpFRERoxIgReuyxxzRhwgQFBwdLkubNm6eKFSuqTp06kqTTp09r7Nixcjqdat26daH2k5CQoL/+9a+65ZZblJaWpsGDB8vX11f+/v7usmHx4sXat2+fkpOTFRERoTfffFMnT57UjBkzFBgYqCVLlrj3dfToUUVERGjs2LGKiIhQ7969dfz48Ss6hk6nUy6XK9e0uXPn6t1339Xtt9/unvbVV1+pTp06WrFiRb7ba9u2rSSpR48emjNnjnu6l5eXpkyZ4v6a1wcAABQ7BgAAWCokJMSEhIQUaZ369eubjRs3GmOMSU9PN/fee6973vjx482HH35ojDEmKyvLNGrUyNSoUcOkpaUZY4yZNWuW8ff3N6mpqcYYY3JyckzDhg1N5cqV3dvo1KmT8fPzy7XPJk2amFatWhVqP0eOHDEdO3Y0ksyTTz5pvvnmG7Nz505TtmxZ06dPH/c2goODTb169dxf79271wwbNsxIMgsWLHBPb9eunQkLC3N/3bRpUxMeHu7+es+ePUaSef/994t0HC9ISUkxY8aMMdWrVzerVq3KNW/58uWmXLlyZs6cOfluIysry/Tq1ctIMpJMWFiYOXbsWJ7leH0KT5KZP39+kdYBAABFFsOZQwAAlDCZmZn64YcfFB8fL0kqV66cnnvuOUnS4cOH9fbbb2vAgAGSfjs7JiQkRL/++qs++eQTnT59Ws8995xGjBih8uXLS5IcDoeaNm2aax8+Pj559nth+cLsp0aNGu5Ls8aMGaNGjRrpzjvvVIsWLdy5L6Zhw4bq2bNnnul/zNi4cWPt3r274INVCGlpaRozZoy2bNmikydPqnPnzpo5c6Z7fteuXZWSkqJ+/frlux2n06kFCxZowoQJKl++vObPn68GDRrkOsOG1wcAABRHroIXAQAAxYm3t7c6d+6sZ555Rnv27NHrr7+uXr16SZI2bdqkzMxMPfbYY7nWGTx4sMqVK6fVq1crOTlZzZs3zzX/j5dXFaSg/Ui/FRJ/3Lafn59+/PHHfLd9sSyff/65JOncuXOaM2eOtm7dKmNMkTJfSvny5TVhwgRJvz1xrG3btnr11Vf1yCOPuJe58L0UxOl0avjw4QoNDdXjjz+ulStX6oEHHtD8+fMVEhLC6wMAAIolyiEAAEqghQsXKiIiQtOnT9fixYsVExOj9u3ba9++fSpfvrymT59+0fXGjBkjSe6C4HIVtJ+rLTs7W2+88Ya2b9+up59+Wi1bttTmzZuv+n4aN26sqKgojR49WpmZmfL29r6s7dStW1crVqzQ008/rXfffVdDhw7VAw88wOsDAACKJS4rAwCgBHK5XJozZ47mzJkjl8uloKAg7du3Tz4+PkpMTFRiYmKedZKSktxni/zxBshFVdB+rqacnBx17dpVe/fu1cKFC903fr5WGjduLD8/vyIVQ99//73eeuutPNMnT54sPz8//frrrzp8+DCvDwAAKJYohwAAKGEyMjI0bdo0SVK/fv20efNmGWO0bt06NWnSRMYYjRgxItc6P/30k/75z3+qYcOGkn574tXvnTlzJtfXLpdLqampys7Odk9LTU1VTk6OJBW4n8Ly8vJSampqvsts3bpVq1evVrt27dzTMjMzr9llS99++6169OiRa9qF7/tSbr75Zk2cODFP8eJwOFSrVi1VrFhRNWvW5PUBAADFEpeVAQBQAs2cOVNPPPGEnE6natWqpUqVKql58+Zq2bKlWrRooblz5+rcuXPq3bu3zpw5o0WLFunf//63KlWqpHr16mnatGlq1KiR2rVrp6+//lq7du3Ktf0mTZpowYIFGj9+vB588EHFxMQoIyNDv/zyi3bu3KnAwMB89yP99oh1ScrKynJv99ixY0pPT3d/XatWLSUnJys+Pl4pKSny9/fXkSNHJP3/GS4Oh0OSNGvWLPn7+2vbtm365ptvdPToUe3evVvVq1d3lydpaWmFPoanTp3SsGHD1L17d/Xu3VsOh0M//vijvvjiCy1atMi9XFxcnB544AHNmDFDISEhF92Wt7e3rrvuOvXs2VOxsbGqXbu2JGnDhg3asWOH3njjDXl5eRV43Hh9AACAJSx7UBoAADDGFP1R9ufOnTMtWrQwnTt3Nq+//rqJjIw006dPd88/fvy46d+/v6lWrZq58cYbzUMPPWQOHTrknv/999+bgIAAU6lSJRMQEGBWrlxpwsPDcz0q/fTp06Z79+6mQoUKplWrVmbbtm3m4YcfNuHh4WbZsmUF7icuLs7Uq1fPSDJDhgwxx44dM7NnzzYVKlQwkszo0aNNVlaW2bVrl/Hz8zP169c3sbGxZu3atea+++4zkszdd99tVq9ebYwx5vHHHzfXX3+9adWqlYmLizOfffaZ8fX1NSEhIebzzz83Xbp0MZJM8+bNzfLlywt1HFNSUkxwcLCpWrWque+++8zYsWPNxx9/bDIzM3Mt9/nnn5uaNWuaJUuW5Lu9Hj16mE6dOpnGjRubHj16mKCgIOPv728+/vjjXMvx+hTu9TGGR9kDAOAhMQ5jOOcXAAArhYaGSpJiY2MtyzBgwAB9+umnOnnypGUZcGl2fX0cDofmz5+vBx980OooAACUZrFcVgYAAEqdG2+8scBlZs6cqe7du3sgDQAAQPFGOQQAAJSenq7z58/LGOO+h0xJdrWfyGW10vb6AACA4oWnlQEAYGOHDx/Wiy++qJUrVyo9PV0vvfSSMjIyrI6F/+H1AQAAnsA9hwAAsFhxuOcQUBxxzyEAADwiljOHAAAAAAAAbIxyCAAAAAAAwMYohwAAAAAAAGyMcggAAAAAAMDGKIcAAAAAAABsjHIIAAAAAADAxiiHAAAAAAAAbIxyCAAAAAAAwMYohwAAAAAAAGyMcggAAAAAAMDGKIcAAAAAAABsjHIIAAAAAADAxiiHAAAAAAAAbMxldQAAACAtWLBADofD6hgAAACwIYcxxlgdAgAAO/v666/1yy+/WB0D+u21ePvttzV//nyro+B/2rRpIz8/P6tjAABQmsVSDgEAAPxPTEyMwsLCxD+PAACAjcRyzyEAAAAAAAAboxwCAAAAAACwMcohAAAAAAAAG6McAgAAAAAAsDHKIQAAAAAAABujHAIAAAAAALAxyiEAAAAAAAAboxwCAAAAAACwMcohAAAAAAAAG6McAgAAAAAAsDHKIQAAAAAAABujHAIAAAAAALAxyiEAAAAAAAAboxwCAAAAAACwMcohAAAAAAAAG6McAgAAAAAAsDHKIQAAAAAAABujHAIAAAAAALAxyiEAAAAAAAAboxwCAAAAAACwMcohAAAAAAAAG6McAgAAAAAAsDHKIQAAAAAAABujHAIAAAAAALAxyiEAAAAAAAAboxwCAAAAAACwMcohAAAAAAAAG6McAgAAAAAAsDHKIQAAAAAAABujHAIAAAAAALAxyiEAAAAAAAAboxwCAAAAAACwMZfVAQAAAKxw9uxZHTlyJNe0o0ePSpL279+fa7rT6VTdunU9lg0AAMCTHMYYY3UIAAAATzt+/Lhq1KihrKysApcNCgrSihUrPJAKAADA42K5rAwAANhS1apVFRgYKC+v/P855HA41KdPHw+lAgAA8DzKIQAAYFvh4eEq6CRql8ulXr16eSgRAACA51EOAQAA2+rZs6fKli17yfkul0s9evRQpUqVPJgKAADAsyiHAACAbZUvX149e/aUt7f3RednZ2erf//+Hk4FAADgWZRDAADA1vr376/MzMyLzitXrpy6dOni4UQAAACeRTkEAABsLSgoSBUrVswz3dvbW2FhYbruuussSAUAAOA5lEMAAMDWvL299eCDD+a5tCwzM1P9+vWzKBUAAIDnUA4BAADb69evX55Ly6pWrar27dtblAgAAMBzKIcAAIDttW3bVtWqVXN/XaZMGYWHh8vpdFqYCgAAwDMohwAAgO15eXkpPDxcZcqUkSSdP39effv2tTgVAACAZ1AOAQAASOrbt6/Onz8vSfLz85O/v7/FiQAAADyDcggAAEDS3XffrZtvvlmS9PDDD8vhcFicCAAAwDNcVgcAAMAOvv76a7311ltWx0ABypUrJ0naunWrQkNDLU6DgsTGxlodAQCAUoEzhwAA8IBffvlFCxYssDoGClCnTh1VqlRJFStWtDoK8pGYmMh4AgDgKuLMIQAAPIgzHYq/VatWqXPnzlbHQD5iYmIUFhZmdQwAAEoNzhwCAAD4HYohAABgN5RDAAAAAAAANkY5BAAAAAAAYGOUQwAAAAAAADZGOQQAAAAAAGBjlEMAAAAAAAA2RjkEAAAAAABgY5RDAAAAAAAANkY5BAAAAAAAYGOUQwAAAAAAADZGOQQAAAAAAGBjlEMAAAAAAAA2RjkEAAAAAABgY5RDAAAAKJSUlBSPrgcAADyDcggAANheVlaWNmzYoJEjR2rVqlVWx3GLjY1Vq1at5HA45HA4FBUVpYSEBI/neO+99xQQEKBWrVoVab1//etfatu2rRo2bHiNkgEAgKuBcggAABRLR44c8di+tm3bpg8++ECvvfaaEhMTPbbfgoSGhurtt9+WJN15552aPHmy7rzzTo/neOyxx3T69Gnl5OQUab3BgwcrJydH2dnZ1ygZAAC4GiiHAABAsXPy5EmFh4d7bH+tW7fW0KFDPba/oqhcubIkqVKlSpZlcLlcql27dpHXczqd8vPzuwaJAADA1UQ5BAAAipX09HT16dNH+/fv9+h+y5Qp49H9FZbD4cj1fwAAgKuNcggAgGLss88+05AhQxQVFaXWrVtr+vTpueYvXLhQTz31lIYPH64uXbropZdeUkZGhiTpm2++0d/+9jfdfvvtOnTokMaOHau6devqz3/+s9atW6dz585p2LBhuvXWW3XTTTflutfO3r17NXLkSDVq1EiHDx9Wr169VKVKFfn7+2vz5s3u5Y4ePaqIiAiNHTtWERER6t27t44fPy5JOnTokF5//XU1btxYJ06cUOfOnVW3bl0dP3483/UWL16sffv2KTk5WREREXrzzTcL9f3mt78rkd8+JSkhIUGDBg1SdHS0evbsqcDAwELN++qrr1SnTh2tWLHiivL9Xn7H9UreD7+3fv16BQUFqUqVKurcuXOeEm/p0qWKjIzUiBEjNHTo0DyXB+aXEQAAWMQAAIBrbv78+aaoP3Znz55t+vTpY7Kzs40xxrz66qtGklm7dq0xxphJkyaZNm3amPPnzxtjjElOTja33Xabadu2rcnJyTHHjh0zAwYMMJJMZGSkiY+PN2fOnDEtW7Y0t9xyi3nyySfN3r17TUpKimnTpo255ZZb3Pt+4YUXTOXKlY3T6TTDhg0z69atMwsXLjS+vr7Gx8fHHD582BhjTLt27UxYWJh7vaZNm5rw8HBjjDErVqwwDRo0ME6n07zyyitm2rRpxt/f3xw6dCjf9YwxJjg42NSrVy/X8Sjo+81vf4WxZ88eI8m8//77hd6nMcbUr1/fbNy40RhjTHp6urn33nvd6+c3b/ny5aZcuXJmzpw5+eb69ttvjSTTrl27Ar+H/I7rlbwfjDEmKCgNJhgxAAAI0UlEQVTIVK1a1TzyyCNmxYoVZuLEiaZMmTKmVq1aJi0tzRhjzJw5c0zLli3N2bNnjTHGJCUlGV9fX1OjRo1CZSysyxlPAADgkmI4cwgAgGIoKSlJQ4cO1WuvvSYvr99+XEdGRur+++9XzZo1dezYMb300kt6/PHH5e3tLUmqWrWq/va3v+mLL77QnDlzdOONN7qfLvXUU0+pefPmuv766xUUFKT9+/dr8ODBatiwoSpUqKAOHTpo//79SkpKkiSNHz9eXbt2lZeXl6Kjo9WuXTvdf//9mjJlitLT0zV16lRJv13q1LRpU3fuxo0ba/fu3ZKkoKAg3XPPPcrOzlZ4eLgiIiK0ZcsW1apVK9/1LqYw329++7schdlnZmamfvjhB8XHx0uSypUrp+eee06S8p0nSV27dlVKSor69et3WfkuJr/jeiXvhwvKli2rGTNmKCgoSM8++6zGjBmjw4cP6/3331d6erqGDx+uqKgoXXfddZIkX19fBQQEFDojAACwhsvqAAAAIK+NGzcqJydHN998s3uar6+vFi5cKElatmyZ0tLSdNNNN+VaLzg4WJK0bt06hYeHy+l0SpK7YJLkvkHwhcJDkns7ycnJuvHGGyVJPj4+cjqduZbr1auXypYtq//85z+SpM8//1ySdO7cOc2ZM0dbt26VMca9vLe3t1yu/2vvXkKi7OI4jv+mhsIIQ3LjoBBo6CIvRBesRVpBF8hKzKhA23Rx5yoogqSi1FpIEIaWKCVRr6JlLcpMIaPJsCgvBbkoMWdGJTN1TGycdxEOr2/qTGM61nw/u+c5c+b8z3nOIP45zzlGRUREjIvTXb3/M5vNHvV3sva84WmbW7ZsUWZmppqbm5Wdna1du3ZJ+tH3ycrGjD2f38XduE5nPkhSYGDguPbS0tJ0/PhxNTY2KjIyUhaLRdHR0eM+s3Dhwl+KEQAAzD5WDgEAMAc1NzdrZGRk0n+aP378KEn6/PnzuPvBwcFatGiROjs7J/3uiTY2Hrvn7qhyo9Eok8mk79+/S5IcDofOnz+vAwcOKCIiQmvXrp2y/phfrTed/nrL0zbLy8u1b98+FRYWKjIyUrW1ta7PTlU2E7x5HtOZDyaTSQEBARoaGtK7d+8kud/Y29s5AwAAZg7JIQAA5qDAwEB9+/ZNra2tP5UNDw+7VhRNdqJXVFTUjMVmt9sVFRWl0dFRbd++Xa2trSovL9eGDRs8qu9NPV/019M2jUajSktLVVpaKqPRqK1bt+rt27duy36n9+/fy263e/U8pstgMGjFihWupNBYUm0i3s4ZAAAws0gOAQAwB61evVqSdPLkyXGrNxobG3X//n3Fx8crMDBQlZWV4+p1dHTIbrcrKSlpRuKyWCzq7u5WSkqKGhoa9PDhQyUkJLjKp1rtNMaTevPmzdPAwIDr2hf99aTN4eFhFRQUSJL2798vs9ksp9Op2traKcvGuFuZI8nteDqdTh09elSvXr3y6nlMx4cPHzQyMqLU1FTFxMRIkm7dujXuM6Ojo3I4HJI8e/YAAGD2kRwCAGAOWrdunbZt26bKykpt2rRJly9f1rFjx5Sfn6/k5GQtXbpUOTk5evr0qWpqalz1Ll26pPT0dCUmJkqSvn79Kkmu18D+e6+np8d1r7+/X5LGHdE+dv369WvX9dmzZ5Wenq41a9a4Xj0qKSlRU1OTioqK1NLSIpvNpjdv3shms2lgYEAOh0NfvnxxfYcn9Uwmk3p6etTY2Ki6ujoFBAR41N+J2vPU2LgMDg5KksdjXFRU5Ep+mEwmLVmyRCtXrnRb9ujRIwUFBamsrGzKuPr6+iRpwj719fXp4MGDCgoKktH4YyvJqcZ1OvNh/vz56u3tdY2P0+nUmTNndOrUKUVFRWn9+vVKTExUcXGxa+PyFy9eqL6+Xt3d3bp586aGhobcxggAAGbf/KysrCxfBwEAwN+upaVFZWVl+pU/u7t371Zvb6+ePXumuro6hYeHKzc313US1KpVqxQXF6e8vDw1NDTIbDYrODhY2dnZMhgMevz4sS5cuCCbzab+/n7FxsaqqalJubm5slqtslqtio2NVVtbm3JycmSxWGS32xUXF6egoCBVVVWpublZCxYs0LVr1/TgwQOFhYXp4sWLMhgMCg0Nlc1mU3V1tZ4/f67k5GRt3LhRVVVVam9vV19fn4qLizU4OKjOzk4tW7ZMISEhbuulpqYqPDxc9+7d0927dxUfH6/Y2Fi3/S0sLFRBQcFP7XmioaFBp0+fVltbm7q6uhQWFqbly5e7bdPhcKikpEQVFRX69OmTSktLlZaWpqSkpCnLpB+vX1VVVSkpKWnS1+Lu3LmjrKwstbe3y2q1qrKyUhUVFbp+/brOnTunEydO6OXLl8rMzNTOnTunHNfg4GDl5eV5PR+io6PV0dGhkpISPXnyRDU1Ndq8ebOOHDkybs5aLBZdvXpVV65c0eLFixUSEqKYmBjFx8crISFBXV1dUz57d3sWSd79ngAAwKRaDU7W8QIAMONu376tvXv3/lGvzxw6dEg3btxwrfYA5oo/8fcEAMAc9g9H2QMAgL/af49in0xRUZF27NgxC9EAAADMPSSHAADAhAYGBlybBU903Pmforu729chAAAAzGlsSA0AAH6Sn5+v6upqORwOHT58WPX19b4OCQAAADOElUMAAOAnGRkZysjI8HUYAAAAmAWsHAIAAAAAAPBjJIcAAAAAAAD8GMkhAAAAAAAAP0ZyCAAAAAAAwI+RHAIAAAAAAPBjJIcAAAAAAAD8GMkhAAAAAAAAP0ZyCAAAAAAAwI+RHAIAAAAAAPBjJIcAAAAAAAD8GMkhAAAAAAAAP0ZyCAAAAAAAwI+RHAIAAAAAAPBjRl8HAACAP9mzZ4+vQwD+eB0dHb4OAQCAvworhwAAmAVhYWFKSUnxdRjAXyE0NJTfEwAAv5HB6XQ6fR0EAAAAAAAAfOIfVg4BAAAAAAD4MZJDAAAAAAAAfozkEAAAAAAAgB8jOQQAAAAAAODH/gWDTguCFkl17gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(deep_triplet_model, to_file='deep_triplet_model.png')\n",
    "Image(filename='deep_triplet_model.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for this model are :\n",
    "- Total reward at run 0 : 118306 with conversion rate 0.256\n",
    "- Total reward at run 1 : 153219 with conversion rate 0.285\n",
    "- Total reward at run 2 : 121467 with conversion rate 0.237\n",
    "- Mean of total reward over 3 independant runs : 130997"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    So, even if the model is deeper, it does not improve the performance of the recommendation system at all, this may be due to the limited data available during learning.\n",
    "    \n",
    "    - Cumulative reward for naive model : 215357 with conversion rate 0.25\n",
    "    - Cumulative reward for Dot model : 119729 with conversion rate 0.25\n",
    "    - Cumulative reward for deep model : 130997 with conversion rate 0.25\n",
    "    \n",
    "> <font color='blue'>\n",
    "    As the dataset is made up of little data, we will first favour weak classifiers such as linear classifiers rather than classifiers based on deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifiers\n",
    "\n",
    "We can transform this problem of recommending items into a problem of binary classification, i.e. predicting if for a couple (user, item) is what the reward will be strictly positive.\n",
    "\n",
    "### 1) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    Thus, we first build the dataset to fit the linear model.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(data) : \n",
    "    df = pd.DataFrame({\"action\":data[\"action_history\"], \"reward\":data[\"rewards_history\"], \"item\":data[\"state_history\"]})\n",
    "    return df\n",
    "\n",
    "def prepare_data(df) : \n",
    "    x, y = [], []\n",
    "    for action, reward, item in zip(df[\"action\"], df[\"reward\"], df[\"item\"]) : \n",
    "        x.append(item[action][2:]) # We do not take care of the item_id and user_id\n",
    "        y.append(reward > 0) # True if reward > 0 else False\n",
    "    return x, y\n",
    "\n",
    "params = {\"user_id\" : USER_ID}\n",
    "data = load_data(url_reset, params)\n",
    "df = build_dataframe(data)\n",
    "x, y = prepare_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    After fiting the model: for each item in state_history, we calculate the probability that the user will buy this item, then we will choose the index of the item with the highest probability of being purchased.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward at run 1 : 66697 with conversion rate : 0.291\n",
      "Total reward at run 2 : 107235 with conversion rate : 0.323\n",
      "Total reward at run 3 : 104483 with conversion rate : 0.327\n",
      "Mean of total reward over 3 independant runs : 92805\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 1000\n",
    "mean = 0\n",
    "\n",
    "for k in range(1, number_run+1):\n",
    "    \n",
    "    # Load the data\n",
    "    params = {\"user_id\" : USER_ID}\n",
    "    data = load_data(url_reset, params)\n",
    "    # Build the dataframe and prepare the data\n",
    "    df = build_dataframe(data)\n",
    "    x, y = prepare_data(df)\n",
    "    next_state = data[\"next_state\"]\n",
    "    \n",
    "    # Define the model\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(x, y)\n",
    "\n",
    "    total_reward = 0\n",
    "    purchase = 0\n",
    "    \n",
    "    for i in range(generations):\n",
    "\n",
    "        predicted_item = np.argmax(logreg.predict_proba(np.array(next_state)[:,2:])[:,1])\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        if reward > 0 :\n",
    "            purchase += 1\n",
    "    \n",
    "    mean += total_reward\n",
    "        \n",
    "    print(\"Total reward at run %d : %d with conversion rate : %.3f\" % (k, total_reward, purchase/generations))\n",
    "print(\"Mean of total reward over 3 independant runs : %d\" % (mean/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    We observe that the conversion rate is better : 0.31 in average against 0.25 for Naive model. But the cumulative reward is much lower.\n",
    "    </font>\n",
    "    \n",
    "> <font color='blue'>\n",
    "    Since the reward is very closely linked to the price, it is more preferable to link the probability of the predict_proba to the price of the item: \n",
    "Thus for all probabilities of the predict_proba, we will multiply it by the price of the item: this quantity will represent a prediction of reward that the agent can earn instead of leaving only probabilities. \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward at run 1 : 252430 with conversion rate : 0.284\n",
      "Total reward at run 2 : 274211 with conversion rate : 0.310\n",
      "Total reward at run 3 : 282086 with conversion rate : 0.326\n",
      "Mean of total reward over 3 independant runs : 269576\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 1000\n",
    "mean = 0\n",
    "\n",
    "for k in range(1, number_run+1):\n",
    "    \n",
    "    # Load the data\n",
    "    params = {\"user_id\" : USER_ID}\n",
    "    data = load_data(url_reset, params)\n",
    "    # Build the dataframe and prepare the data\n",
    "    df = build_dataframe(data)\n",
    "    x, y = prepare_data(df)\n",
    "    next_state = data[\"next_state\"]\n",
    "    \n",
    "    # Define the model\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(x, y)\n",
    "\n",
    "    total_reward = 0\n",
    "    purchase = 0\n",
    "    \n",
    "    for i in range(generations):\n",
    "        \n",
    "        # Multiply each probability for purchase by the price of the item\n",
    "        predicted_item = np.argmax(logreg.predict_proba(np.array(next_state)[:,2:])[:,1] * np.array(next_state)[:,2])\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        if reward > 0 :\n",
    "            purchase += 1\n",
    "    \n",
    "    mean += total_reward\n",
    "        \n",
    "    print(\"Total reward at run %d : %d with conversion rate : %.3f\" % (k, total_reward, purchase/generations))\n",
    "print(\"Mean of total reward over 3 independant runs : %d\" % (mean/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    Thus, we observe that we earn more reward compared to the Naive Model, while keeping a very reasonable conversion rate (0.31 against 0.25 for the Naive Model). In a way, we outperform the naive model that does not using machine learning, so it is suffering from a high variance and bias. \n",
    "    </font>\n",
    "\n",
    "> <font color='blue'>\n",
    "    Now let's try to improve this by using online learning.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward at run 1 : 256165 with conversion rate : 0.318\n",
      "Total reward at run 2 : 253840 with conversion rate : 0.315\n",
      "Total reward at run 3 : 251844 with conversion rate : 0.331\n",
      "Mean of total reward over 3 independant runs : 253950\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 1000\n",
    "mean = 0\n",
    "\n",
    "for k in range(1, number_run+1):\n",
    "    \n",
    "    # Load the data\n",
    "    params = {\"user_id\" : USER_ID}\n",
    "    data = load_data(url_reset, params)\n",
    "    # Build the dataframe and prepare the data\n",
    "    df = build_dataframe(data)\n",
    "    x, y = prepare_data(df)\n",
    "    next_state = data[\"next_state\"]\n",
    "    \n",
    "    # Define the model\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(x, y)\n",
    "    \n",
    "    total_reward = 0\n",
    "    purchase = 0\n",
    "    \n",
    "    x_online = []\n",
    "    y_online = []\n",
    "    \n",
    "    for i in range(1, generations+1):\n",
    "        \n",
    "        # Multiply each probability for purchase by the price of the item\n",
    "        predicted_item = np.argmax(logreg.predict_proba(np.array(next_state)[:,2:])[:,1] * np.array(next_state)[:,2])\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        \n",
    "        # Online learning\n",
    "        x_online.append(np.array(next_state)[predicted_item, 2:])\n",
    "        y_online.append(reward > 0)\n",
    "        if i%30 == 0 : \n",
    "            logreg.fit(x_online, y_online)\n",
    "            x_online, y_online = [], []\n",
    "        \n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        if reward > 0 :\n",
    "            purchase += 1\n",
    "    \n",
    "    mean += total_reward\n",
    "        \n",
    "    print(\"Total reward at run %d : %d with conversion rate : %.3f\" % (k, total_reward, purchase/generations))\n",
    "print(\"Mean of total reward over 3 independant runs : %d\" % (mean/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    Thus the results are almost the same with or without online learning. So we will keep online learning in order to take into account new trends.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) All linear classifier and ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try all linear classifiers and ensemble methods : \n",
    "- logistic regressin\n",
    "- SVM\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_classifier(model,online_learning, weights) :\n",
    "\n",
    "    number_run = 3\n",
    "    generations = 1000\n",
    "    mean = 0\n",
    "\n",
    "    for k in range(1, number_run+1):\n",
    "\n",
    "        # Load the data\n",
    "        params = {\"user_id\" : USER_ID}\n",
    "        data = load_data(url_reset, params)\n",
    "        # Build the dataframe and prepare the data\n",
    "        df = build_dataframe(data)\n",
    "        x, y = prepare_data(df)\n",
    "        next_state = data[\"next_state\"]\n",
    "\n",
    "        # Define the model\n",
    "        if model == \"logreg\" :\n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(x, y)\n",
    "        elif model == \"svm\" : \n",
    "            clf = svm.SVC()\n",
    "            clf.probability = True\n",
    "            clf.fit(x,y)\n",
    "        elif model == \"rf\" :\n",
    "            clf = RandomForestClassifier(n_estimators=50, min_samples_split=10)\n",
    "            clf.fit(x, y)\n",
    "        elif model == \"GB\" : \n",
    "            clf = GradientBoostingClassifier(learning_rate=0.8, n_estimators=50)\n",
    "            clf.fit(x, y)\n",
    "        elif model == \"XGB\" : \n",
    "            clf = XGBClassifier(learning_rate=0.8, n_estimators=50)\n",
    "            clf.fit(np.array(x), np.array(y))\n",
    "\n",
    "        total_reward = 0\n",
    "        purchase = 0\n",
    "\n",
    "        x_online = []\n",
    "        y_online = []\n",
    "\n",
    "        for i in range(1, generations+1):\n",
    "            if weights : \n",
    "                # Multiply each probability for purchase by the price of the item\n",
    "                predicted_item = np.argmax(clf.predict_proba(np.array(next_state)[:,2:])[:,1] * np.array(next_state)[:,2])\n",
    "            else : \n",
    "                predicted_item = np.argmax(clf.predict_proba(np.array(next_state)[:,2:])[:,1])\n",
    "            params['recommended_item'] = predicted_item\n",
    "\n",
    "            r = requests.get(url = url_predict, params=params).json()\n",
    "            reward = r['reward']\n",
    "\n",
    "            # Online learning\n",
    "            if online_learning :\n",
    "                x_online.append(np.array(next_state)[predicted_item, 2:])\n",
    "                y_online.append(reward > 0)\n",
    "                if i%30 == 0 : \n",
    "                    clf.fit(np.array(x_online), np.array(y_online))\n",
    "                    x_online, y_online = [], []\n",
    "\n",
    "            total_reward += reward\n",
    "            next_state = r['state']\n",
    "\n",
    "            if reward > 0 :\n",
    "                purchase += 1\n",
    "\n",
    "        mean += total_reward\n",
    "\n",
    "        print(\"Total reward at run %d : %d with conversion rate : %.3f\" % (k, total_reward, purchase/generations))\n",
    "    print(\"Mean of total reward over 3 independant runs : %d\" % (mean/3))\n",
    "    return (mean/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------LogReg-------------------------+\n",
      "Total reward at run 1 : 162633 with conversion rate : 0.253\n",
      "Total reward at run 2 : 140144 with conversion rate : 0.336\n",
      "Total reward at run 3 : 77488 with conversion rate : 0.329\n",
      "Mean of total reward over 3 independant runs : 126755\n",
      "+-------------------------LogReg with weights-------------------------+\n",
      "Total reward at run 1 : 234390 with conversion rate : 0.253\n",
      "Total reward at run 2 : 241871 with conversion rate : 0.272\n",
      "Total reward at run 3 : 240670 with conversion rate : 0.298\n",
      "Mean of total reward over 3 independant runs : 238977\n",
      "+-------------------------LogReg with online learning-------------------------+\n",
      "Total reward at run 1 : 206017 with conversion rate : 0.234\n",
      "Total reward at run 2 : 264377 with conversion rate : 0.279\n",
      "Total reward at run 3 : 248992 with conversion rate : 0.284\n",
      "Mean of total reward over 3 independant runs : 239795\n",
      "\n",
      "\n",
      "+-------------------------SVM-------------------------+\n",
      "Total reward at run 1 : 150033 with conversion rate : 0.336\n",
      "Total reward at run 2 : 192363 with conversion rate : 0.339\n",
      "Total reward at run 3 : 113533 with conversion rate : 0.295\n",
      "Mean of total reward over 3 independant runs : 151976\n",
      "+-------------------------SVM with weights-------------------------+\n",
      "Total reward at run 1 : 222208 with conversion rate : 0.283\n",
      "Total reward at run 2 : 236903 with conversion rate : 0.264\n",
      "Total reward at run 3 : 200896 with conversion rate : 0.214\n",
      "Mean of total reward over 3 independant runs : 220002\n",
      "+-------------------------SVM with online learning-------------------------+\n",
      "Total reward at run 1 : 270167 with conversion rate : 0.294\n",
      "Total reward at run 2 : 216443 with conversion rate : 0.244\n",
      "Total reward at run 3 : 210065 with conversion rate : 0.239\n",
      "Mean of total reward over 3 independant runs : 232225\n",
      "\n",
      "\n",
      "+-------------------------RF-------------------------+\n",
      "Total reward at run 1 : 55981 with conversion rate : 0.244\n",
      "Total reward at run 2 : 178074 with conversion rate : 0.322\n",
      "Total reward at run 3 : 113149 with conversion rate : 0.270\n",
      "Mean of total reward over 3 independant runs : 115735\n",
      "+-------------------------RF with weights-------------------------+\n",
      "Total reward at run 1 : 322322 with conversion rate : 0.347\n",
      "Total reward at run 2 : 237328 with conversion rate : 0.284\n",
      "Total reward at run 3 : 212265 with conversion rate : 0.269\n",
      "Mean of total reward over 3 independant runs : 257305\n",
      "+-------------------------RF with online learning-------------------------+\n",
      "Total reward at run 1 : 259854 with conversion rate : 0.320\n",
      "Total reward at run 2 : 208099 with conversion rate : 0.240\n",
      "Total reward at run 3 : 251927 with conversion rate : 0.289\n",
      "Mean of total reward over 3 independant runs : 239960\n",
      "\n",
      "\n",
      "+-------------------------GB-------------------------+\n",
      "Total reward at run 1 : 184868 with conversion rate : 0.307\n",
      "Total reward at run 2 : 141750 with conversion rate : 0.311\n",
      "Total reward at run 3 : 132216 with conversion rate : 0.304\n",
      "Mean of total reward over 3 independant runs : 152945\n",
      "+-------------------------GB with weights-------------------------+\n",
      "Total reward at run 1 : 188634 with conversion rate : 0.272\n",
      "Total reward at run 2 : 263117 with conversion rate : 0.336\n",
      "Total reward at run 3 : 262000 with conversion rate : 0.347\n",
      "Mean of total reward over 3 independant runs : 237917\n",
      "+-------------------------GB with online learning-------------------------+\n",
      "Total reward at run 1 : 241906 with conversion rate : 0.320\n",
      "Total reward at run 2 : 259585 with conversion rate : 0.301\n",
      "Total reward at run 3 : 193314 with conversion rate : 0.263\n",
      "Mean of total reward over 3 independant runs : 231601\n",
      "\n",
      "\n",
      "+-------------------------XGBoost-------------------------+\n",
      "Total reward at run 1 : 162447 with conversion rate : 0.367\n",
      "Total reward at run 2 : 232050 with conversion rate : 0.403\n",
      "Total reward at run 3 : 152956 with conversion rate : 0.305\n",
      "Mean of total reward over 3 independant runs : 182484\n",
      "+-------------------------XGBoost with weights-------------------------+\n",
      "Total reward at run 1 : 244023 with conversion rate : 0.333\n",
      "Total reward at run 2 : 237163 with conversion rate : 0.300\n",
      "Total reward at run 3 : 288828 with conversion rate : 0.357\n",
      "Mean of total reward over 3 independant runs : 256671\n",
      "+-------------------------XGBoost with online learning-------------------------+\n",
      "Total reward at run 1 : 223967 with conversion rate : 0.271\n",
      "Total reward at run 2 : 203455 with conversion rate : 0.288\n",
      "Total reward at run 3 : 218282 with conversion rate : 0.262\n",
      "Mean of total reward over 3 independant runs : 215235\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "data_weights = []\n",
    "data_online_learning = []\n",
    "\n",
    "print(\"+-------------------------LogReg-------------------------+\")\n",
    "logreg_1 = linear_classifier(model=\"svm\", online_learning=False, weights=False)\n",
    "data.append(logreg_1)\n",
    "print(\"+-------------------------LogReg with weights-------------------------+\")\n",
    "logreg_2 = linear_classifier(model=\"svm\", online_learning=False, weights=True)\n",
    "data_weights.append(logreg_2)\n",
    "print(\"+-------------------------LogReg with online learning-------------------------+\")\n",
    "logreg_3 = linear_classifier(model=\"svm\", online_learning=True, weights=True)\n",
    "data_online_learning.append(logreg_3)\n",
    "print('\\n')\n",
    "print(\"+-------------------------SVM-------------------------+\")\n",
    "svm_1 = linear_classifier(model=\"svm\", online_learning=False, weights=False)\n",
    "data.append(svm_1)\n",
    "print(\"+-------------------------SVM with weights-------------------------+\")\n",
    "svm_2 = linear_classifier(model=\"svm\", online_learning=False, weights=True) \n",
    "data_weights.append(svm_2)\n",
    "print(\"+-------------------------SVM with online learning-------------------------+\")\n",
    "svm_3 = linear_classifier(model=\"svm\", online_learning=True, weights=True)\n",
    "data_online_learning.append(svm_3)\n",
    "print('\\n')\n",
    "print(\"+-------------------------RF-------------------------+\")\n",
    "rf_1 = linear_classifier(model=\"rf\", online_learning=False, weights=False)\n",
    "data.append(rf_1)\n",
    "print(\"+-------------------------RF with weights-------------------------+\")\n",
    "rf_2 = linear_classifier(model=\"rf\", online_learning=False, weights=True)\n",
    "data_weights.append(rf_2)\n",
    "print(\"+-------------------------RF with online learning-------------------------+\")\n",
    "rf_3 = linear_classifier(model=\"rf\", online_learning=True, weights=True)\n",
    "data_online_learning.append(rf_3)\n",
    "print('\\n')\n",
    "print(\"+-------------------------GB-------------------------+\")\n",
    "gb_1 = linear_classifier(model=\"GB\", online_learning=False, weights=False)\n",
    "data.append(gb_1)\n",
    "print(\"+-------------------------GB with weights-------------------------+\")\n",
    "gb_2 = linear_classifier(model=\"GB\", online_learning=False, weights=True)\n",
    "data_weights.append(gb_2)\n",
    "print(\"+-------------------------GB with online learning-------------------------+\")\n",
    "gb_3 = linear_classifier(model=\"GB\", online_learning=True, weights=True)\n",
    "data_online_learning.append(gb_3)\n",
    "print('\\n')\n",
    "print(\"+-------------------------XGBoost-------------------------+\")\n",
    "xgb_1 = linear_classifier(model=\"XGB\", online_learning=False, weights=False)\n",
    "data.append(xgb_1)\n",
    "print(\"+-------------------------XGBoost with weights-------------------------+\")\n",
    "xgb_2 = linear_classifier(model=\"XGB\", online_learning=False, weights=True)\n",
    "data_weights.append(xgb_2)\n",
    "print(\"+-------------------------XGBoost with online learning-------------------------+\")\n",
    "xgb_3 = linear_classifier(model=\"XGB\", online_learning=True, weights=True)\n",
    "data_online_learning.append(xgb_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar displaying its height\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.01*height,\n",
    "                '%d' % height,\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "width = 0.2\n",
    "alp = 0.8\n",
    "ind = np.arange(5)\n",
    "\n",
    "label = [\"Logreg\", \"SVM\", \"RF\", \"GB\", \"XGBoost\"]\n",
    "\n",
    "rects1 = ax.bar(ind, [l for l in data], \n",
    "                width = width,label='Without anything', color='skyblue')\n",
    "\n",
    "rects2 = ax.bar(ind + width, [l for l in data_weights], \n",
    "                width, alpha=alp, label='With price as weights', color='gold')\n",
    "#              \n",
    "rects3 = ax.bar(ind + 2 * width, [l for l in data_online_learning], \n",
    "                width, alpha=alp, label='With online learning', color='lightpink')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Influence of many factors in performance')\n",
    "ax.set_xticks(ind + 2 * width / 2)\n",
    "ax.set_xticklabels(label)\n",
    "#ax.set_ylim(0.4,0.9)\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    **So our 2nd approach is the one that works the most. Moreover, online learning also gives equivalent results. It must therefore be added to our models. There are not really any classifiers who stand out from the others. The overall ensemble methods generally work better.**\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

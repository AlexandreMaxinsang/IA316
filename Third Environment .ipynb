{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Concatenate, Dropout, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third environmement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['action_history', 'nb_items', 'nb_users', 'next_state', 'rewards_history', 'state_history'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attributes fa\n",
    "USER_ID = 'IAHIZPIW80WPGDW7P7JE'\n",
    "BASE_URL = \"http://35.180.178.243\"\n",
    "url_reset = BASE_URL + \"/reset\"\n",
    "url_predict = BASE_URL + \"/predict\"\n",
    "params = {\"user_id\" : USER_ID}\n",
    "\n",
    "def load_data(url_reset,params) :\n",
    "    r = requests.get(url=url_reset, params=params)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "data = load_data(url_reset,params)\n",
    "nb_users = data[\"nb_users\"]\n",
    "nb_items = data[\"nb_items\"]\n",
    "next_state = data[\"next_state\"]\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #See the request predict from our API\n",
    "# params = {\"user_id\" : user_id,\"recommended_item\": 2}\n",
    "# requests.get(url=url_predict, params=params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[36, 0, 431.715260015307, 0.9136113202751137,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[22, 0, 431.715260015307, 0.7639212155176396,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[78, 0, 431.715260015307, 0.1533502480463882,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>113.081583</td>\n",
       "      <td>[[5, 0, 431.715260015307, 0.25319661091464896,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[75, 0, 431.715260015307, 0.7772904200674785,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0               9         0.000000   \n",
       "1               4         0.000000   \n",
       "2               7         0.000000   \n",
       "3              29       113.081583   \n",
       "4               4         0.000000   \n",
       "\n",
       "                                       state_history  \n",
       "0  [[36, 0, 431.715260015307, 0.9136113202751137,...  \n",
       "1  [[22, 0, 431.715260015307, 0.7639212155176396,...  \n",
       "2  [[78, 0, 431.715260015307, 0.1533502480463882,...  \n",
       "3  [[5, 0, 431.715260015307, 0.25319661091464896,...  \n",
       "4  [[75, 0, 431.715260015307, 0.7772904200674785,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklisted_set = set((\"next_state\",\"nb_items\",\"nb_users\"))\n",
    "new_dict = {key : value for key, value in data.items() if key not in blacklisted_set}\n",
    "\n",
    "df = pd.DataFrame(new_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.210000</td>\n",
       "      <td>108.771438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.428589</td>\n",
       "      <td>243.844696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>982.671823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       action_history  rewards_history\n",
       "count      200.000000       200.000000\n",
       "mean        14.210000       108.771438\n",
       "std          8.428589       243.844696\n",
       "min          0.000000         0.000000\n",
       "25%          6.750000         0.000000\n",
       "50%         14.000000         0.000000\n",
       "75%         21.000000         0.000000\n",
       "max         29.000000       982.671823"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action_history     156\n",
       "rewards_history    156\n",
       "state_history      156\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['rewards_history']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>431.715260</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>0.193816</td>\n",
       "      <td>2.756896</td>\n",
       "      <td>1.008183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>657.000117</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>0.598705</td>\n",
       "      <td>0.499659</td>\n",
       "      <td>1.783631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>380.046806</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>2.338361</td>\n",
       "      <td>0.349464</td>\n",
       "      <td>2.200802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>220.205662</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>-0.310786</td>\n",
       "      <td>0.234045</td>\n",
       "      <td>-0.618043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>31.062136</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>0.123133</td>\n",
       "      <td>1.436098</td>\n",
       "      <td>1.186815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>461.783725</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>1.587258</td>\n",
       "      <td>1.230895</td>\n",
       "      <td>-0.269951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>978.504194</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>-0.572030</td>\n",
       "      <td>0.205803</td>\n",
       "      <td>1.239934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>494.500368</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>2.894246</td>\n",
       "      <td>0.740143</td>\n",
       "      <td>0.199190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>392.776779</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>0.262130</td>\n",
       "      <td>-0.645260</td>\n",
       "      <td>2.897554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>982.671823</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>-1.125255</td>\n",
       "      <td>1.991306</td>\n",
       "      <td>0.541967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>811.281738</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>-2.425234</td>\n",
       "      <td>0.801530</td>\n",
       "      <td>1.551791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>464.699521</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>0.206914</td>\n",
       "      <td>0.902344</td>\n",
       "      <td>1.278109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>531.125558</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>0.966576</td>\n",
       "      <td>0.340078</td>\n",
       "      <td>-0.545217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>828.008766</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>1.840380</td>\n",
       "      <td>1.224338</td>\n",
       "      <td>1.350459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>305.859439</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>-0.143827</td>\n",
       "      <td>1.140605</td>\n",
       "      <td>2.196252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>763.885452</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>0.534664</td>\n",
       "      <td>2.199344</td>\n",
       "      <td>2.124546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>706.159345</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>-0.214417</td>\n",
       "      <td>0.653734</td>\n",
       "      <td>1.712434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>736.340564</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>1.056546</td>\n",
       "      <td>1.264718</td>\n",
       "      <td>-0.582549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>36</td>\n",
       "      <td>18</td>\n",
       "      <td>332.035127</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>1.812460</td>\n",
       "      <td>-0.008659</td>\n",
       "      <td>0.489024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>692.780802</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>0.823034</td>\n",
       "      <td>0.451862</td>\n",
       "      <td>-0.297801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>36</td>\n",
       "      <td>20</td>\n",
       "      <td>536.343869</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>-0.956654</td>\n",
       "      <td>1.107363</td>\n",
       "      <td>0.379098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>746.498519</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>1.383314</td>\n",
       "      <td>0.490724</td>\n",
       "      <td>1.010523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>952.778756</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>1.077740</td>\n",
       "      <td>1.167964</td>\n",
       "      <td>0.717835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>758.480579</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>2.142417</td>\n",
       "      <td>-0.461178</td>\n",
       "      <td>1.794913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>43.080737</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>1.610101</td>\n",
       "      <td>1.139960</td>\n",
       "      <td>2.471106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>21.184357</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>0.209883</td>\n",
       "      <td>1.111563</td>\n",
       "      <td>1.840584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>36</td>\n",
       "      <td>26</td>\n",
       "      <td>927.844330</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>1.754547</td>\n",
       "      <td>-0.483586</td>\n",
       "      <td>0.688212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "      <td>765.604795</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>1.545159</td>\n",
       "      <td>0.598420</td>\n",
       "      <td>0.262876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>415.800811</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>2.371725</td>\n",
       "      <td>-0.387494</td>\n",
       "      <td>1.950125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "      <td>113.081583</td>\n",
       "      <td>0.913611</td>\n",
       "      <td>0.245992</td>\n",
       "      <td>1.098807</td>\n",
       "      <td>1.389522</td>\n",
       "      <td>0.487058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>431.715260</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>0.193816</td>\n",
       "      <td>2.756896</td>\n",
       "      <td>3.170770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>657.000117</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>0.598705</td>\n",
       "      <td>0.499659</td>\n",
       "      <td>-0.713785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>380.046806</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>2.338361</td>\n",
       "      <td>0.349464</td>\n",
       "      <td>1.726894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5934</th>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>220.205662</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>-0.310786</td>\n",
       "      <td>0.234045</td>\n",
       "      <td>1.311975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>31.062136</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>0.123133</td>\n",
       "      <td>1.436098</td>\n",
       "      <td>1.692702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>461.783725</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>1.587258</td>\n",
       "      <td>1.230895</td>\n",
       "      <td>1.401016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>978.504194</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>-0.572030</td>\n",
       "      <td>0.205803</td>\n",
       "      <td>0.959093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>494.500368</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>2.894246</td>\n",
       "      <td>0.740143</td>\n",
       "      <td>0.291029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>49</td>\n",
       "      <td>8</td>\n",
       "      <td>392.776779</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>0.262130</td>\n",
       "      <td>-0.645260</td>\n",
       "      <td>0.668147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5940</th>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>982.671823</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>-1.125255</td>\n",
       "      <td>1.991306</td>\n",
       "      <td>-0.738621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>811.281738</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>-2.425234</td>\n",
       "      <td>0.801530</td>\n",
       "      <td>0.775691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5942</th>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>464.699521</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>0.206914</td>\n",
       "      <td>0.902344</td>\n",
       "      <td>1.328724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5943</th>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>531.125558</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>0.966576</td>\n",
       "      <td>0.340078</td>\n",
       "      <td>1.697506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5944</th>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "      <td>828.008766</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>1.840380</td>\n",
       "      <td>1.224338</td>\n",
       "      <td>-0.377163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945</th>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>305.859439</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>-0.143827</td>\n",
       "      <td>1.140605</td>\n",
       "      <td>0.547571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5946</th>\n",
       "      <td>49</td>\n",
       "      <td>15</td>\n",
       "      <td>763.885452</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>0.534664</td>\n",
       "      <td>2.199344</td>\n",
       "      <td>1.248758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>49</td>\n",
       "      <td>16</td>\n",
       "      <td>706.159345</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>-0.214417</td>\n",
       "      <td>0.653734</td>\n",
       "      <td>0.506979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>49</td>\n",
       "      <td>17</td>\n",
       "      <td>736.340564</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>1.056546</td>\n",
       "      <td>1.264718</td>\n",
       "      <td>1.935106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>49</td>\n",
       "      <td>18</td>\n",
       "      <td>332.035127</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>1.812460</td>\n",
       "      <td>-0.008659</td>\n",
       "      <td>0.286151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>49</td>\n",
       "      <td>19</td>\n",
       "      <td>692.780802</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>0.823034</td>\n",
       "      <td>0.451862</td>\n",
       "      <td>1.095407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5951</th>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>536.343869</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>-0.956654</td>\n",
       "      <td>1.107363</td>\n",
       "      <td>1.449276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5952</th>\n",
       "      <td>49</td>\n",
       "      <td>21</td>\n",
       "      <td>746.498519</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>1.383314</td>\n",
       "      <td>0.490724</td>\n",
       "      <td>1.867436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5953</th>\n",
       "      <td>49</td>\n",
       "      <td>22</td>\n",
       "      <td>952.778756</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>1.077740</td>\n",
       "      <td>1.167964</td>\n",
       "      <td>1.055640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5954</th>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>758.480579</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>2.142417</td>\n",
       "      <td>-0.461178</td>\n",
       "      <td>1.592587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>49</td>\n",
       "      <td>24</td>\n",
       "      <td>43.080737</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>1.610101</td>\n",
       "      <td>1.139960</td>\n",
       "      <td>1.150442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5956</th>\n",
       "      <td>49</td>\n",
       "      <td>25</td>\n",
       "      <td>21.184357</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>0.209883</td>\n",
       "      <td>1.111563</td>\n",
       "      <td>0.036052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5957</th>\n",
       "      <td>49</td>\n",
       "      <td>26</td>\n",
       "      <td>927.844330</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>1.754547</td>\n",
       "      <td>-0.483586</td>\n",
       "      <td>-0.259572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5958</th>\n",
       "      <td>49</td>\n",
       "      <td>27</td>\n",
       "      <td>765.604795</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>1.545159</td>\n",
       "      <td>0.598420</td>\n",
       "      <td>3.032547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959</th>\n",
       "      <td>49</td>\n",
       "      <td>28</td>\n",
       "      <td>415.800811</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>2.371725</td>\n",
       "      <td>-0.387494</td>\n",
       "      <td>1.455402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5960</th>\n",
       "      <td>49</td>\n",
       "      <td>29</td>\n",
       "      <td>113.081583</td>\n",
       "      <td>0.835343</td>\n",
       "      <td>-1.544673</td>\n",
       "      <td>1.098807</td>\n",
       "      <td>1.389522</td>\n",
       "      <td>-1.231352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5961 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id       price        v1        v2        v3        v4  \\\n",
       "0          36        0  431.715260  0.913611  0.245992  0.193816  2.756896   \n",
       "1          36        1  657.000117  0.913611  0.245992  0.598705  0.499659   \n",
       "2          36        2  380.046806  0.913611  0.245992  2.338361  0.349464   \n",
       "3          36        3  220.205662  0.913611  0.245992 -0.310786  0.234045   \n",
       "4          36        4   31.062136  0.913611  0.245992  0.123133  1.436098   \n",
       "5          36        5  461.783725  0.913611  0.245992  1.587258  1.230895   \n",
       "6          36        6  978.504194  0.913611  0.245992 -0.572030  0.205803   \n",
       "7          36        7  494.500368  0.913611  0.245992  2.894246  0.740143   \n",
       "8          36        8  392.776779  0.913611  0.245992  0.262130 -0.645260   \n",
       "9          36        9  982.671823  0.913611  0.245992 -1.125255  1.991306   \n",
       "10         36       10  811.281738  0.913611  0.245992 -2.425234  0.801530   \n",
       "11         36       11  464.699521  0.913611  0.245992  0.206914  0.902344   \n",
       "12         36       12  531.125558  0.913611  0.245992  0.966576  0.340078   \n",
       "13         36       13  828.008766  0.913611  0.245992  1.840380  1.224338   \n",
       "14         36       14  305.859439  0.913611  0.245992 -0.143827  1.140605   \n",
       "15         36       15  763.885452  0.913611  0.245992  0.534664  2.199344   \n",
       "16         36       16  706.159345  0.913611  0.245992 -0.214417  0.653734   \n",
       "17         36       17  736.340564  0.913611  0.245992  1.056546  1.264718   \n",
       "18         36       18  332.035127  0.913611  0.245992  1.812460 -0.008659   \n",
       "19         36       19  692.780802  0.913611  0.245992  0.823034  0.451862   \n",
       "20         36       20  536.343869  0.913611  0.245992 -0.956654  1.107363   \n",
       "21         36       21  746.498519  0.913611  0.245992  1.383314  0.490724   \n",
       "22         36       22  952.778756  0.913611  0.245992  1.077740  1.167964   \n",
       "23         36       23  758.480579  0.913611  0.245992  2.142417 -0.461178   \n",
       "24         36       24   43.080737  0.913611  0.245992  1.610101  1.139960   \n",
       "25         36       25   21.184357  0.913611  0.245992  0.209883  1.111563   \n",
       "26         36       26  927.844330  0.913611  0.245992  1.754547 -0.483586   \n",
       "27         36       27  765.604795  0.913611  0.245992  1.545159  0.598420   \n",
       "28         36       28  415.800811  0.913611  0.245992  2.371725 -0.387494   \n",
       "29         36       29  113.081583  0.913611  0.245992  1.098807  1.389522   \n",
       "...       ...      ...         ...       ...       ...       ...       ...   \n",
       "5931       49        0  431.715260  0.835343 -1.544673  0.193816  2.756896   \n",
       "5932       49        1  657.000117  0.835343 -1.544673  0.598705  0.499659   \n",
       "5933       49        2  380.046806  0.835343 -1.544673  2.338361  0.349464   \n",
       "5934       49        3  220.205662  0.835343 -1.544673 -0.310786  0.234045   \n",
       "5935       49        4   31.062136  0.835343 -1.544673  0.123133  1.436098   \n",
       "5936       49        5  461.783725  0.835343 -1.544673  1.587258  1.230895   \n",
       "5937       49        6  978.504194  0.835343 -1.544673 -0.572030  0.205803   \n",
       "5938       49        7  494.500368  0.835343 -1.544673  2.894246  0.740143   \n",
       "5939       49        8  392.776779  0.835343 -1.544673  0.262130 -0.645260   \n",
       "5940       49        9  982.671823  0.835343 -1.544673 -1.125255  1.991306   \n",
       "5941       49       10  811.281738  0.835343 -1.544673 -2.425234  0.801530   \n",
       "5942       49       11  464.699521  0.835343 -1.544673  0.206914  0.902344   \n",
       "5943       49       12  531.125558  0.835343 -1.544673  0.966576  0.340078   \n",
       "5944       49       13  828.008766  0.835343 -1.544673  1.840380  1.224338   \n",
       "5945       49       14  305.859439  0.835343 -1.544673 -0.143827  1.140605   \n",
       "5946       49       15  763.885452  0.835343 -1.544673  0.534664  2.199344   \n",
       "5947       49       16  706.159345  0.835343 -1.544673 -0.214417  0.653734   \n",
       "5948       49       17  736.340564  0.835343 -1.544673  1.056546  1.264718   \n",
       "5949       49       18  332.035127  0.835343 -1.544673  1.812460 -0.008659   \n",
       "5950       49       19  692.780802  0.835343 -1.544673  0.823034  0.451862   \n",
       "5951       49       20  536.343869  0.835343 -1.544673 -0.956654  1.107363   \n",
       "5952       49       21  746.498519  0.835343 -1.544673  1.383314  0.490724   \n",
       "5953       49       22  952.778756  0.835343 -1.544673  1.077740  1.167964   \n",
       "5954       49       23  758.480579  0.835343 -1.544673  2.142417 -0.461178   \n",
       "5955       49       24   43.080737  0.835343 -1.544673  1.610101  1.139960   \n",
       "5956       49       25   21.184357  0.835343 -1.544673  0.209883  1.111563   \n",
       "5957       49       26  927.844330  0.835343 -1.544673  1.754547 -0.483586   \n",
       "5958       49       27  765.604795  0.835343 -1.544673  1.545159  0.598420   \n",
       "5959       49       28  415.800811  0.835343 -1.544673  2.371725 -0.387494   \n",
       "5960       49       29  113.081583  0.835343 -1.544673  1.098807  1.389522   \n",
       "\n",
       "            v5  \n",
       "0     1.008183  \n",
       "1     1.783631  \n",
       "2     2.200802  \n",
       "3    -0.618043  \n",
       "4     1.186815  \n",
       "5    -0.269951  \n",
       "6     1.239934  \n",
       "7     0.199190  \n",
       "8     2.897554  \n",
       "9     0.541967  \n",
       "10    1.551791  \n",
       "11    1.278109  \n",
       "12   -0.545217  \n",
       "13    1.350459  \n",
       "14    2.196252  \n",
       "15    2.124546  \n",
       "16    1.712434  \n",
       "17   -0.582549  \n",
       "18    0.489024  \n",
       "19   -0.297801  \n",
       "20    0.379098  \n",
       "21    1.010523  \n",
       "22    0.717835  \n",
       "23    1.794913  \n",
       "24    2.471106  \n",
       "25    1.840584  \n",
       "26    0.688212  \n",
       "27    0.262876  \n",
       "28    1.950125  \n",
       "29    0.487058  \n",
       "...        ...  \n",
       "5931  3.170770  \n",
       "5932 -0.713785  \n",
       "5933  1.726894  \n",
       "5934  1.311975  \n",
       "5935  1.692702  \n",
       "5936  1.401016  \n",
       "5937  0.959093  \n",
       "5938  0.291029  \n",
       "5939  0.668147  \n",
       "5940 -0.738621  \n",
       "5941  0.775691  \n",
       "5942  1.328724  \n",
       "5943  1.697506  \n",
       "5944 -0.377163  \n",
       "5945  0.547571  \n",
       "5946  1.248758  \n",
       "5947  0.506979  \n",
       "5948  1.935106  \n",
       "5949  0.286151  \n",
       "5950  1.095407  \n",
       "5951  1.449276  \n",
       "5952  1.867436  \n",
       "5953  1.055640  \n",
       "5954  1.592587  \n",
       "5955  1.150442  \n",
       "5956  0.036052  \n",
       "5957 -0.259572  \n",
       "5958  3.032547  \n",
       "5959  1.455402  \n",
       "5960 -1.231352  \n",
       "\n",
       "[5961 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_items_df = []\n",
    "for i in range(df.shape[0]):\n",
    "    users_items_df += df['state_history'][i] \n",
    "    \n",
    "users_items_df = pd.DataFrame(users_items_df,columns = ['user_id','item_id',\n",
    "                                                        'price','v1','v2','v3','v4','v5'])\n",
    "users_items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the first two covariates variables are features about users \n",
    "# and the forth and fifth variable are features about items\n",
    "# The variable 5 depends on .....\n",
    "\n",
    "user_v1 = users_items_df['v1'].unique()\n",
    "user_v2 = users_items_df['v2'].unique()\n",
    "users_ids = users_items_df['user_id'].unique()\n",
    "users_content= pd.DataFrame({'user_id' : users_ids,'v1':user_v1,'v2':user_v2 })\n",
    "\n",
    "item_v3 = users_items_df['v3'].unique()\n",
    "item_v4 = users_items_df['v4'].unique()\n",
    "item_id = users_items_df['item_id'].unique()\n",
    "items_content= pd.DataFrame({'item_id' : item_id,'v3':item_v3,'v4':item_v4 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df[df[\"rewards_history\"] > 0].reset_index(drop=True)\n",
    "neg_df = df[df[\"rewards_history\"] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>113.081583</td>\n",
       "      <td>[[5, 0, 431.715260015307, 0.25319661091464896,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>982.671823</td>\n",
       "      <td>[[81, 0, 431.715260015307, 1.6612831675612696,...</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>461.783725</td>\n",
       "      <td>[[37, 0, 431.715260015307, 2.178404367619431, ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>43.080737</td>\n",
       "      <td>[[5, 0, 431.715260015307, 0.25319661091464896,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>43.080737</td>\n",
       "      <td>[[8, 0, 431.715260015307, 1.55670750676844, 1....</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0              29       113.081583   \n",
       "1               9       982.671823   \n",
       "2               5       461.783725   \n",
       "3              24        43.080737   \n",
       "4              24        43.080737   \n",
       "\n",
       "                                       state_history  user_id  \n",
       "0  [[5, 0, 431.715260015307, 0.25319661091464896,...        5  \n",
       "1  [[81, 0, 431.715260015307, 1.6612831675612696,...       81  \n",
       "2  [[37, 0, 431.715260015307, 2.178404367619431, ...       37  \n",
       "3  [[5, 0, 431.715260015307, 0.25319661091464896,...        5  \n",
       "4  [[8, 0, 431.715260015307, 1.55670750676844, 1....        8  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df[\"user_id\"] = [pos_df[\"state_history\"][i][0][0] for i in range(pos_df.shape[0])]\n",
    "pos_df.head()## There are some users that have more than one positive item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[36, 0, 431.715260015307, 0.9136113202751137,...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[22, 0, 431.715260015307, 0.7639212155176396,...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[78, 0, 431.715260015307, 0.1533502480463882,...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[75, 0, 431.715260015307, 0.7772904200674785,...</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[34, 0, 431.715260015307, 0.19634282824106852...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0               9              0.0   \n",
       "1               4              0.0   \n",
       "2               7              0.0   \n",
       "3               4              0.0   \n",
       "4               3              0.0   \n",
       "\n",
       "                                       state_history  user_id  \n",
       "0  [[36, 0, 431.715260015307, 0.9136113202751137,...       36  \n",
       "1  [[22, 0, 431.715260015307, 0.7639212155176396,...       22  \n",
       "2  [[78, 0, 431.715260015307, 0.1533502480463882,...       78  \n",
       "3  [[75, 0, 431.715260015307, 0.7772904200674785,...       75  \n",
       "4  [[34, 0, 431.715260015307, 0.19634282824106852...       34  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df[\"user_id\"] = [neg_df[\"state_history\"][i][0][0] for i in range(neg_df.shape[0])]\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"Ignore y_true and return the mean of y_pred\n",
    "    \n",
    "    This is a hack to work-around the design of the Keras API that is\n",
    "    not really suited to train networks with a triplet loss by default.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(y_pred + 0 * y_true)\n",
    "\n",
    "\n",
    "def margin_comparator_loss(inputs, margin=1.):\n",
    "    \"\"\"Comparator loss for a pair of precomputed similarities\n",
    "    \n",
    "    If the inputs are cosine similarities, they each have range in\n",
    "    (-1, 1), therefore their difference have range in (-2, 2). Using\n",
    "    a margin of 1. can therefore make sense.\n",
    "\n",
    "    If the input similarities are not normalized, it can be beneficial\n",
    "    to use larger values for the margin of the comparator loss.\n",
    "    \"\"\"\n",
    "    positive_pair_sim, negative_pair_sim = inputs\n",
    "    return tf.maximum(negative_pair_sim - positive_pair_sim + margin, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model(state):\n",
    "    greater_index = 0\n",
    "    greater_value = 0\n",
    "    \n",
    "    if i in range(len(state)):\n",
    "        if(state[i][2] > greater_value):\n",
    "            greater_value = state[i][2]\n",
    "            greater_index = i\n",
    "    return greater_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 305808.927461 and Total reward per run: 101936.309154\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "n_epochs = 1000\n",
    "total_reward = 0\n",
    "\n",
    "for k in range(number_run):\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "\n",
    "        predicted_item = naive_model(next_state)\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        \n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, Dense\n",
    "from keras.layers import Lambda, Dot\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import dot, concatenate\n",
    "\n",
    "\n",
    "def build_models(n_users, n_items, latent_dim=64, l2_reg=0):\n",
    "    \"\"\"Build a triplet model and its companion similarity model\n",
    "    \n",
    "    The triplet model is used to train the weights of the companion\n",
    "    similarity model. The triplet model takes 1 user, 1 positive item\n",
    "    (relative to the selected user) and one negative item and is\n",
    "    trained with comparator loss.\n",
    "    \n",
    "    The similarity model takes one user and one item as input and return\n",
    "    compatibility score (aka the match score).\n",
    "    \"\"\"\n",
    "    # Common architectural components for the two models:\n",
    "    # - symbolic input placeholders\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "\n",
    "    # - embeddings\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(input_dim=n_users, output_dim=latent_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "    \n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(input_dim=n_items, output_dim=latent_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # - similarity computation between embeddings\n",
    "    positive_similarity = Dot(name=\"positive_similarity\",\n",
    "                              axes=1, normalize=True)(\n",
    "        [user_embedding, positive_item_embedding])\n",
    "    negative_similarity = Dot(name=\"negative_similarity\",\n",
    "                              axes=1, normalize=True)(\n",
    "        [user_embedding, negative_item_embedding])\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss,\n",
    "                          name='comparator_loss',\n",
    "                          output_shape=(1,))([positive_similarity, negative_similarity])\n",
    "\n",
    "    triplet_model = Model(inputs=[user_input,\n",
    "                                  positive_item_input,\n",
    "                                  negative_item_input],\n",
    "                          outputs=triplet_loss)\n",
    "    \n",
    "    # The match-score model, only use at inference to rank items for a given\n",
    "    # model: the model weights are shared with the triplet_model therefore\n",
    "    # we do not need to train it and therefore we do not need to plug a loss\n",
    "    # and an optimizer.\n",
    "    match_model = Model(inputs=[user_input, positive_item_input],\n",
    "                        outputs=positive_similarity)\n",
    "    \n",
    "    return triplet_model, match_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_df,max_items):\n",
    "    \"\"\"\n",
    "    Sample negatives at random\n",
    "    \n",
    "    \"\"\"\n",
    "    range_itens = np.arange(max_items)\n",
    "    user_ids = pos_df['user_id'].values\n",
    "    pos_item_ids = pos_df['action_history'].values\n",
    "    \n",
    "    \n",
    "    user_pos_items = pos_df.groupby('user_id')['action_history'].apply(list)\n",
    "    neg_item_ids = np.array([])\n",
    "    for i in user_ids:\n",
    "        if i in set(user_pos_items.index):\n",
    "            number = np.random.choice([item for item in range_itens \n",
    "                                       if not item in user_pos_items[user_pos_items.index == i]])\n",
    "        else:\n",
    "            number = np.random.choice(range_itens,1)\n",
    "        neg_item_ids = np.append(neg_item_ids,number)\n",
    "    \n",
    "  \n",
    "    return [user_ids, pos_item_ids, neg_item_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.9740\n",
      "Epoch 1/1\n",
      "45/45 [==============================] - 0s 99us/step - loss: 0.8877\n",
      "Epoch 1/1\n",
      "45/45 [==============================] - 0s 133us/step - loss: 0.8148\n",
      "Epoch 1/1\n",
      "45/45 [==============================] - 0s 83us/step - loss: 0.8302\n",
      "Epoch 1/1\n",
      "45/45 [==============================] - 0s 112us/step - loss: 0.7735\n",
      "Epoch 1/1\n",
      "46/46 [==============================] - 0s 87us/step - loss: 0.6963\n",
      "Epoch 1/1\n",
      "46/46 [==============================] - 0s 117us/step - loss: 0.7526\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 93us/step - loss: 0.7351\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 211us/step - loss: 0.6535\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 91us/step - loss: 0.6119\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 89us/step - loss: 0.6249\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 131us/step - loss: 0.5495\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 103us/step - loss: 0.5340\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 98us/step - loss: 0.5271\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 94us/step - loss: 0.5190\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 103us/step - loss: 0.5066\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 80us/step - loss: 0.3969\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 376us/step - loss: 0.4440\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 206us/step - loss: 0.3839\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 87us/step - loss: 0.3528\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 125us/step - loss: 0.3495\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 597us/step - loss: 0.3275\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 116us/step - loss: 0.2899\n",
      "Epoch 1/1\n",
      "47/47 [==============================] - 0s 77us/step - loss: 0.3206\n",
      "Epoch 1/1\n",
      "48/48 [==============================] - 0s 88us/step - loss: 0.3651\n",
      "Epoch 1/1\n",
      "48/48 [==============================] - 0s 78us/step - loss: 0.3390\n",
      "Epoch 1/1\n",
      "48/48 [==============================] - 0s 84us/step - loss: 0.2698\n",
      "Epoch 1/1\n",
      "48/48 [==============================] - 0s 106us/step - loss: 0.2639\n",
      "Epoch 1/1\n",
      "48/48 [==============================] - 0s 83us/step - loss: 0.2658\n",
      "Epoch 1/1\n",
      "48/48 [==============================] - 0s 83us/step - loss: 0.2616\n",
      "Epoch 1/1\n",
      "48/48 [==============================] - 0s 91us/step - loss: 0.2871\n",
      "Epoch 1/1\n",
      "48/48 [==============================] - 0s 91us/step - loss: 0.2833\n",
      "Epoch 1/1\n",
      "48/48 [==============================] - 0s 80us/step - loss: 0.2719\n",
      "Epoch 1/1\n",
      "49/49 [==============================] - 0s 84us/step - loss: 0.2411\n",
      "Epoch 1/1\n",
      "49/49 [==============================] - 0s 73us/step - loss: 0.2015\n",
      "Epoch 1/1\n",
      "49/49 [==============================] - 0s 103us/step - loss: 0.2481\n",
      "Epoch 1/1\n",
      "49/49 [==============================] - 0s 97us/step - loss: 0.1915\n",
      "Epoch 1/1\n",
      "49/49 [==============================] - 0s 77us/step - loss: 0.1926\n",
      "Epoch 1/1\n",
      "49/49 [==============================] - 0s 79us/step - loss: 0.1813\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 78us/step - loss: 0.1955\n",
      "Epoch 1/1\n",
      "51/51 [==============================] - 0s 99us/step - loss: 0.2249\n",
      "Epoch 1/1\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.2909\n",
      "Epoch 1/1\n",
      "53/53 [==============================] - 0s 74us/step - loss: 0.2435\n",
      "Epoch 1/1\n",
      "53/53 [==============================] - 0s 76us/step - loss: 0.2758\n",
      "Epoch 1/1\n",
      "53/53 [==============================] - 0s 83us/step - loss: 0.2015\n",
      "Epoch 1/1\n",
      "53/53 [==============================] - 0s 77us/step - loss: 0.1307\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 77us/step - loss: 0.1884\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 77us/step - loss: 0.1959\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 73us/step - loss: 0.2265\n",
      "Epoch 1/1\n",
      "55/55 [==============================] - 0s 77us/step - loss: 0.2354\n",
      "Epoch 1/1\n",
      "55/55 [==============================] - 0s 73us/step - loss: 0.1238\n",
      "Epoch 1/1\n",
      "55/55 [==============================] - 0s 74us/step - loss: 0.2119\n",
      "Epoch 1/1\n",
      "56/56 [==============================] - 0s 70us/step - loss: 0.1438\n",
      "Epoch 1/1\n",
      "56/56 [==============================] - 0s 96us/step - loss: 0.2042\n",
      "Epoch 1/1\n",
      "56/56 [==============================] - 0s 66us/step - loss: 0.1425\n",
      "Epoch 1/1\n",
      "56/56 [==============================] - 0s 79us/step - loss: 0.1338\n",
      "Epoch 1/1\n",
      "57/57 [==============================] - 0s 222us/step - loss: 0.1822\n",
      "Epoch 1/1\n",
      "57/57 [==============================] - 0s 69us/step - loss: 0.2560\n",
      "Epoch 1/1\n",
      "57/57 [==============================] - 0s 76us/step - loss: 0.2013\n",
      "Epoch 1/1\n",
      "58/58 [==============================] - 0s 93us/step - loss: 0.1528\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 88us/step - loss: 0.1994\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 64us/step - loss: 0.1737\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 178us/step - loss: 0.2018\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 76us/step - loss: 0.2077\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 68us/step - loss: 0.1719\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 64us/step - loss: 0.1879\n",
      "Epoch 1/1\n",
      "59/59 [==============================] - 0s 63us/step - loss: 0.1672\n",
      "Epoch 1/1\n",
      "60/60 [==============================] - 0s 62us/step - loss: 0.1491\n",
      "Epoch 1/1\n",
      "61/61 [==============================] - 0s 71us/step - loss: 0.1851\n",
      "Epoch 1/1\n",
      "61/61 [==============================] - 0s 63us/step - loss: 0.1120\n",
      "Epoch 1/1\n",
      "61/61 [==============================] - 0s 63us/step - loss: 0.1786\n",
      "Epoch 1/1\n",
      "62/62 [==============================] - 0s 61us/step - loss: 0.2308\n",
      "Epoch 1/1\n",
      "62/62 [==============================] - 0s 64us/step - loss: 0.1726\n",
      "Epoch 1/1\n",
      "63/63 [==============================] - 0s 59us/step - loss: 0.2427\n",
      "Epoch 1/1\n",
      "63/63 [==============================] - 0s 61us/step - loss: 0.1973\n",
      "Epoch 1/1\n",
      "63/63 [==============================] - 0s 76us/step - loss: 0.1701\n",
      "Epoch 1/1\n",
      "63/63 [==============================] - 0s 65us/step - loss: 0.1904\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 64us/step - loss: 0.2012\n",
      "Epoch 1/1\n",
      "65/65 [==============================] - 0s 81us/step - loss: 0.1661\n",
      "Epoch 1/1\n",
      "65/65 [==============================] - 0s 93us/step - loss: 0.1615\n",
      "Epoch 1/1\n",
      "65/65 [==============================] - 0s 84us/step - loss: 0.1702\n",
      "Epoch 1/1\n",
      "65/65 [==============================] - 0s 86us/step - loss: 0.1316\n",
      "Epoch 1/1\n",
      "65/65 [==============================] - 0s 88us/step - loss: 0.1573\n",
      "Epoch 1/1\n",
      "65/65 [==============================] - 0s 83us/step - loss: 0.1231\n",
      "Epoch 1/1\n",
      "65/65 [==============================] - 0s 87us/step - loss: 0.1024\n",
      "Epoch 1/1\n",
      "65/65 [==============================] - 0s 88us/step - loss: 0.1423\n",
      "Epoch 1/1\n",
      "66/66 [==============================] - 0s 88us/step - loss: 0.1670\n",
      "Epoch 1/1\n",
      "66/66 [==============================] - 0s 85us/step - loss: 0.1676\n",
      "Epoch 1/1\n",
      "67/67 [==============================] - 0s 83us/step - loss: 0.1748\n",
      "Epoch 1/1\n",
      "68/68 [==============================] - 0s 84us/step - loss: 0.1729\n",
      "Epoch 1/1\n",
      "68/68 [==============================] - 0s 96us/step - loss: 0.1236\n",
      "Epoch 1/1\n",
      "68/68 [==============================] - 0s 102us/step - loss: 0.1956\n",
      "Epoch 1/1\n",
      "69/69 [==============================] - 0s 87us/step - loss: 0.1265\n",
      "Epoch 1/1\n",
      "69/69 [==============================] - 0s 79us/step - loss: 0.1587\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 95us/step - loss: 0.1647\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 83us/step - loss: 0.1737\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 118us/step - loss: 0.1758\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 82us/step - loss: 0.1418\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 84us/step - loss: 0.1367\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 76us/step - loss: 0.1979\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 92us/step - loss: 0.1727\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 87us/step - loss: 0.1490\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 76us/step - loss: 0.1444\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 77us/step - loss: 0.1121\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 87us/step - loss: 0.1528\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 77us/step - loss: 0.1528\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 91us/step - loss: 0.1284\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 85us/step - loss: 0.1229\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 85us/step - loss: 0.2065\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 85us/step - loss: 0.1361\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 81us/step - loss: 0.1217\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 78us/step - loss: 0.1523\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 81us/step - loss: 0.2182\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 77us/step - loss: 0.1145\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 85us/step - loss: 0.1615\n",
      "Epoch 1/1\n",
      "70/70 [==============================] - 0s 80us/step - loss: 0.1697\n",
      "Epoch 1/1\n",
      "71/71 [==============================] - 0s 81us/step - loss: 0.1795\n",
      "Epoch 1/1\n",
      "71/71 [==============================] - 0s 80us/step - loss: 0.1477\n",
      "Epoch 1/1\n",
      "71/71 [==============================] - 0s 78us/step - loss: 0.1460\n",
      "Epoch 1/1\n",
      "71/71 [==============================] - 0s 75us/step - loss: 0.1400\n",
      "Epoch 1/1\n",
      "71/71 [==============================] - 0s 79us/step - loss: 0.1372\n",
      "Epoch 1/1\n",
      "71/71 [==============================] - 0s 75us/step - loss: 0.1307\n",
      "Epoch 1/1\n",
      "71/71 [==============================] - 0s 76us/step - loss: 0.2019\n",
      "Epoch 1/1\n",
      "71/71 [==============================] - 0s 79us/step - loss: 0.1170\n",
      "Epoch 1/1\n",
      "71/71 [==============================] - 0s 77us/step - loss: 0.1249\n",
      "Epoch 1/1\n",
      "71/71 [==============================] - 0s 85us/step - loss: 0.1562\n",
      "Epoch 1/1\n",
      "72/72 [==============================] - 0s 76us/step - loss: 0.1312\n",
      "Epoch 1/1\n",
      "73/73 [==============================] - 0s 78us/step - loss: 0.2238\n",
      "Epoch 1/1\n",
      "73/73 [==============================] - 0s 78us/step - loss: 0.1192\n",
      "Epoch 1/1\n",
      "74/74 [==============================] - 0s 77us/step - loss: 0.1545\n",
      "Epoch 1/1\n",
      "75/75 [==============================] - 0s 75us/step - loss: 0.2077\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 0s 75us/step - loss: 0.1634\n",
      "Epoch 1/1\n",
      "77/77 [==============================] - 0s 72us/step - loss: 0.2111\n",
      "Epoch 1/1\n",
      "77/77 [==============================] - 0s 71us/step - loss: 0.1802\n",
      "Epoch 1/1\n",
      "77/77 [==============================] - 0s 71us/step - loss: 0.1700\n",
      "Epoch 1/1\n",
      "77/77 [==============================] - 0s 70us/step - loss: 0.1816\n",
      "Epoch 1/1\n",
      "77/77 [==============================] - 0s 71us/step - loss: 0.1398\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 71us/step - loss: 0.2024\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 95us/step - loss: 0.1312\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 69us/step - loss: 0.1487\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 69us/step - loss: 0.1320\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 79us/step - loss: 0.1496\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 77us/step - loss: 0.1681\n",
      "Epoch 1/1\n",
      "79/79 [==============================] - 0s 70us/step - loss: 0.1464\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 0s 69us/step - loss: 0.2077\n",
      "Epoch 1/1\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.1615\n",
      "Epoch 1/1\n",
      "81/81 [==============================] - 0s 68us/step - loss: 0.1855\n",
      "Epoch 1/1\n",
      "81/81 [==============================] - 0s 66us/step - loss: 0.1588\n",
      "Epoch 1/1\n",
      "81/81 [==============================] - 0s 68us/step - loss: 0.1715\n",
      "Epoch 1/1\n",
      "81/81 [==============================] - 0s 66us/step - loss: 0.1846\n",
      "Epoch 1/1\n",
      "81/81 [==============================] - 0s 71us/step - loss: 0.1714\n",
      "Epoch 1/1\n",
      "81/81 [==============================] - 0s 70us/step - loss: 0.1491\n",
      "Epoch 1/1\n",
      "82/82 [==============================] - 0s 73us/step - loss: 0.1708\n",
      "Epoch 1/1\n",
      "83/83 [==============================] - 0s 67us/step - loss: 0.1593\n",
      "Epoch 1/1\n",
      "83/83 [==============================] - 0s 73us/step - loss: 0.1570\n",
      "Epoch 1/1\n",
      "83/83 [==============================] - 0s 65us/step - loss: 0.1783\n",
      "Epoch 1/1\n",
      "84/84 [==============================] - 0s 65us/step - loss: 0.1617\n",
      "Epoch 1/1\n",
      "84/84 [==============================] - 0s 67us/step - loss: 0.1634\n",
      "Epoch 1/1\n",
      "84/84 [==============================] - 0s 68us/step - loss: 0.1463\n",
      "Epoch 1/1\n",
      "85/85 [==============================] - 0s 64us/step - loss: 0.1645\n",
      "Epoch 1/1\n",
      "85/85 [==============================] - 0s 65us/step - loss: 0.1816\n",
      "Epoch 1/1\n",
      "85/85 [==============================] - 0s 69us/step - loss: 0.1377\n",
      "Epoch 1/1\n",
      "85/85 [==============================] - 0s 66us/step - loss: 0.1205\n",
      "Epoch 1/1\n",
      "86/86 [==============================] - 0s 69us/step - loss: 0.1468\n",
      "Epoch 1/1\n",
      "86/86 [==============================] - 0s 67us/step - loss: 0.1433\n",
      "Epoch 1/1\n",
      "86/86 [==============================] - 0s 64us/step - loss: 0.1380\n",
      "Epoch 1/1\n",
      "86/86 [==============================] - 0s 64us/step - loss: 0.1295\n",
      "Epoch 1/1\n",
      "86/86 [==============================] - 0s 64us/step - loss: 0.1848\n",
      "Epoch 1/1\n",
      "86/86 [==============================] - 0s 65us/step - loss: 0.1727\n",
      "Epoch 1/1\n",
      "86/86 [==============================] - 0s 65us/step - loss: 0.1653\n",
      "Epoch 1/1\n",
      "86/86 [==============================] - 0s 65us/step - loss: 0.1954\n",
      "Epoch 1/1\n",
      "87/87 [==============================] - 0s 63us/step - loss: 0.2219\n",
      "Epoch 1/1\n",
      "87/87 [==============================] - 0s 64us/step - loss: 0.1362\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 64us/step - loss: 0.1858\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 63us/step - loss: 0.1517\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 68us/step - loss: 0.1374\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.1312\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.2062\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 66us/step - loss: 0.1843\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 64us/step - loss: 0.1191\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 66us/step - loss: 0.1524\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 62us/step - loss: 0.1631\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.1166\n",
      "Epoch 1/1\n",
      "88/88 [==============================] - 0s 63us/step - loss: 0.1601\n",
      "Epoch 1/1\n",
      "89/89 [==============================] - 0s 61us/step - loss: 0.1203\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 0s 62us/step - loss: 0.1350\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 0s 71us/step - loss: 0.1464\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 0s 61us/step - loss: 0.1883\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 0s 59us/step - loss: 0.1695\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 0s 63us/step - loss: 0.2454\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 0s 63us/step - loss: 0.1355\n",
      "Epoch 1/1\n",
      "91/91 [==============================] - 0s 61us/step - loss: 0.1714\n",
      "Epoch 1/1\n",
      "91/91 [==============================] - 0s 61us/step - loss: 0.1414\n",
      "Epoch 1/1\n",
      "91/91 [==============================] - 0s 60us/step - loss: 0.1636\n",
      "Epoch 1/1\n",
      "92/92 [==============================] - 0s 63us/step - loss: 0.1770\n",
      "Epoch 1/1\n",
      "92/92 [==============================] - 0s 60us/step - loss: 0.1506\n",
      "Epoch 1/1\n",
      "92/92 [==============================] - 0s 65us/step - loss: 0.2085\n",
      "Epoch 1/1\n",
      "92/92 [==============================] - 0s 63us/step - loss: 0.1836\n",
      "Epoch 1/1\n",
      "93/93 [==============================] - 0s 62us/step - loss: 0.1569\n",
      "Epoch 1/1\n",
      "94/94 [==============================] - 0s 62us/step - loss: 0.1846\n",
      "Epoch 1/1\n",
      "94/94 [==============================] - 0s 65us/step - loss: 0.1926\n",
      "Epoch 1/1\n",
      "95/95 [==============================] - 0s 59us/step - loss: 0.1246\n",
      "Epoch 1/1\n",
      "96/96 [==============================] - 0s 62us/step - loss: 0.2153\n",
      "Epoch 1/1\n",
      "96/96 [==============================] - 0s 58us/step - loss: 0.1845\n",
      "Epoch 1/1\n",
      "96/96 [==============================] - 0s 60us/step - loss: 0.1832\n",
      "Epoch 1/1\n",
      "96/96 [==============================] - 0s 59us/step - loss: 0.1304\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 57us/step - loss: 0.1803\n",
      "Epoch 1/1\n",
      "96/96 [==============================] - 0s 59us/step - loss: 0.1382\n",
      "Epoch 1/1\n",
      "96/96 [==============================] - 0s 64us/step - loss: 0.2249\n",
      "Epoch 1/1\n",
      "96/96 [==============================] - 0s 56us/step - loss: 0.1982\n",
      "Epoch 1/1\n",
      "96/96 [==============================] - 0s 57us/step - loss: 0.1782\n",
      "Epoch 1/1\n",
      "97/97 [==============================] - 0s 73us/step - loss: 0.1407\n",
      "Epoch 1/1\n",
      "97/97 [==============================] - 0s 70us/step - loss: 0.2247\n",
      "Epoch 1/1\n",
      "97/97 [==============================] - 0s 70us/step - loss: 0.1079\n",
      "Epoch 1/1\n",
      "97/97 [==============================] - 0s 73us/step - loss: 0.2079\n",
      "Epoch 1/1\n",
      "97/97 [==============================] - 0s 71us/step - loss: 0.1686\n",
      "Epoch 1/1\n",
      "97/97 [==============================] - 0s 72us/step - loss: 0.1682\n",
      "Epoch 1/1\n",
      "98/98 [==============================] - 0s 69us/step - loss: 0.2162\n",
      "Epoch 1/1\n",
      "98/98 [==============================] - 0s 68us/step - loss: 0.1580\n",
      "Epoch 1/1\n",
      "99/99 [==============================] - 0s 71us/step - loss: 0.1986\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2112\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.2081\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 73us/step - loss: 0.1978\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.2311\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.1898\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.1971\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.1917\n",
      "Epoch 1/1\n",
      "101/101 [==============================] - 0s 69us/step - loss: 0.1789\n",
      "Epoch 1/1\n",
      "102/102 [==============================] - 0s 69us/step - loss: 0.2120\n",
      "Epoch 1/1\n",
      "102/102 [==============================] - 0s 71us/step - loss: 0.2107\n",
      "Epoch 1/1\n",
      "102/102 [==============================] - 0s 69us/step - loss: 0.1585\n",
      "Epoch 1/1\n",
      "102/102 [==============================] - 0s 69us/step - loss: 0.1969\n",
      "Epoch 1/1\n",
      "103/103 [==============================] - 0s 73us/step - loss: 0.1870\n",
      "Epoch 1/1\n",
      "103/103 [==============================] - 0s 74us/step - loss: 0.1794\n",
      "Epoch 1/1\n",
      "103/103 [==============================] - 0s 68us/step - loss: 0.1863\n",
      "Epoch 1/1\n",
      "103/103 [==============================] - 0s 75us/step - loss: 0.2106\n",
      "Epoch 1/1\n",
      "103/103 [==============================] - 0s 67us/step - loss: 0.2059\n",
      "Epoch 1/1\n",
      "103/103 [==============================] - 0s 86us/step - loss: 0.1802\n",
      "Epoch 1/1\n",
      "104/104 [==============================] - 0s 66us/step - loss: 0.2618\n",
      "Epoch 1/1\n",
      "104/104 [==============================] - 0s 65us/step - loss: 0.1912\n",
      "Epoch 1/1\n",
      "104/104 [==============================] - 0s 69us/step - loss: 0.2159\n",
      "Epoch 1/1\n",
      "104/104 [==============================] - 0s 65us/step - loss: 0.1789\n",
      "Epoch 1/1\n",
      "104/104 [==============================] - 0s 67us/step - loss: 0.1686\n",
      "Epoch 1/1\n",
      "104/104 [==============================] - 0s 72us/step - loss: 0.2066\n",
      "Epoch 1/1\n",
      "105/105 [==============================] - 0s 73us/step - loss: 0.1713\n",
      "Epoch 1/1\n",
      "106/106 [==============================] - 0s 66us/step - loss: 0.1885\n",
      "Epoch 1/1\n",
      "106/106 [==============================] - 0s 66us/step - loss: 0.2004\n",
      "Epoch 1/1\n",
      "107/107 [==============================] - 0s 65us/step - loss: 0.1651\n",
      "Epoch 1/1\n",
      "107/107 [==============================] - 0s 74us/step - loss: 0.1975\n",
      "Epoch 1/1\n",
      "107/107 [==============================] - 0s 67us/step - loss: 0.1956\n",
      "Epoch 1/1\n",
      "107/107 [==============================] - 0s 66us/step - loss: 0.1986\n",
      "Epoch 1/1\n",
      "107/107 [==============================] - 0s 64us/step - loss: 0.2001\n",
      "Epoch 1/1\n",
      "107/107 [==============================] - 0s 65us/step - loss: 0.1609\n",
      "Epoch 1/1\n",
      "107/107 [==============================] - 0s 65us/step - loss: 0.2353\n",
      "Epoch 1/1\n",
      "108/108 [==============================] - 0s 65us/step - loss: 0.1772\n",
      "Epoch 1/1\n",
      "108/108 [==============================] - 0s 65us/step - loss: 0.1680\n",
      "Epoch 1/1\n",
      "108/108 [==============================] - 0s 65us/step - loss: 0.1753\n",
      "Epoch 1/1\n",
      "108/108 [==============================] - 0s 71us/step - loss: 0.2113\n",
      "Epoch 1/1\n",
      "109/109 [==============================] - 0s 67us/step - loss: 0.1909\n",
      "Epoch 1/1\n",
      "109/109 [==============================] - 0s 69us/step - loss: 0.1631\n",
      "Epoch 1/1\n",
      "110/110 [==============================] - 0s 66us/step - loss: 0.1918\n",
      "Epoch 1/1\n",
      "110/110 [==============================] - 0s 74us/step - loss: 0.1768\n",
      "Epoch 1/1\n",
      "110/110 [==============================] - 0s 67us/step - loss: 0.1504\n",
      "Epoch 1/1\n",
      "111/111 [==============================] - 0s 66us/step - loss: 0.1521\n",
      "Epoch 1/1\n",
      "111/111 [==============================] - 0s 61us/step - loss: 0.2160\n",
      "Epoch 1/1\n",
      "111/111 [==============================] - 0s 66us/step - loss: 0.2082\n",
      "Epoch 1/1\n",
      "111/111 [==============================] - 0s 68us/step - loss: 0.1603\n",
      "Epoch 1/1\n",
      "112/112 [==============================] - 0s 64us/step - loss: 0.1635\n",
      "Epoch 1/1\n",
      "112/112 [==============================] - 0s 67us/step - loss: 0.2195\n",
      "Epoch 1/1\n",
      "112/112 [==============================] - 0s 61us/step - loss: 0.1934\n",
      "Epoch 1/1\n",
      "113/113 [==============================] - 0s 65us/step - loss: 0.1963\n",
      "Epoch 1/1\n",
      "113/113 [==============================] - 0s 62us/step - loss: 0.2097\n",
      "Epoch 1/1\n",
      "113/113 [==============================] - 0s 66us/step - loss: 0.2011\n",
      "Epoch 1/1\n",
      "114/114 [==============================] - 0s 61us/step - loss: 0.1761\n",
      "Epoch 1/1\n",
      "114/114 [==============================] - 0s 65us/step - loss: 0.1752\n",
      "Epoch 1/1\n",
      "115/115 [==============================] - 0s 66us/step - loss: 0.2278\n",
      "Epoch 1/1\n",
      "115/115 [==============================] - 0s 64us/step - loss: 0.1518\n",
      "Epoch 1/1\n",
      "116/116 [==============================] - 0s 60us/step - loss: 0.1685\n",
      "Epoch 1/1\n",
      "116/116 [==============================] - 0s 64us/step - loss: 0.2029\n",
      "Epoch 1/1\n",
      "116/116 [==============================] - 0s 60us/step - loss: 0.2491\n",
      "Epoch 1/1\n",
      "117/117 [==============================] - 0s 60us/step - loss: 0.2270\n",
      "Epoch 1/1\n",
      "117/117 [==============================] - 0s 62us/step - loss: 0.2164\n",
      "Epoch 1/1\n",
      "117/117 [==============================] - 0s 60us/step - loss: 0.1826\n",
      "Epoch 1/1\n",
      "117/117 [==============================] - 0s 65us/step - loss: 0.2042\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 0s 60us/step - loss: 0.1699\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 0s 60us/step - loss: 0.2161\n",
      "Epoch 1/1\n",
      "119/119 [==============================] - 0s 62us/step - loss: 0.2165\n",
      "Epoch 1/1\n",
      "120/120 [==============================] - 0s 62us/step - loss: 0.2285\n",
      "Epoch 1/1\n",
      "121/121 [==============================] - 0s 57us/step - loss: 0.2080\n",
      "Epoch 1/1\n",
      "121/121 [==============================] - 0s 58us/step - loss: 0.2252\n",
      "Epoch 1/1\n",
      "121/121 [==============================] - 0s 61us/step - loss: 0.1971\n",
      "Epoch 1/1\n",
      "121/121 [==============================] - 0s 59us/step - loss: 0.2237\n",
      "Epoch 1/1\n",
      "121/121 [==============================] - 0s 56us/step - loss: 0.2367\n",
      "Epoch 1/1\n",
      "121/121 [==============================] - 0s 60us/step - loss: 0.1618\n",
      "Epoch 1/1\n",
      "121/121 [==============================] - 0s 62us/step - loss: 0.2131\n",
      "Epoch 1/1\n",
      "121/121 [==============================] - 0s 57us/step - loss: 0.2361\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 56us/step - loss: 0.2297\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 65us/step - loss: 0.2119\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 57us/step - loss: 0.2039\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 57us/step - loss: 0.2284\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 55us/step - loss: 0.2041\n",
      "Epoch 1/1\n",
      "122/122 [==============================] - 0s 64us/step - loss: 0.2129\n",
      "Epoch 1/1\n",
      "123/123 [==============================] - 0s 61us/step - loss: 0.1754\n",
      "Epoch 1/1\n",
      "124/124 [==============================] - 0s 58us/step - loss: 0.1701\n",
      "Epoch 1/1\n",
      "124/124 [==============================] - 0s 58us/step - loss: 0.2083\n",
      "Epoch 1/1\n",
      "125/125 [==============================] - 0s 56us/step - loss: 0.2419\n",
      "Epoch 1/1\n",
      "125/125 [==============================] - 0s 61us/step - loss: 0.2395\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 0s 53us/step - loss: 0.2597\n",
      "Epoch 1/1\n",
      "126/126 [==============================] - 0s 60us/step - loss: 0.2269\n",
      "Epoch 1/1\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.1815\n",
      "Epoch 1/1\n",
      "126/126 [==============================] - 0s 54us/step - loss: 0.2512\n",
      "Epoch 1/1\n",
      "126/126 [==============================] - 0s 61us/step - loss: 0.2086\n",
      "Epoch 1/1\n",
      "126/126 [==============================] - 0s 55us/step - loss: 0.2092\n",
      "Epoch 1/1\n",
      "127/127 [==============================] - 0s 54us/step - loss: 0.2062\n",
      "Epoch 1/1\n",
      "127/127 [==============================] - 0s 54us/step - loss: 0.2562\n",
      "Epoch 1/1\n",
      "127/127 [==============================] - 0s 55us/step - loss: 0.2537\n",
      "Epoch 1/1\n",
      "127/127 [==============================] - 0s 58us/step - loss: 0.2167\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 55us/step - loss: 0.2818\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 55us/step - loss: 0.2042\n",
      "Epoch 1/1\n",
      "129/129 [==============================] - 0s 67us/step - loss: 0.2640\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 63us/step - loss: 0.2552\n",
      "Epoch 1/1\n",
      "131/131 [==============================] - 0s 65us/step - loss: 0.2280\n",
      "Epoch 1/1\n",
      "132/132 [==============================] - 0s 66us/step - loss: 0.2103\n",
      "Epoch 1/1\n",
      "133/133 [==============================] - 0s 62us/step - loss: 0.2190\n",
      "Epoch 1/1\n",
      "133/133 [==============================] - 0s 63us/step - loss: 0.2566\n",
      "Epoch 1/1\n",
      "133/133 [==============================] - 0s 64us/step - loss: 0.2267\n",
      "Epoch 1/1\n",
      "133/133 [==============================] - 0s 62us/step - loss: 0.1845\n",
      "Epoch 1/1\n",
      "134/134 [==============================] - 0s 63us/step - loss: 0.2712\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 0s 59us/step - loss: 0.2549\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 0s 62us/step - loss: 0.2778\n",
      "Epoch 1/1\n",
      "136/136 [==============================] - 0s 63us/step - loss: 0.2211\n",
      "Epoch 1/1\n",
      "136/136 [==============================] - 0s 59us/step - loss: 0.2462\n",
      "Epoch 1/1\n",
      "137/137 [==============================] - 0s 62us/step - loss: 0.2257\n",
      "Epoch 1/1\n",
      "137/137 [==============================] - 0s 61us/step - loss: 0.2335\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 59us/step - loss: 0.2271\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 60us/step - loss: 0.2924\n",
      "Epoch 1/1\n",
      "139/139 [==============================] - 0s 61us/step - loss: 0.2647\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 59us/step - loss: 0.2298\n",
      "Epoch 1/1\n",
      "141/141 [==============================] - 0s 62us/step - loss: 0.2583\n",
      "Epoch 1/1\n",
      "141/141 [==============================] - 0s 58us/step - loss: 0.2644\n",
      "Epoch 1/1\n",
      "141/141 [==============================] - 0s 61us/step - loss: 0.2150\n",
      "Epoch 1/1\n",
      "141/141 [==============================] - 0s 64us/step - loss: 0.2272\n",
      "Epoch 1/1\n",
      "141/141 [==============================] - 0s 59us/step - loss: 0.2303\n",
      "Epoch 1/1\n",
      "142/142 [==============================] - 0s 61us/step - loss: 0.1906\n",
      "Epoch 1/1\n",
      "142/142 [==============================] - 0s 59us/step - loss: 0.2357\n",
      "Epoch 1/1\n",
      "142/142 [==============================] - 0s 62us/step - loss: 0.2256\n",
      "Epoch 1/1\n",
      "143/143 [==============================] - 0s 57us/step - loss: 0.2108\n",
      "Epoch 1/1\n",
      "143/143 [==============================] - 0s 58us/step - loss: 0.2091\n",
      "Epoch 1/1\n",
      "143/143 [==============================] - 0s 58us/step - loss: 0.1929\n",
      "Epoch 1/1\n",
      "143/143 [==============================] - 0s 58us/step - loss: 0.2461\n",
      "Epoch 1/1\n",
      "143/143 [==============================] - 0s 58us/step - loss: 0.1988\n",
      "Epoch 1/1\n",
      "143/143 [==============================] - 0s 58us/step - loss: 0.2752\n",
      "Epoch 1/1\n",
      "143/143 [==============================] - 0s 58us/step - loss: 0.2146\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 58us/step - loss: 0.2159\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 56us/step - loss: 0.2538\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 58us/step - loss: 0.2086\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 57us/step - loss: 0.2587\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 55us/step - loss: 0.2314\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 57us/step - loss: 0.2220\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 60us/step - loss: 0.2323\n",
      "Epoch 1/1\n",
      "145/145 [==============================] - 0s 57us/step - loss: 0.2305\n",
      "Epoch 1/1\n",
      "145/145 [==============================] - 0s 59us/step - loss: 0.2395\n",
      "Epoch 1/1\n",
      "145/145 [==============================] - 0s 58us/step - loss: 0.2133\n",
      "Epoch 1/1\n",
      "146/146 [==============================] - 0s 62us/step - loss: 0.2118\n",
      "Epoch 1/1\n",
      "147/147 [==============================] - 0s 57us/step - loss: 0.1943\n",
      "Epoch 1/1\n",
      "147/147 [==============================] - 0s 57us/step - loss: 0.2376\n",
      "Epoch 1/1\n",
      "147/147 [==============================] - 0s 57us/step - loss: 0.2316\n",
      "Epoch 1/1\n",
      "147/147 [==============================] - 0s 55us/step - loss: 0.2544\n",
      "Epoch 1/1\n",
      "147/147 [==============================] - 0s 56us/step - loss: 0.2475\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 54us/step - loss: 0.2582\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 59us/step - loss: 0.2167\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 58us/step - loss: 0.2202\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 55us/step - loss: 0.2430\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 57us/step - loss: 0.2153\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 60us/step - loss: 0.1681\n",
      "Epoch 1/1\n",
      "149/149 [==============================] - 0s 55us/step - loss: 0.2458\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 57us/step - loss: 0.2254\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 55us/step - loss: 0.2683\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 55us/step - loss: 0.2427\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 56us/step - loss: 0.2379\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 57us/step - loss: 0.2164\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 54us/step - loss: 0.2572\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 61us/step - loss: 0.1977\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 54us/step - loss: 0.2763\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 53us/step - loss: 0.2839\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 61us/step - loss: 0.2352\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 56us/step - loss: 0.2633\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 57us/step - loss: 0.2524\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 56us/step - loss: 0.2061\n",
      "Epoch 1/1\n",
      "151/151 [==============================] - 0s 54us/step - loss: 0.2404\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 54us/step - loss: 0.1886\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 56us/step - loss: 0.2079\n",
      "Epoch 1/1\n",
      "153/153 [==============================] - 0s 55us/step - loss: 0.2030\n",
      "Epoch 1/1\n",
      "153/153 [==============================] - 0s 53us/step - loss: 0.2966\n",
      "Epoch 1/1\n",
      "153/153 [==============================] - 0s 55us/step - loss: 0.2301\n",
      "Epoch 1/1\n",
      "153/153 [==============================] - 0s 54us/step - loss: 0.2239\n",
      "Epoch 1/1\n",
      "153/153 [==============================] - 0s 54us/step - loss: 0.2450\n",
      "Epoch 1/1\n",
      "153/153 [==============================] - 0s 56us/step - loss: 0.2248\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 54us/step - loss: 0.2367\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 54us/step - loss: 0.2632\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 58us/step - loss: 0.2259\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 55us/step - loss: 0.1901\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 54us/step - loss: 0.1962\n",
      "Epoch 1/1\n",
      "155/155 [==============================] - 0s 57us/step - loss: 0.1923\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 55us/step - loss: 0.2241\n",
      "Epoch 1/1\n",
      "157/157 [==============================] - 0s 53us/step - loss: 0.2217\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 55us/step - loss: 0.2468\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 52us/step - loss: 0.2321\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 0s 53us/step - loss: 0.1349\n",
      "Epoch 1/1\n",
      "159/159 [==============================] - 0s 58us/step - loss: 0.2388\n",
      "Epoch 1/1\n",
      "159/159 [==============================] - 0s 51us/step - loss: 0.2249\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 53us/step - loss: 0.2345\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 66us/step - loss: 0.2384\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 59us/step - loss: 0.2436\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 61us/step - loss: 0.3036\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 63us/step - loss: 0.2258\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 59us/step - loss: 0.2177\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 59us/step - loss: 0.2139\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 60us/step - loss: 0.1942\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 63us/step - loss: 0.1876\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 60us/step - loss: 0.2790\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 62us/step - loss: 0.2293\n",
      "Epoch 1/1\n",
      "161/161 [==============================] - 0s 59us/step - loss: 0.2484\n",
      "Epoch 1/1\n",
      "162/162 [==============================] - 0s 62us/step - loss: 0.2767\n",
      "Epoch 1/1\n",
      "162/162 [==============================] - 0s 61us/step - loss: 0.2623\n",
      "Epoch 1/1\n",
      "162/162 [==============================] - 0s 60us/step - loss: 0.2071\n",
      "Epoch 1/1\n",
      "162/162 [==============================] - 0s 58us/step - loss: 0.3011\n",
      "Epoch 1/1\n",
      "163/163 [==============================] - 0s 58us/step - loss: 0.2149\n",
      "Epoch 1/1\n",
      "163/163 [==============================] - 0s 60us/step - loss: 0.2323\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 57us/step - loss: 0.2320\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 60us/step - loss: 0.2041\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 55us/step - loss: 0.2431\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 56us/step - loss: 0.2545\n",
      "Epoch 1/1\n",
      "165/165 [==============================] - 0s 60us/step - loss: 0.2435\n",
      "Epoch 1/1\n",
      "165/165 [==============================] - 0s 61us/step - loss: 0.2553\n",
      "Epoch 1/1\n",
      "166/166 [==============================] - 0s 57us/step - loss: 0.2596\n",
      "Epoch 1/1\n",
      "167/167 [==============================] - 0s 58us/step - loss: 0.2401\n",
      "Epoch 1/1\n",
      "167/167 [==============================] - 0s 59us/step - loss: 0.2532\n",
      "Epoch 1/1\n",
      "167/167 [==============================] - 0s 58us/step - loss: 0.2487\n",
      "Epoch 1/1\n",
      "168/168 [==============================] - 0s 57us/step - loss: 0.2510\n",
      "Epoch 1/1\n",
      "168/168 [==============================] - 0s 58us/step - loss: 0.2345\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 0s 57us/step - loss: 0.2171\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 0s 57us/step - loss: 0.2522\n",
      "Epoch 1/1\n",
      "170/170 [==============================] - 0s 57us/step - loss: 0.2541\n",
      "Epoch 1/1\n",
      "170/170 [==============================] - 0s 59us/step - loss: 0.2177\n",
      "Epoch 1/1\n",
      "171/171 [==============================] - 0s 54us/step - loss: 0.2404\n",
      "Epoch 1/1\n",
      "171/171 [==============================] - 0s 58us/step - loss: 0.2194\n",
      "Epoch 1/1\n",
      "172/172 [==============================] - 0s 54us/step - loss: 0.2354\n",
      "Epoch 1/1\n",
      "173/173 [==============================] - 0s 56us/step - loss: 0.2029\n",
      "Epoch 1/1\n",
      "173/173 [==============================] - 0s 57us/step - loss: 0.2741\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 54us/step - loss: 0.2571\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 55us/step - loss: 0.2778\n",
      "Epoch 1/1\n",
      "175/175 [==============================] - 0s 55us/step - loss: 0.2576\n",
      "Epoch 1/1\n",
      "176/176 [==============================] - 0s 56us/step - loss: 0.2392\n",
      "Epoch 1/1\n",
      "177/177 [==============================] - 0s 57us/step - loss: 0.2426\n",
      "Epoch 1/1\n",
      "177/177 [==============================] - 0s 52us/step - loss: 0.2359\n",
      "Epoch 1/1\n",
      "177/177 [==============================] - 0s 55us/step - loss: 0.2412\n",
      "Epoch 1/1\n",
      "177/177 [==============================] - 0s 53us/step - loss: 0.2399\n",
      "Epoch 1/1\n",
      "177/177 [==============================] - 0s 56us/step - loss: 0.2628\n",
      "Epoch 1/1\n",
      "177/177 [==============================] - 0s 53us/step - loss: 0.2289\n",
      "Epoch 1/1\n",
      "178/178 [==============================] - 0s 56us/step - loss: 0.2620\n",
      "Epoch 1/1\n",
      "179/179 [==============================] - 0s 53us/step - loss: 0.2521\n",
      "Epoch 1/1\n",
      "179/179 [==============================] - 0s 55us/step - loss: 0.2475\n",
      "Epoch 1/1\n",
      "179/179 [==============================] - 0s 57us/step - loss: 0.2598\n",
      "Epoch 1/1\n",
      "179/179 [==============================] - 0s 53us/step - loss: 0.2121\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 53us/step - loss: 0.2597\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 52us/step - loss: 0.2354\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 53us/step - loss: 0.2274\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 52us/step - loss: 0.2830\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 53us/step - loss: 0.2269\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 53us/step - loss: 0.2325\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 55us/step - loss: 0.2420\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 53us/step - loss: 0.2820\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 53us/step - loss: 0.2624\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 52us/step - loss: 0.2292\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 54us/step - loss: 0.1944\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 55us/step - loss: 0.2521\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 54us/step - loss: 0.2019\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 52us/step - loss: 0.2286\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 53us/step - loss: 0.1907\n",
      "Epoch 1/1\n",
      "181/181 [==============================] - 0s 51us/step - loss: 0.2455\n",
      "Epoch 1/1\n",
      "181/181 [==============================] - 0s 53us/step - loss: 0.2379\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 53us/step - loss: 0.2655\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 55us/step - loss: 0.1912\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 53us/step - loss: 0.2552\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 53us/step - loss: 0.2488\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 54us/step - loss: 0.2053\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 54us/step - loss: 0.2413\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 52us/step - loss: 0.2728\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 53us/step - loss: 0.2615\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 53us/step - loss: 0.2119\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 53us/step - loss: 0.2266\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 51us/step - loss: 0.2358\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 53us/step - loss: 0.2955\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 0s 52us/step - loss: 0.2539\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 53us/step - loss: 0.2299\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 55us/step - loss: 0.2354\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 51us/step - loss: 0.2309\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 53us/step - loss: 0.2576\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 52us/step - loss: 0.2879\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 53us/step - loss: 0.2288\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 55us/step - loss: 0.2464\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 54us/step - loss: 0.2823\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 51us/step - loss: 0.2589\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 53us/step - loss: 0.2653\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 52us/step - loss: 0.2124\n",
      "Epoch 1/1\n",
      "183/183 [==============================] - 0s 55us/step - loss: 0.2454\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 55us/step - loss: 0.2322\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 52us/step - loss: 0.2433\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 51us/step - loss: 0.2833\n",
      "Epoch 1/1\n",
      "185/185 [==============================] - 0s 51us/step - loss: 0.2545\n",
      "Epoch 1/1\n",
      "185/185 [==============================] - 0s 52us/step - loss: 0.2338\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 55us/step - loss: 0.2800\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 54us/step - loss: 0.2488\n",
      "Epoch 1/1\n",
      "187/187 [==============================] - 0s 51us/step - loss: 0.2498\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 52us/step - loss: 0.2223\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 51us/step - loss: 0.2521\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 52us/step - loss: 0.2228\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 51us/step - loss: 0.2498\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 50us/step - loss: 0.2634\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 49us/step - loss: 0.2943\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 51us/step - loss: 0.2608\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 54us/step - loss: 0.2528\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 53us/step - loss: 0.1967\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 62us/step - loss: 0.2189\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 54us/step - loss: 0.2694\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 50us/step - loss: 0.2764\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 51us/step - loss: 0.2115\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 53us/step - loss: 0.2365\n",
      "Epoch 1/1\n",
      "189/189 [==============================] - 0s 51us/step - loss: 0.2200\n",
      "Epoch 1/1\n",
      "189/189 [==============================] - 0s 53us/step - loss: 0.2260\n",
      "Epoch 1/1\n",
      "189/189 [==============================] - 0s 51us/step - loss: 0.2549\n",
      "Epoch 1/1\n",
      "189/189 [==============================] - 0s 52us/step - loss: 0.2205\n",
      "Epoch 1/1\n",
      "189/189 [==============================] - 0s 55us/step - loss: 0.2276\n",
      "Epoch 1/1\n",
      "189/189 [==============================] - 0s 54us/step - loss: 0.2720\n",
      "Epoch 1/1\n",
      "189/189 [==============================] - 0s 50us/step - loss: 0.2370\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 56us/step - loss: 0.2236\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 50us/step - loss: 0.2671\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 50us/step - loss: 0.2720\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 58us/step - loss: 0.2746\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 52us/step - loss: 0.3028\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 50us/step - loss: 0.2409\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 51us/step - loss: 0.2627\n",
      "Epoch 1/1\n",
      "191/191 [==============================] - 0s 51us/step - loss: 0.2375\n",
      "Epoch 1/1\n",
      "192/192 [==============================] - 0s 51us/step - loss: 0.2970\n",
      "Epoch 1/1\n",
      "193/193 [==============================] - 0s 55us/step - loss: 0.2367\n",
      "Epoch 1/1\n",
      "193/193 [==============================] - 0s 57us/step - loss: 0.2683\n",
      "Epoch 1/1\n",
      "193/193 [==============================] - 0s 57us/step - loss: 0.2714\n",
      "Epoch 1/1\n",
      "193/193 [==============================] - 0s 63us/step - loss: 0.2503\n",
      "Epoch 1/1\n",
      "193/193 [==============================] - 0s 55us/step - loss: 0.2561\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 57us/step - loss: 0.2374\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 54us/step - loss: 0.2703\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 59us/step - loss: 0.2651\n",
      "Epoch 1/1\n",
      "195/195 [==============================] - 0s 56us/step - loss: 0.2806\n",
      "Epoch 1/1\n",
      "195/195 [==============================] - 0s 57us/step - loss: 0.3049\n",
      "Epoch 1/1\n",
      "195/195 [==============================] - 0s 60us/step - loss: 0.2320\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 55us/step - loss: 0.2751\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 68us/step - loss: 0.2380\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 61us/step - loss: 0.2701\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 76us/step - loss: 0.2723\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 80us/step - loss: 0.2719\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 60us/step - loss: 0.3027\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 55us/step - loss: 0.2683\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "n_epochs = 1\n",
    "total_reward = 0\n",
    "generations = 1000\n",
    "\n",
    "for k in range(number_run):\n",
    "\n",
    "    triplet_model, match_model = build_models(nb_users, nb_items, latent_dim=16, l2_reg=1e-6)\n",
    "    # we plug the identity loss and the a fake target variable ignored by\n",
    "    # the model to be able to use the Keras API to train the triplet model\n",
    "    triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "    fake_y = np.ones_like(pos_df['user_id'])\n",
    "    params = {'user_id': USER_ID}\n",
    "\n",
    "    positive_data = pos_df.copy()\n",
    "\n",
    "    for j in range(generations):\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            # Sample new negatives to build different triplets at each epoch\n",
    "            triplet_inputs = sample_triplets(positive_data, max_items=nb_items)\n",
    "\n",
    "            # Fit the model incrementally by doing a single pass over the\n",
    "            # sampled triplets.\n",
    "            triplet_model.fit(triplet_inputs, fake_y, shuffle=True, epochs=1)\n",
    "\n",
    "        # The user that we need to recommend a product\n",
    "        new_user_id = next_state[0][0]\n",
    "        # Take all the items in the new state\n",
    "        items_ids = np.array([next_state[i][1]\n",
    "                              for i in range(len(next_state))])\n",
    "\n",
    "        # Create an array of the same size of 'items_ids' of all values equal to 'new_user_id'\n",
    "        repeated_user_id = np.empty_like(items_ids)\n",
    "        repeated_user_id.fill(new_user_id)\n",
    "\n",
    "        # predict the product using our model\n",
    "        predicted = match_model.predict([repeated_user_id, items_ids])\n",
    "        predicted_item = np.argmax(predicted)\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        # take the reward from the API\n",
    "        r = requests.get(url=url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "\n",
    "        # Check if the reward is greater than 0 , if yes add to the 'positive_data'\n",
    "        if(reward > 0):\n",
    "            positive_data = positive_data.append({'action_history': predicted_item,\n",
    "                                                  'state_history': next_state,\n",
    "                                                  'user_id': next_state[0][0]},\n",
    "                                                 ignore_index=True)\n",
    "\n",
    "        fake_y = np.ones_like(positive_data['user_id'])\n",
    "        # Take the nex_state\n",
    "        next_state = r['state']\n",
    "\n",
    "\n",
    "print(\"Total reward: %f and Total reward per run: %f\" %(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dense, Dropout\n",
    "from keras.layers import Concatenate, Lambda\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def make_interaction_mlp(input_dim, n_hidden=1, hidden_size=64,\n",
    "                         dropout=0, l2_reg=None):\n",
    "    \"\"\"\n",
    "    Build the shared multi layer perceptron\n",
    "\n",
    "    \"\"\"\n",
    "    mlp = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        # Plug the output unit directly: this is a simple\n",
    "        # linear regression model. Not dropout required.\n",
    "        mlp.add(Dense(1, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "    else:\n",
    "        mlp.add(Dense(hidden_size, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "        mlp.add(Dropout(dropout))\n",
    "        for i in range(n_hidden - 1):\n",
    "            mlp.add(Dense(hidden_size, activation='relu',\n",
    "                          kernel_regularizer=l2_reg))\n",
    "            mlp.add(Dropout(dropout))\n",
    "        mlp.add(Dense(1, activation='relu', kernel_regularizer=l2_reg))\n",
    "    return mlp\n",
    "\n",
    "\n",
    "def build_models(n_users, n_items, user_dim=32, item_dim=64,\n",
    "                 n_hidden=1, hidden_size=64, dropout=0, l2_reg=0):\n",
    "    \"\"\"\n",
    "    Build models to train a deep triplet network\n",
    "\n",
    "    \"\"\"\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "    positive_meta_input = Input((2,), name='positive_meta_input')\n",
    "    negative_meta_input = Input((2,), name='negative_meta_input')\n",
    "\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(n_users, user_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "\n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(n_items, item_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # Similarity computation between embeddings using a MLP similarity\n",
    "    positive_embeddings_pair = Concatenate(name=\"positive_embeddings_pair\")(\n",
    "        [user_embedding, positive_item_embedding, positive_meta_input])\n",
    "    positive_embeddings_pair = Dropout(dropout)(positive_embeddings_pair)\n",
    "\n",
    "    negative_embeddings_pair = Concatenate(name=\"negative_embeddings_pair\")(\n",
    "        [user_embedding, negative_item_embedding, negative_meta_input])\n",
    "    negative_embeddings_pair = Dropout(dropout)(negative_embeddings_pair)\n",
    "\n",
    "    # Instanciate the shared similarity architecture\n",
    "    interaction_layers = make_interaction_mlp(\n",
    "        user_dim + item_dim + 2, n_hidden=n_hidden, hidden_size=hidden_size,\n",
    "        dropout=dropout, l2_reg=l2_reg)\n",
    "\n",
    "    positive_similarity = interaction_layers(positive_embeddings_pair)\n",
    "    negative_similarity = interaction_layers(negative_embeddings_pair)\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss, output_shape=(1,),\n",
    "                          name='comparator_loss')(\n",
    "        [positive_similarity, negative_similarity])\n",
    "\n",
    "    deep_triplet_model = Model(inputs=[user_input,\n",
    "                                       positive_item_input,\n",
    "                                       negative_item_input,\n",
    "                                       positive_meta_input,\n",
    "                                       negative_meta_input\n",
    "                                       ],\n",
    "                               outputs=[triplet_loss])\n",
    "\n",
    "    # The match-score model, only used at inference\n",
    "    deep_match_model = Model(inputs=[user_input, positive_item_input, positive_meta_input],\n",
    "                             outputs=[positive_similarity])\n",
    "\n",
    "    return deep_match_model, deep_triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_df, items_content, max_items):\n",
    "    \"\"\"\n",
    "    Sample the data and put in the format \n",
    "    [user_ids, pos_item_ids, neg_item_ids,pos_item_metadata,neg_item_metadata]\n",
    "\n",
    "    \"\"\"\n",
    "    range_itens = np.arange(max_items)\n",
    "    user_ids = pos_df['user_id'].values\n",
    "    pos_item_ids = pos_df['action_history'].values\n",
    "\n",
    "    pos_item_metadata = items_content.loc[pos_item_ids][['v3', 'v4']].values\n",
    "\n",
    "    # TODO\n",
    "    user_pos_items = pos_df.groupby('user_id')['action_history'].apply(list)\n",
    "    neg_item_ids = np.array([])\n",
    "    for i in user_ids:\n",
    "        if i in set(user_pos_items.index):\n",
    "            number = np.random.choice([item for item in range_itens \n",
    "                                       if not item in user_pos_items[user_pos_items.index == i]])\n",
    "        else:\n",
    "            number = np.random.choice(range_itens, 1)\n",
    "        neg_item_ids = np.append(neg_item_ids, number)\n",
    "\n",
    "    neg_item_metadata = items_content.loc[neg_item_ids][['v3', 'v4']].values\n",
    "\n",
    "    return [user_ids, pos_item_ids, neg_item_ids, pos_item_metadata, neg_item_metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = dict(\n",
    "    user_dim=50,\n",
    "    item_dim=16,\n",
    "    n_hidden=1,\n",
    "    hidden_size=16,\n",
    "    dropout=0.2,\n",
    "    l2_reg=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_run = 3\n",
    "n_epochs = 1\n",
    "total_reward = 0\n",
    "generations = 1000\n",
    "\n",
    "\n",
    "for k in range(number_run):\n",
    "\n",
    "    deep_match_model, deep_triplet_model = build_models(nb_users, nb_items,**hyper_parameters)\n",
    "    # we plug the identity loss and the a fake target variable ignored by\n",
    "    # the model to be able to use the Keras API to train the triplet model\n",
    "    deep_triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "    fake_y = np.ones_like(pos_data['user_id'])\n",
    "    params = {'user_id': USER_ID}\n",
    "    \n",
    "    positive_data = pos_df.copy()\n",
    "\n",
    "    for j in range(generations):\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "\n",
    "            # Sample new negatives to build different triplets at each epoch\n",
    "            triplet_inputs = sample_triplets(\n",
    "                positive_data, items_content, max_items=nb_items)\n",
    "\n",
    "            # Fit the model incrementally by doing a single pass over the\n",
    "            # sampled triplets.\n",
    "            deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                                   batch_size=64, epochs=1)\n",
    "\n",
    "        # Predicting\n",
    "        new_user_id = next_state[0][0]\n",
    "        items_ids = np.array([next_state[i][1]\n",
    "                              for i in range(len(next_state))])\n",
    "        repeated_user_id = np.empty_like(items_ids)\n",
    "        repeated_user_id.fill(new_user_id)\n",
    "\n",
    "        pos_metadata = items_content.loc[items_ids][['v3', 'v4']].values\n",
    "        predicted = deep_match_model.predict(\n",
    "            [repeated_user_id, items_ids, pos_metadata])\n",
    "        predicted_item = np.argmax(predicted)\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url=url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "\n",
    "        if(reward > 0):\n",
    "            positive_data = positive_data.append({'action_history': predicted_item,\n",
    "                                                  'state_history': next_state,\n",
    "                                                  'user_id': next_state[0][0]},\n",
    "                                                 ignore_index=True)\n",
    "\n",
    "        fake_y = np.ones_like(positive_data['user_id'])\n",
    "        next_state = r['state']\n",
    "\n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

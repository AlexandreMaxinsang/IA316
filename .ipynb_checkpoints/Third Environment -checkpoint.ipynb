{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project on recommander systems : Third Environmement\n",
    "\n",
    "#### Author : Raymond KUOCH and Alexandre MAXINSANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Concatenate, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['action_history', 'nb_items', 'nb_users', 'next_state', 'rewards_history', 'state_history'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attributes \n",
    "USER_ID = 'IAHIZPIW80WPGDW7P7JE'\n",
    "BASE_URL = \"http://35.180.178.243\"\n",
    "url_reset = BASE_URL + \"/reset\"\n",
    "url_predict = BASE_URL + \"/predict\"\n",
    "params = {\"user_id\" : USER_ID}\n",
    "\n",
    "def load_data(url_reset,params) :\n",
    "    r = requests.get(url=url_reset, params=params)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "data = load_data(url_reset,params)\n",
    "nb_users = data[\"nb_users\"]\n",
    "nb_items = data[\"nb_items\"]\n",
    "next_state = data[\"next_state\"]\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[78, 0, 669.3972072487512, 0.1267472727154109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>89.071751</td>\n",
       "      <td>[[32, 0, 669.3972072487512, 1.5760900723032845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>32.166424</td>\n",
       "      <td>[[29, 0, 669.3972072487512, 1.3201441218369938...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[16, 0, 669.3972072487512, 1.1395888598214905...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[32, 0, 669.3972072487512, 1.5760900723032845...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0              22         0.000000   \n",
       "1               4        89.071751   \n",
       "2              23        32.166424   \n",
       "3              29         0.000000   \n",
       "4               2         0.000000   \n",
       "\n",
       "                                       state_history  \n",
       "0  [[78, 0, 669.3972072487512, 0.1267472727154109...  \n",
       "1  [[32, 0, 669.3972072487512, 1.5760900723032845...  \n",
       "2  [[29, 0, 669.3972072487512, 1.3201441218369938...  \n",
       "3  [[16, 0, 669.3972072487512, 1.1395888598214905...  \n",
       "4  [[32, 0, 669.3972072487512, 1.5760900723032845...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklisted_set = set((\"next_state\",\"nb_items\",\"nb_users\"))\n",
    "new_dict = {key : value for key, value in data.items() if key not in blacklisted_set}\n",
    "\n",
    "df = pd.DataFrame(new_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.990000</td>\n",
       "      <td>176.521194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.234008</td>\n",
       "      <td>290.139641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>339.861719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>956.670243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       action_history  rewards_history\n",
       "count      200.000000       200.000000\n",
       "mean        14.990000       176.521194\n",
       "std          8.234008       290.139641\n",
       "min          0.000000         0.000000\n",
       "25%          9.000000         0.000000\n",
       "50%         16.000000         0.000000\n",
       "75%         21.000000       339.861719\n",
       "max         29.000000       956.670243"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action_history     123\n",
       "rewards_history    123\n",
       "state_history      123\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['rewards_history']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>price</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>669.397207</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.136273</td>\n",
       "      <td>2.340520</td>\n",
       "      <td>1.412632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>790.698445</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.660775</td>\n",
       "      <td>-0.827421</td>\n",
       "      <td>2.167880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>126.291602</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.862382</td>\n",
       "      <td>0.714819</td>\n",
       "      <td>0.140997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>607.207472</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>-0.687495</td>\n",
       "      <td>0.348351</td>\n",
       "      <td>1.389516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>89.071751</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.936429</td>\n",
       "      <td>3.144384</td>\n",
       "      <td>0.524762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>78</td>\n",
       "      <td>5</td>\n",
       "      <td>500.300782</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>2.287400</td>\n",
       "      <td>0.785726</td>\n",
       "      <td>0.881132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>78</td>\n",
       "      <td>6</td>\n",
       "      <td>328.222061</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.714169</td>\n",
       "      <td>0.452891</td>\n",
       "      <td>3.143563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>78</td>\n",
       "      <td>7</td>\n",
       "      <td>55.972493</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.703540</td>\n",
       "      <td>1.652969</td>\n",
       "      <td>1.275741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>918.789420</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.288299</td>\n",
       "      <td>1.824092</td>\n",
       "      <td>1.040711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>374.780692</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>2.230748</td>\n",
       "      <td>1.616011</td>\n",
       "      <td>1.296468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>78</td>\n",
       "      <td>10</td>\n",
       "      <td>479.205347</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.367243</td>\n",
       "      <td>0.946868</td>\n",
       "      <td>1.921442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>129.437314</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.455375</td>\n",
       "      <td>1.638058</td>\n",
       "      <td>1.974233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "      <td>840.778594</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.751387</td>\n",
       "      <td>1.593795</td>\n",
       "      <td>-0.200903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>78</td>\n",
       "      <td>13</td>\n",
       "      <td>167.745696</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.342300</td>\n",
       "      <td>1.573207</td>\n",
       "      <td>1.312079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>78</td>\n",
       "      <td>14</td>\n",
       "      <td>673.345997</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.194431</td>\n",
       "      <td>1.201813</td>\n",
       "      <td>1.507455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>78</td>\n",
       "      <td>15</td>\n",
       "      <td>284.427405</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.550911</td>\n",
       "      <td>1.136435</td>\n",
       "      <td>1.654379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78</td>\n",
       "      <td>16</td>\n",
       "      <td>117.249879</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.965586</td>\n",
       "      <td>1.835404</td>\n",
       "      <td>3.110217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>78</td>\n",
       "      <td>17</td>\n",
       "      <td>702.660323</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.711317</td>\n",
       "      <td>1.217806</td>\n",
       "      <td>1.038178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>78</td>\n",
       "      <td>18</td>\n",
       "      <td>463.742426</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>-1.786886</td>\n",
       "      <td>1.550778</td>\n",
       "      <td>1.730284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>78</td>\n",
       "      <td>19</td>\n",
       "      <td>774.007775</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.376041</td>\n",
       "      <td>1.225591</td>\n",
       "      <td>-0.535410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>78</td>\n",
       "      <td>20</td>\n",
       "      <td>956.670243</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>3.261380</td>\n",
       "      <td>0.812874</td>\n",
       "      <td>0.296267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>78</td>\n",
       "      <td>21</td>\n",
       "      <td>856.190655</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.260633</td>\n",
       "      <td>2.328331</td>\n",
       "      <td>2.016604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>78</td>\n",
       "      <td>22</td>\n",
       "      <td>501.052674</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.362323</td>\n",
       "      <td>1.971156</td>\n",
       "      <td>1.114079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>78</td>\n",
       "      <td>23</td>\n",
       "      <td>32.166424</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.996532</td>\n",
       "      <td>0.556029</td>\n",
       "      <td>1.753426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>78</td>\n",
       "      <td>24</td>\n",
       "      <td>635.457239</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.352316</td>\n",
       "      <td>-0.039424</td>\n",
       "      <td>2.368985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>78</td>\n",
       "      <td>25</td>\n",
       "      <td>805.812603</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.705435</td>\n",
       "      <td>0.916320</td>\n",
       "      <td>1.093568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>78</td>\n",
       "      <td>26</td>\n",
       "      <td>128.189917</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.520646</td>\n",
       "      <td>2.784435</td>\n",
       "      <td>0.020155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>78</td>\n",
       "      <td>27</td>\n",
       "      <td>700.460909</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>3.712177</td>\n",
       "      <td>2.940265</td>\n",
       "      <td>1.348737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>78</td>\n",
       "      <td>28</td>\n",
       "      <td>604.353170</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>0.881103</td>\n",
       "      <td>0.882971</td>\n",
       "      <td>1.132085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78</td>\n",
       "      <td>29</td>\n",
       "      <td>13.778164</td>\n",
       "      <td>0.126747</td>\n",
       "      <td>1.798951</td>\n",
       "      <td>1.805581</td>\n",
       "      <td>0.371109</td>\n",
       "      <td>0.762880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5897</th>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "      <td>604.353170</td>\n",
       "      <td>0.064040</td>\n",
       "      <td>1.487957</td>\n",
       "      <td>0.881103</td>\n",
       "      <td>0.882971</td>\n",
       "      <td>1.992845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5898</th>\n",
       "      <td>96</td>\n",
       "      <td>29</td>\n",
       "      <td>13.778164</td>\n",
       "      <td>0.064040</td>\n",
       "      <td>1.487957</td>\n",
       "      <td>1.805581</td>\n",
       "      <td>0.371109</td>\n",
       "      <td>0.171471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>669.397207</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.136273</td>\n",
       "      <td>2.340520</td>\n",
       "      <td>3.348205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>790.698445</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.660775</td>\n",
       "      <td>-0.827421</td>\n",
       "      <td>0.102910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5901</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>126.291602</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>1.862382</td>\n",
       "      <td>0.714819</td>\n",
       "      <td>0.752498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>607.207472</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>-0.687495</td>\n",
       "      <td>0.348351</td>\n",
       "      <td>1.359929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>89.071751</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.936429</td>\n",
       "      <td>3.144384</td>\n",
       "      <td>1.494489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>500.300782</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>2.287400</td>\n",
       "      <td>0.785726</td>\n",
       "      <td>-0.088030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>328.222061</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.714169</td>\n",
       "      <td>0.452891</td>\n",
       "      <td>1.748767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>55.972493</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>1.703540</td>\n",
       "      <td>1.652969</td>\n",
       "      <td>-0.445365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>918.789420</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.288299</td>\n",
       "      <td>1.824092</td>\n",
       "      <td>-0.790891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>374.780692</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>2.230748</td>\n",
       "      <td>1.616011</td>\n",
       "      <td>2.148185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>479.205347</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.367243</td>\n",
       "      <td>0.946868</td>\n",
       "      <td>0.851215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>129.437314</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>1.455375</td>\n",
       "      <td>1.638058</td>\n",
       "      <td>1.476287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5911</th>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>840.778594</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>1.751387</td>\n",
       "      <td>1.593795</td>\n",
       "      <td>0.006224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5912</th>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>167.745696</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.342300</td>\n",
       "      <td>1.573207</td>\n",
       "      <td>2.492427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>673.345997</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.194431</td>\n",
       "      <td>1.201813</td>\n",
       "      <td>0.079537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5914</th>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>284.427405</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>1.550911</td>\n",
       "      <td>1.136435</td>\n",
       "      <td>1.765428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>702.660323</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>1.711317</td>\n",
       "      <td>1.217806</td>\n",
       "      <td>3.020111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>463.742426</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>-1.786886</td>\n",
       "      <td>1.550778</td>\n",
       "      <td>1.113513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>774.007775</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>1.376041</td>\n",
       "      <td>1.225591</td>\n",
       "      <td>1.440668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>956.670243</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>3.261380</td>\n",
       "      <td>0.812874</td>\n",
       "      <td>2.184920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>856.190655</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>1.260633</td>\n",
       "      <td>2.328331</td>\n",
       "      <td>0.531175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5920</th>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>501.052674</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.362323</td>\n",
       "      <td>1.971156</td>\n",
       "      <td>0.167420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5921</th>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>635.457239</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>1.352316</td>\n",
       "      <td>-0.039424</td>\n",
       "      <td>2.251390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>805.812603</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.705435</td>\n",
       "      <td>0.916320</td>\n",
       "      <td>0.199287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>128.189917</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.520646</td>\n",
       "      <td>2.784435</td>\n",
       "      <td>1.787093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>700.460909</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>3.712177</td>\n",
       "      <td>2.940265</td>\n",
       "      <td>1.984631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925</th>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>604.353170</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.881103</td>\n",
       "      <td>0.882971</td>\n",
       "      <td>1.798022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5926</th>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>13.778164</td>\n",
       "      <td>2.227239</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>1.805581</td>\n",
       "      <td>0.371109</td>\n",
       "      <td>2.591765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5927 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id       price        v1        v2        v3        v4  \\\n",
       "0          78        0  669.397207  0.126747  1.798951  0.136273  2.340520   \n",
       "1          78        1  790.698445  0.126747  1.798951  0.660775 -0.827421   \n",
       "2          78        2  126.291602  0.126747  1.798951  1.862382  0.714819   \n",
       "3          78        3  607.207472  0.126747  1.798951 -0.687495  0.348351   \n",
       "4          78        4   89.071751  0.126747  1.798951  0.936429  3.144384   \n",
       "5          78        5  500.300782  0.126747  1.798951  2.287400  0.785726   \n",
       "6          78        6  328.222061  0.126747  1.798951  0.714169  0.452891   \n",
       "7          78        7   55.972493  0.126747  1.798951  1.703540  1.652969   \n",
       "8          78        8  918.789420  0.126747  1.798951  0.288299  1.824092   \n",
       "9          78        9  374.780692  0.126747  1.798951  2.230748  1.616011   \n",
       "10         78       10  479.205347  0.126747  1.798951  0.367243  0.946868   \n",
       "11         78       11  129.437314  0.126747  1.798951  1.455375  1.638058   \n",
       "12         78       12  840.778594  0.126747  1.798951  1.751387  1.593795   \n",
       "13         78       13  167.745696  0.126747  1.798951  0.342300  1.573207   \n",
       "14         78       14  673.345997  0.126747  1.798951  0.194431  1.201813   \n",
       "15         78       15  284.427405  0.126747  1.798951  1.550911  1.136435   \n",
       "16         78       16  117.249879  0.126747  1.798951  0.965586  1.835404   \n",
       "17         78       17  702.660323  0.126747  1.798951  1.711317  1.217806   \n",
       "18         78       18  463.742426  0.126747  1.798951 -1.786886  1.550778   \n",
       "19         78       19  774.007775  0.126747  1.798951  1.376041  1.225591   \n",
       "20         78       20  956.670243  0.126747  1.798951  3.261380  0.812874   \n",
       "21         78       21  856.190655  0.126747  1.798951  1.260633  2.328331   \n",
       "22         78       22  501.052674  0.126747  1.798951  0.362323  1.971156   \n",
       "23         78       23   32.166424  0.126747  1.798951  1.996532  0.556029   \n",
       "24         78       24  635.457239  0.126747  1.798951  1.352316 -0.039424   \n",
       "25         78       25  805.812603  0.126747  1.798951  0.705435  0.916320   \n",
       "26         78       26  128.189917  0.126747  1.798951  0.520646  2.784435   \n",
       "27         78       27  700.460909  0.126747  1.798951  3.712177  2.940265   \n",
       "28         78       28  604.353170  0.126747  1.798951  0.881103  0.882971   \n",
       "29         78       29   13.778164  0.126747  1.798951  1.805581  0.371109   \n",
       "...       ...      ...         ...       ...       ...       ...       ...   \n",
       "5897       96       28  604.353170  0.064040  1.487957  0.881103  0.882971   \n",
       "5898       96       29   13.778164  0.064040  1.487957  1.805581  0.371109   \n",
       "5899       19        0  669.397207  2.227239  0.730774  0.136273  2.340520   \n",
       "5900       19        1  790.698445  2.227239  0.730774  0.660775 -0.827421   \n",
       "5901       19        2  126.291602  2.227239  0.730774  1.862382  0.714819   \n",
       "5902       19        3  607.207472  2.227239  0.730774 -0.687495  0.348351   \n",
       "5903       19        4   89.071751  2.227239  0.730774  0.936429  3.144384   \n",
       "5904       19        5  500.300782  2.227239  0.730774  2.287400  0.785726   \n",
       "5905       19        6  328.222061  2.227239  0.730774  0.714169  0.452891   \n",
       "5906       19        7   55.972493  2.227239  0.730774  1.703540  1.652969   \n",
       "5907       19        8  918.789420  2.227239  0.730774  0.288299  1.824092   \n",
       "5908       19        9  374.780692  2.227239  0.730774  2.230748  1.616011   \n",
       "5909       19       10  479.205347  2.227239  0.730774  0.367243  0.946868   \n",
       "5910       19       11  129.437314  2.227239  0.730774  1.455375  1.638058   \n",
       "5911       19       12  840.778594  2.227239  0.730774  1.751387  1.593795   \n",
       "5912       19       13  167.745696  2.227239  0.730774  0.342300  1.573207   \n",
       "5913       19       14  673.345997  2.227239  0.730774  0.194431  1.201813   \n",
       "5914       19       15  284.427405  2.227239  0.730774  1.550911  1.136435   \n",
       "5915       19       17  702.660323  2.227239  0.730774  1.711317  1.217806   \n",
       "5916       19       18  463.742426  2.227239  0.730774 -1.786886  1.550778   \n",
       "5917       19       19  774.007775  2.227239  0.730774  1.376041  1.225591   \n",
       "5918       19       20  956.670243  2.227239  0.730774  3.261380  0.812874   \n",
       "5919       19       21  856.190655  2.227239  0.730774  1.260633  2.328331   \n",
       "5920       19       22  501.052674  2.227239  0.730774  0.362323  1.971156   \n",
       "5921       19       24  635.457239  2.227239  0.730774  1.352316 -0.039424   \n",
       "5922       19       25  805.812603  2.227239  0.730774  0.705435  0.916320   \n",
       "5923       19       26  128.189917  2.227239  0.730774  0.520646  2.784435   \n",
       "5924       19       27  700.460909  2.227239  0.730774  3.712177  2.940265   \n",
       "5925       19       28  604.353170  2.227239  0.730774  0.881103  0.882971   \n",
       "5926       19       29   13.778164  2.227239  0.730774  1.805581  0.371109   \n",
       "\n",
       "            v5  \n",
       "0     1.412632  \n",
       "1     2.167880  \n",
       "2     0.140997  \n",
       "3     1.389516  \n",
       "4     0.524762  \n",
       "5     0.881132  \n",
       "6     3.143563  \n",
       "7     1.275741  \n",
       "8     1.040711  \n",
       "9     1.296468  \n",
       "10    1.921442  \n",
       "11    1.974233  \n",
       "12   -0.200903  \n",
       "13    1.312079  \n",
       "14    1.507455  \n",
       "15    1.654379  \n",
       "16    3.110217  \n",
       "17    1.038178  \n",
       "18    1.730284  \n",
       "19   -0.535410  \n",
       "20    0.296267  \n",
       "21    2.016604  \n",
       "22    1.114079  \n",
       "23    1.753426  \n",
       "24    2.368985  \n",
       "25    1.093568  \n",
       "26    0.020155  \n",
       "27    1.348737  \n",
       "28    1.132085  \n",
       "29    0.762880  \n",
       "...        ...  \n",
       "5897  1.992845  \n",
       "5898  0.171471  \n",
       "5899  3.348205  \n",
       "5900  0.102910  \n",
       "5901  0.752498  \n",
       "5902  1.359929  \n",
       "5903  1.494489  \n",
       "5904 -0.088030  \n",
       "5905  1.748767  \n",
       "5906 -0.445365  \n",
       "5907 -0.790891  \n",
       "5908  2.148185  \n",
       "5909  0.851215  \n",
       "5910  1.476287  \n",
       "5911  0.006224  \n",
       "5912  2.492427  \n",
       "5913  0.079537  \n",
       "5914  1.765428  \n",
       "5915  3.020111  \n",
       "5916  1.113513  \n",
       "5917  1.440668  \n",
       "5918  2.184920  \n",
       "5919  0.531175  \n",
       "5920  0.167420  \n",
       "5921  2.251390  \n",
       "5922  0.199287  \n",
       "5923  1.787093  \n",
       "5924  1.984631  \n",
       "5925  1.798022  \n",
       "5926  2.591765  \n",
       "\n",
       "[5927 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_items_df = []\n",
    "for i in range(df.shape[0]):\n",
    "    users_items_df += df['state_history'][i] \n",
    "    \n",
    "users_items_df = pd.DataFrame(users_items_df,columns = ['user_id','item_id',\n",
    "                                                        'price','v1','v2','v3','v4','v5'])\n",
    "users_items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate the 'v5'\n",
    "new_users_items_df = users_items_df.drop(columns='v5').drop_duplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the first two covariates variables are features about users \n",
    "# and the forth and fifth variable are features about items\n",
    "# The variable 5 depends on .....\n",
    "\n",
    "user_v1 = users_items_df['v1'].unique()\n",
    "user_v2 = users_items_df['v2'].unique()\n",
    "users_ids = users_items_df['user_id'].unique()\n",
    "users_content= pd.DataFrame({'user_id' : users_ids,'v1':user_v1,'v2':user_v2 })\n",
    "\n",
    "item_v3 = users_items_df['v3'].unique()\n",
    "item_v4 = users_items_df['v4'].unique()\n",
    "item_id = users_items_df['item_id'].unique()\n",
    "items_content= pd.DataFrame({'item_id' : item_id,'v3':item_v3,'v4':item_v4 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't have all the metadat from all the users\n",
    "users_content['user_id'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df[df[\"rewards_history\"] > 0].reset_index(drop=True)\n",
    "neg_df = df[df[\"rewards_history\"] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>89.071751</td>\n",
       "      <td>[[32, 0, 669.3972072487512, 1.5760900723032845...</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>32.166424</td>\n",
       "      <td>[[29, 0, 669.3972072487512, 1.3201441218369938...</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>840.778594</td>\n",
       "      <td>[[50, 0, 669.3972072487512, -0.182345260281261...</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>774.007775</td>\n",
       "      <td>[[23, 0, 669.3972072487512, 0.6399321827728754...</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>774.007775</td>\n",
       "      <td>[[4, 0, 669.3972072487512, -0.2547050722492416...</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0               4        89.071751   \n",
       "1              23        32.166424   \n",
       "2              12       840.778594   \n",
       "3              19       774.007775   \n",
       "4              19       774.007775   \n",
       "\n",
       "                                       state_history  user_id  item_id_action  \n",
       "0  [[32, 0, 669.3972072487512, 1.5760900723032845...       32               4  \n",
       "1  [[29, 0, 669.3972072487512, 1.3201441218369938...       29              23  \n",
       "2  [[50, 0, 669.3972072487512, -0.182345260281261...       50              12  \n",
       "3  [[23, 0, 669.3972072487512, 0.6399321827728754...       23              19  \n",
       "4  [[4, 0, 669.3972072487512, -0.2547050722492416...        4              19  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df[\"user_id\"] = [pos_df[\"state_history\"][i][0][0] for i in range(pos_df.shape[0])]\n",
    "pos_df[\"item_id_action\"] = [pos_df[\"state_history\"][i][pos_df[\"action_history\"][i]][1] for i in range(pos_df.shape[0])]\n",
    "pos_df.head()## There are some users that have more than one positive item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_history</th>\n",
       "      <th>rewards_history</th>\n",
       "      <th>state_history</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[78, 0, 669.3972072487512, 0.1267472727154109...</td>\n",
       "      <td>78</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[16, 0, 669.3972072487512, 1.1395888598214905...</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[32, 0, 669.3972072487512, 1.5760900723032845...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[7, 0, 669.3972072487512, 0.9263053626545149,...</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[92, 0, 669.3972072487512, -0.567211841153722...</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_history  rewards_history  \\\n",
       "0              22              0.0   \n",
       "1              29              0.0   \n",
       "2               2              0.0   \n",
       "3              26              0.0   \n",
       "4               2              0.0   \n",
       "\n",
       "                                       state_history  user_id  item_id_action  \n",
       "0  [[78, 0, 669.3972072487512, 0.1267472727154109...       78              22  \n",
       "1  [[16, 0, 669.3972072487512, 1.1395888598214905...       16              29  \n",
       "2  [[32, 0, 669.3972072487512, 1.5760900723032845...       32               2  \n",
       "3  [[7, 0, 669.3972072487512, 0.9263053626545149,...        7              26  \n",
       "4  [[92, 0, 669.3972072487512, -0.567211841153722...       92               2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df[\"user_id\"] = [neg_df[\"state_history\"][i][0][0] for i in range(neg_df.shape[0])]\n",
    "neg_df[\"item_id_action\"] = [neg_df[\"state_history\"][i][neg_df[\"action_history\"][i]][1] for i in range(neg_df.shape[0])]\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"Ignore y_true and return the mean of y_pred\n",
    "    \n",
    "    This is a hack to work-around the design of the Keras API that is\n",
    "    not really suited to train networks with a triplet loss by default.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(y_pred + 0 * y_true)\n",
    "\n",
    "\n",
    "def margin_comparator_loss(inputs, margin=1.):\n",
    "    \"\"\"Comparator loss for a pair of precomputed similarities\n",
    "    \n",
    "    If the inputs are cosine similarities, they each have range in\n",
    "    (-1, 1), therefore their difference have range in (-2, 2). Using\n",
    "    a margin of 1. can therefore make sense.\n",
    "\n",
    "    If the input similarities are not normalized, it can be beneficial\n",
    "    to use larger values for the margin of the comparator loss.\n",
    "    \"\"\"\n",
    "    positive_pair_sim, negative_pair_sim = inputs\n",
    "    return tf.maximum(negative_pair_sim - positive_pair_sim + margin, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model(The most expensive item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model_expensive(state):\n",
    "    greater_index = 0\n",
    "    greater_value = 0\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if(state[i][2] > greater_value):\n",
    "            greater_value = state[i][2]\n",
    "            greater_index = i\n",
    "    return greater_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 89107.155700 and Total reward per run: 29702.385233\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 1000\n",
    "total_reward = 0\n",
    "params = {\"user_id\" : USER_ID}\n",
    "\n",
    "for k in range(number_run):\n",
    "    \n",
    "    for i in range(generations):\n",
    "\n",
    "        predicted_item = naive_model_expensive(next_state)\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        \n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model(The cheapest item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model_cheapest(state):\n",
    "    cheapest_index = 0\n",
    "    cheapest_value =  state[0][2]\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if(state[i][2] < cheapest_value):\n",
    "            cheapest_value = state[i][2]\n",
    "            cheapest_index = i\n",
    "            \n",
    "    return cheapest_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 143663.655489 and Total reward per run: 47887.885163\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "generations = 1000\n",
    "total_reward = 0\n",
    "params = {\"user_id\" : USER_ID}\n",
    "\n",
    "for k in range(number_run):\n",
    "    \n",
    "    for i in range(generations):\n",
    "\n",
    "        predicted_item = naive_model_cheapest(next_state)\n",
    "\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url = url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "        next_state = r['state']\n",
    "        \n",
    "        \n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (without covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, Dense\n",
    "from keras.layers import Lambda, Dot\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import dot, concatenate\n",
    "\n",
    "\n",
    "def build_models(n_users, n_items, latent_dim=64, l2_reg=0):\n",
    "    \"\"\"Build a triplet model and its companion similarity model\n",
    "    \n",
    "    The triplet model is used to train the weights of the companion\n",
    "    similarity model. The triplet model takes 1 user, 1 positive item\n",
    "    (relative to the selected user) and one negative item and is\n",
    "    trained with comparator loss.\n",
    "    \n",
    "    The similarity model takes one user and one item as input and return\n",
    "    compatibility score (aka the match score).\n",
    "    \"\"\"\n",
    "    # Common architectural components for the two models:\n",
    "    # - symbolic input placeholders\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "\n",
    "    # - embeddings\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(input_dim=n_users + 1, output_dim=latent_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "    \n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(input_dim=n_items + 1, output_dim=latent_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # - similarity computation between embeddings\n",
    "    positive_similarity = Dot(name=\"positive_similarity\",\n",
    "                              axes=1, normalize=True)(\n",
    "        [user_embedding, positive_item_embedding])\n",
    "    negative_similarity = Dot(name=\"negative_similarity\",\n",
    "                              axes=1, normalize=True)(\n",
    "        [user_embedding, negative_item_embedding])\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss,\n",
    "                          name='comparator_loss',\n",
    "                          output_shape=(1,))([positive_similarity, negative_similarity])\n",
    "\n",
    "    triplet_model = Model(inputs=[user_input,\n",
    "                                  positive_item_input,\n",
    "                                  negative_item_input],\n",
    "                          outputs=triplet_loss)\n",
    "    \n",
    "    # The match-score model, only use at inference to rank items for a given\n",
    "    # model: the model weights are shared with the triplet_model therefore\n",
    "    # we do not need to train it and therefore we do not need to plug a loss\n",
    "    # and an optimizer.\n",
    "    match_model = Model(inputs=[user_input, positive_item_input],\n",
    "                        outputs=positive_similarity)\n",
    "    \n",
    "    return triplet_model, match_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_df,max_items):\n",
    "    \"\"\"\n",
    "    Sample negatives at random\n",
    "    \n",
    "    \"\"\"\n",
    "    range_itens = np.arange(max_items)\n",
    "    user_ids = pos_df['user_id'].values\n",
    "    pos_item_ids = pos_df['item_id_action'].values\n",
    "    \n",
    "    \n",
    "    user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "    neg_item_ids = np.array([])\n",
    "    for i in user_ids:\n",
    "        if i in set(user_pos_items.index):\n",
    "            number = np.random.choice([item for item in range_itens \n",
    "                                       if not item in user_pos_items[user_pos_items.index == i]])\n",
    "        else:\n",
    "            number = np.random.choice(range_itens,1)\n",
    "        neg_item_ids = np.append(neg_item_ids,number)\n",
    "    \n",
    "  \n",
    "    return [user_ids, pos_item_ids, neg_item_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 1.0410\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0374\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8433\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1565\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1662\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0550\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8563\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2423\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2254\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7120\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1477\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6654\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7226\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8289\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9872\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7051\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8235\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9449\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9448\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7173\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8691\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1609\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9581\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5663\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8021\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5041\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5731\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0786\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8035\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5014\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0033\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9364\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6064\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5787\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3425\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0468\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8441\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9741\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1591\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1976\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8553\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0959\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7947\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2139\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0282\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2773\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6842e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8273\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3404\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7483\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6985\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2201\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4432\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4214\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9549\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2725\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6813\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2908\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3923\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9522\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4186\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7042\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4366\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3405\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0147\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7433\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1603\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6317\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8003\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8087\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0625\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9441\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8773\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0803\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0878\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8226\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9681\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0256\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9784\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8694\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8106\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8606\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2450\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0337\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8830\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3915\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8354\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1992\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0548\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0746\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8238\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9976\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3588\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4168\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3006\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5974\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4290\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3235\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1146\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8647\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2420\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2772\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5959\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5432\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9237\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2824\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3087\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7488\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6320\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3182\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4153\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7478\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0371\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9853\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0655\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9774\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1780\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9280\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5007\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6781\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4192\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1742\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2508\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7587\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4336\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0625\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8529\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4340\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2445\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8091\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5639\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2588\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2433\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6261\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5530\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3218\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6294\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0788\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5089\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4324\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4138\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4175\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0307\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3558\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6325\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1581\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7458\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3301\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0839\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8612\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9271\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7959\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9183\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5680\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7277\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4075\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0262\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7300\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6314\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8955\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3594\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9246\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6731\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6339\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4554\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5788\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8221\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7254\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7220\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3475\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3872\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1026\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5044\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3039\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4464\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3061\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3129\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7822\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2966\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8949\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0326\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6834\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0985\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4126\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7854\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7563\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0245\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6147\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7159\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1751\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4727\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0809\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0583\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7058\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4943\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8169\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1831\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1362\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1447\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2608\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1201\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7419\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1744\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3900\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6957\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7715\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4144\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0068\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7773\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5042\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1871\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7303\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5784\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1399\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9843\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1772\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1738\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4237\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7957\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6010\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1939\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6602\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3460\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5750\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7763\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3818\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0476\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3303\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7240e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4173\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4481\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0771\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3580\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8679\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7346e-06\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9285\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2929\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1490\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3861\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9322\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1192\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3798\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0797\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8712\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1087\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4786\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0192\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6352\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5879\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6217\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5553\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6733\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7608\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7229\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8739\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9759\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8257\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9571\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0559\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1716\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3624\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6848\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4547\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6175\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0621\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3177\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1107\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5316\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8321\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3518\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1125\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5996\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0630\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8953\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5205\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8948\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4897\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5135\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4571\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9621\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2157\n",
      "Epoch 1/1\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 1.0084\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1183\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8049\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9615\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0122\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7799\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3430\n"
     ]
    }
   ],
   "source": [
    "number_run = 3\n",
    "n_epochs = 1\n",
    "total_reward = 0\n",
    "generations = 1000\n",
    "max_items = 30\n",
    "\n",
    "\n",
    "for k in range(number_run):\n",
    "\n",
    "    triplet_model, match_model = build_models(nb_users, nb_items, latent_dim=16, l2_reg=1e-6)\n",
    "    # we plug the identity loss and the a fake target variable ignored by\n",
    "    # the model to be able to use the Keras API to train the triplet model\n",
    "    triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "    fake_y = np.ones_like(pos_df['user_id'])\n",
    "    params = {'user_id': USER_ID}\n",
    "\n",
    "    positive_data = pos_df.copy()\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        # Sample new negatives to build different triplets at each epoch\n",
    "        triplet_inputs = sample_triplets(positive_data, max_items=nb_items)\n",
    "\n",
    "        # Fit the model incrementally by doing a single pass over the\n",
    "        # sampled triplets.\n",
    "        triplet_model.fit(triplet_inputs, fake_y, shuffle=True, epochs=1)\n",
    "\n",
    "    for j in range(generations):\n",
    "\n",
    "\n",
    "        # The user that we need to recommend a product\n",
    "        new_user_id = next_state[0][0]\n",
    "        # Take all the items in the new state\n",
    "        items_ids = np.array([next_state[i][1]\n",
    "                              for i in range(len(next_state))])\n",
    "\n",
    "        # Create an array of the same size of 'items_ids' of all values equal to 'new_user_id'\n",
    "        repeated_user_id = np.empty_like(items_ids)\n",
    "        repeated_user_id.fill(new_user_id)\n",
    "\n",
    "        # predict the product using our model\n",
    "        predicted = match_model.predict([repeated_user_id, items_ids])\n",
    "        predicted_item = np.argmax(predicted)\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        # take the reward from the API\n",
    "        r = requests.get(url=url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "\n",
    "        # (Online learning)Check if the reward is greater than 0 , if yes train again \n",
    "        # the model the sample point\n",
    "        if(reward > 0):\n",
    "            range_itens = np.arange(max_items)\n",
    "            user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "            if new_user_id in set(user_pos_items.index):\n",
    "                neg_item_id = np.random.choice([item for item in range_itens \n",
    "                                           if not item in user_pos_items[user_pos_items.index == new_user_id]])\n",
    "            else:\n",
    "                neg_item_id = np.random.choice(range_itens,1)\n",
    "            \n",
    "           \n",
    "            \n",
    "            triplet_inputs = [[new_user_id], [predicted_item], [neg_item_id]]\n",
    "    \n",
    "            fake_y = np.ones_like([1])\n",
    "            triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size=64, epochs=1)\n",
    "    \n",
    "            \n",
    "        # Take the nex_state\n",
    "        next_state = r['state']\n",
    "\n",
    "\n",
    "print(\"Total reward: %f and Total reward per run: %f\" %(total_reward, total_reward/number_run,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(triplet_model, to_file='triplet_model.png')\n",
    "Image(filename='triplet_model.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2(with deep learning and covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Flatten, Input, Dense, Dropout\n",
    "from keras.layers import Concatenate, Lambda\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def make_interaction_mlp(input_dim, n_hidden=1, hidden_size=64,\n",
    "                         dropout=0, l2_reg=None):\n",
    "    \"\"\"\n",
    "    Build the shared multi layer perceptron\n",
    "\n",
    "    \"\"\"\n",
    "    mlp = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        # Plug the output unit directly: this is a simple\n",
    "        # linear regression model. Not dropout required.\n",
    "        mlp.add(Dense(1, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "    else:\n",
    "        mlp.add(Dense(hidden_size, input_dim=input_dim,\n",
    "                      activation='relu', kernel_regularizer=l2_reg))\n",
    "        mlp.add(Dropout(dropout))\n",
    "        for i in range(n_hidden - 1):\n",
    "            mlp.add(Dense(hidden_size, activation='relu',\n",
    "                          kernel_regularizer=l2_reg))\n",
    "            mlp.add(Dropout(dropout))\n",
    "        mlp.add(Dense(1, activation='relu', kernel_regularizer=l2_reg))\n",
    "    return mlp\n",
    "\n",
    "\n",
    "def build_models(n_users, n_items, user_dim=32, item_dim=64,\n",
    "                 n_hidden=1, hidden_size=64, dropout=0, l2_reg=0):\n",
    "    \"\"\"\n",
    "    Build models to train a deep triplet network\n",
    "\n",
    "    \"\"\"\n",
    "    user_input = Input((1,), name='user_input')\n",
    "    positive_item_input = Input((1,), name='positive_item_input')\n",
    "    negative_item_input = Input((1,), name='negative_item_input')\n",
    "    positive_meta_input = Input((2,), name='positive_meta_input')\n",
    "    negative_meta_input = Input((2,), name='negative_meta_input')\n",
    "\n",
    "    l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "    user_layer = Embedding(n_users + 1, user_dim, input_length=1,\n",
    "                           name='user_embedding', embeddings_regularizer=l2_reg)\n",
    "\n",
    "    # The following embedding parameters will be shared to encode both\n",
    "    # the positive and negative items.\n",
    "    item_layer = Embedding(n_items + 1, item_dim, input_length=1,\n",
    "                           name=\"item_embedding\", embeddings_regularizer=l2_reg)\n",
    "\n",
    "    user_embedding = Flatten()(user_layer(user_input))\n",
    "    positive_item_embedding = Flatten()(item_layer(positive_item_input))\n",
    "    negative_item_embedding = Flatten()(item_layer(negative_item_input))\n",
    "\n",
    "    # Similarity computation between embeddings using a MLP similarity\n",
    "    positive_embeddings_pair = Concatenate(name=\"positive_embeddings_pair\")(\n",
    "        [user_embedding, positive_item_embedding, positive_meta_input])\n",
    "    positive_embeddings_pair = Dropout(dropout)(positive_embeddings_pair)\n",
    "\n",
    "    negative_embeddings_pair = Concatenate(name=\"negative_embeddings_pair\")(\n",
    "        [user_embedding, negative_item_embedding, negative_meta_input])\n",
    "    negative_embeddings_pair = Dropout(dropout)(negative_embeddings_pair)\n",
    "\n",
    "    # Instanciate the shared similarity architecture\n",
    "    interaction_layers = make_interaction_mlp(\n",
    "        user_dim + item_dim + 2, n_hidden=n_hidden, hidden_size=hidden_size,\n",
    "        dropout=dropout, l2_reg=l2_reg)\n",
    "\n",
    "    positive_similarity = interaction_layers(positive_embeddings_pair)\n",
    "    negative_similarity = interaction_layers(negative_embeddings_pair)\n",
    "\n",
    "    # The triplet network model, only used for training\n",
    "    triplet_loss = Lambda(margin_comparator_loss, output_shape=(1,),\n",
    "                          name='comparator_loss')(\n",
    "        [positive_similarity, negative_similarity])\n",
    "\n",
    "    deep_triplet_model = Model(inputs=[user_input,\n",
    "                                       positive_item_input,\n",
    "                                       negative_item_input,\n",
    "                                       positive_meta_input,\n",
    "                                       negative_meta_input\n",
    "                                       ],\n",
    "                               outputs=[triplet_loss])\n",
    "\n",
    "    # The match-score model, only used at inference\n",
    "    deep_match_model = Model(inputs=[user_input, positive_item_input, positive_meta_input],\n",
    "                             outputs=[positive_similarity])\n",
    "\n",
    "    return deep_match_model, deep_triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_df, items_content, max_items):\n",
    "    \"\"\"\n",
    "    Sample the data and put in the format \n",
    "    [user_ids, pos_item_ids, neg_item_ids,pos_item_metadata,neg_item_metadata]\n",
    "\n",
    "    \"\"\"\n",
    "    range_itens = np.arange(max_items)\n",
    "    user_ids = pos_df['user_id'].values\n",
    "    \n",
    "\n",
    "    pos_item_ids = pos_df['item_id_action'].values\n",
    "    pos_item_metadata = items_content.loc[pos_item_ids][['v3', 'v4']].values\n",
    "\n",
    "    \n",
    "    user_pos_items = pos_df.groupby('user_id')['item_id_action'].apply(list)\n",
    "    neg_item_ids = np.array([])\n",
    "    for i in user_ids:\n",
    "        if i in set(user_pos_items.index):\n",
    "            number = np.random.choice([item for item in range_itens \n",
    "                                       if not item in user_pos_items[user_pos_items.index == i]])\n",
    "        else:\n",
    "            number = np.random.choice(range_itens, 1)\n",
    "        neg_item_ids = np.append(neg_item_ids, number)\n",
    "\n",
    "    neg_item_metadata = items_content.loc[neg_item_ids][['v3', 'v4']].values\n",
    "\n",
    "    return [user_ids, pos_item_ids, neg_item_ids, pos_item_metadata, neg_item_metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = dict(\n",
    "    user_dim=50,\n",
    "    item_dim=15,\n",
    "    n_hidden=1,\n",
    "    hidden_size=16,\n",
    "    dropout=0.5,\n",
    "    l2_reg=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_run = 3\n",
    "n_epochs = 1\n",
    "total_reward = 0\n",
    "generations = 1000\n",
    "max_items = 30 \n",
    "\n",
    "for k in range(number_run):\n",
    "\n",
    "    deep_match_model, deep_triplet_model = build_models(nb_users, nb_items,**hyper_parameters)\n",
    "    # we plug the identity loss and the a fake target variable ignored by\n",
    "    # the model to be able to use the Keras API to train the triplet model\n",
    "    deep_triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "    fake_y = np.ones_like(pos_df['user_id'])\n",
    "    params = {'user_id': USER_ID}\n",
    "    \n",
    "    positive_data = pos_df.copy()\n",
    "    \n",
    "    # Training the model \n",
    "    for i in range(n_epochs):\n",
    "        # Sample new negatives to build different triplets at each epoch\n",
    "        triplet_inputs = sample_triplets(positive_data,items_content, max_items=nb_items)\n",
    "\n",
    "        # Fit the model incrementally by doing a single pass over the\n",
    "        # sampled triplets.\n",
    "        deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size=64, epochs=1)\n",
    "\n",
    "\n",
    "    for j in range(generations):\n",
    "        \n",
    "        # Predicting\n",
    "        new_user_id = next_state[0][0]\n",
    "        items_ids = np.array([next_state[i][1] for i in range(len(next_state))])\n",
    "        repeated_user_id = np.empty_like(items_ids)\n",
    "        repeated_user_id.fill(new_user_id)\n",
    "\n",
    "        pos_metadata = items_content.loc[items_ids][['v3', 'v4']].values\n",
    "        predicted = deep_match_model.predict([repeated_user_id, items_ids, pos_metadata])\n",
    "        predicted_item = np.argmax(predicted)\n",
    "        params['recommended_item'] = predicted_item\n",
    "\n",
    "        r = requests.get(url=url_predict, params=params).json()\n",
    "        reward = r['reward']\n",
    "        total_reward += reward\n",
    "\n",
    "        # Online learning\n",
    "        if(reward > 0):\n",
    "            range_itens = np.arange(max_items)\n",
    "            user_pos_items = positive_data.groupby('user_id')['item_id_action'].apply(list)\n",
    "            if new_user_id in set(user_pos_items.index):\n",
    "                neg_item_id = np.random.choice([item for item in range_itens \n",
    "                                           if not item in user_pos_items[user_pos_items.index == new_user_id]])\n",
    "            else:\n",
    "                neg_item_id = np.random.choice(range_itens,1)\n",
    "            \n",
    "            pos_item_metadata = items_content.set_index('item_id').loc[predicted_item].values\n",
    "            neg_item_metadata = items_content.set_index('item_id').loc[neg_item_id].values\n",
    "            \n",
    "            triplet_inputs = [[new_user_id], [predicted_item], [neg_item_id], \n",
    "                              pos_item_metadata.reshape((1,2)), neg_item_metadata.reshape((1,2))]\n",
    "    \n",
    "            fake_y = np.ones_like([1])\n",
    "            deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                               batch_size = 8, epochs=1)\n",
    "\n",
    "        next_state = r['state']\n",
    "\n",
    "print(\"Total reward: %f and Total reward per run: %f\"%(total_reward, total_reward/number_run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(deep_triplet_model, to_file='deep_triplet_model.png')\n",
    "Image(filename='deep_triplet_model.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

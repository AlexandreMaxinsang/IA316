{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project on recommander systems : Second Environment\n",
    "\n",
    "#### Author : Raymond Kuoch and Alexandre Maxinsang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Concatenate, Dropout, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IP adress of each environnement and the login\n",
    "url_env = {\"first_env\":\"http://52.47.62.31\", \n",
    "           \"second_env\":\"http://35.180.254.42\", \n",
    "           \"third_env\":\"http://35.180.178.243\"}\n",
    "user_id = 'Y6EKWA0GK1D0VCTN0RT7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second environnement : Explicit feedback with covariates\n",
    "\n",
    "### 1) Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['item_history', 'nb_items', 'nb_users', 'next_item', 'next_user', 'next_variables', 'rating_history', 'user_history', 'variables_history'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_reset = url_env[\"second_env\"] + \"/reset\"\n",
    "url_predict = url_env[\"second_env\"] + \"/predict\"\n",
    "params = {\"user_id\" : user_id}\n",
    "\n",
    "def load_data(url_reset,params) :\n",
    "    r = requests.get(url=url_reset, params=params)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "def mean_square_error(true, predicted):\n",
    "    return (true - predicted) ** 2\n",
    "\n",
    "def mean_absolute_error(true, predicted):\n",
    "    return np.abs(true - predicted)\n",
    "\n",
    "data = load_data(url_reset,params)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>meta_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>[2.0229713310147543, 0.42495590999159794, 3.40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>260</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.8901757698061652, 0.28177368044240225, -0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.1902549853172952, -2.1600596464800983, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>152</td>\n",
       "      <td>2</td>\n",
       "      <td>[1.5171732006877425, 2.8021736438877367, 1.348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>136</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.054679361852992714, 2.1380702017779543, 0.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating                                          meta_data\n",
       "0        7       78       2  [2.0229713310147543, 0.42495590999159794, 3.40...\n",
       "1       27      260       4  [0.8901757698061652, 0.28177368044240225, -0.7...\n",
       "2       84      200       1  [-1.1902549853172952, -2.1600596464800983, -1....\n",
       "3       29      152       2  [1.5171732006877425, 2.8021736438877367, 1.348...\n",
       "4       45      136       5  [0.054679361852992714, 2.1380702017779543, 0.4..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_history = data[\"user_history\"]\n",
    "item_history = data[\"item_history\"]\n",
    "rating_history = data[\"rating_history\"]\n",
    "variables_history = data[\"variables_history\"]\n",
    "\n",
    "train_ratings = pd.DataFrame({\"user_id\": user_history, \"item_id\": item_history, \"rating\": rating_history, \"meta_data\" : variables_history})\n",
    "train_ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 users and 300 items.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of users and items\n",
    "print(\"There are %d users and %d items.\" % (data[\"nb_users\"], data[\"nb_items\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Model without using covariates\n",
    "\n",
    "#### 2.1) Constant agent\n",
    "\n",
    "Let's start with a baseline : constant agent, for instance an agent who predicts the value 3 for each pair (user_id, item_id).\n",
    "\n",
    "The performance metric will be MSE over 1000 steps on 3 independent run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE over 1000 samples on the run 0 : 1.7300 \n",
      "MSE over 1000 samples on the run 1 : 1.7600 \n",
      "MSE over 1000 samples on the run 2 : 2.0300 \n",
      "--------------------------------------------\n",
      "Mean of the MSE on 3 independant runs 1.8400\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "\n",
    "for i in range(0, 3) :\n",
    "\n",
    "    # Load the data\n",
    "    data = load_data(url_reset,params)\n",
    "    user_history = data[\"user_history\"]\n",
    "    item_history = data[\"item_history\"]\n",
    "    rating_history = data[\"rating_history\"]\n",
    "    train_ratings = pd.DataFrame({\"user_id\": user_history, \"item_id\": item_history, \"rating\": rating_history})\n",
    "\n",
    "    nb_users = data[\"nb_users\"]\n",
    "    nb_items = data[\"nb_items\"]\n",
    "    next_user = data[\"next_user\"]\n",
    "    next_item = data[\"next_item\"]\n",
    "\n",
    "    nb_samples = 1000\n",
    "\n",
    "    prediction = 3\n",
    "    params[\"predicted_score\"] = prediction\n",
    "\n",
    "    mse = 0\n",
    "    \n",
    "    for j in range(nb_samples) : \n",
    "        time.sleep(0.5)\n",
    "        next_data = requests.get(url=url_predict, params=params).json()\n",
    "        rating = next_data[\"rating\"]\n",
    "        mse += mean_square_error(rating, prediction)\n",
    "    \n",
    "    mean += mse/nb_samples\n",
    "    print(\"MSE over 1000 samples on the run %d : %.4f \" % (i, mse/nb_samples))\n",
    "    \n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Mean of the MSE on 3 independant runs %.4f\" % (mean/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2) Matrix factorisation approach with deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    First of all, let's use the best model from the lab 1 which a deep model with 2 FC layers and let's try to outperform the constant agent.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_system_deep_2(train_x, train_y, embedding_size, nb_users, nb_items) :    \n",
    "    \n",
    "    user_id_input = Input(shape=[1],name='user')\n",
    "    item_id_input = Input(shape=[1], name='item')\n",
    "    embedding_size = embedding_size\n",
    "    user_embedding = Embedding(output_dim=embedding_size, input_dim=nb_users + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "    item_embedding = Embedding(output_dim=embedding_size, input_dim=nb_items + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    item_vecs = Flatten()(item_embedding)\n",
    "    input_vecs = Concatenate()([user_vecs, item_vecs])\n",
    "    x = Dense(64, activation='relu')(input_vecs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[user_id_input, item_id_input], outputs=y)\n",
    "    model.compile(optimizer='adam', loss='MSE')\n",
    "    history = model.fit([train_ratings[\"user_id\"], train_ratings[\"item_id\"]], train_ratings[\"rating\"],\n",
    "                        batch_size=64, epochs=50, validation_split=0.1,\n",
    "                        shuffle=True)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size : 50\n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 0s 226us/step - loss: 8.8137 - val_loss: 8.1782\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 5.2869 - val_loss: 2.5112\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 1.5573 - val_loss: 1.3336\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.9100 - val_loss: 1.1479\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.7851 - val_loss: 1.1658\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.7364 - val_loss: 1.1778\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.7452 - val_loss: 1.1573\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.7352 - val_loss: 1.1469\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.7136 - val_loss: 1.1540\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.7228 - val_loss: 1.1599\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.6978 - val_loss: 1.2025\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.6759 - val_loss: 1.1582\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6846 - val_loss: 1.1607\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.6383 - val_loss: 1.1124\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6353 - val_loss: 1.1112\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6326 - val_loss: 1.0950\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6223 - val_loss: 1.0957\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.6103 - val_loss: 1.0867\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5952 - val_loss: 1.0990\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5957 - val_loss: 1.0875\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5626 - val_loss: 1.0573\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5540 - val_loss: 1.0526\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.5578 - val_loss: 1.0524\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5472 - val_loss: 1.0226\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.5249 - val_loss: 1.0534\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5258 - val_loss: 1.0208\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5379 - val_loss: 1.0120\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.5196 - val_loss: 1.0476\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 20us/step - loss: 0.5238 - val_loss: 1.0587\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.5057 - val_loss: 1.0647\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5120 - val_loss: 1.0399\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5032 - val_loss: 1.0405\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5004 - val_loss: 1.0401\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4899 - val_loss: 1.0722\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4732 - val_loss: 1.0746\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4697 - val_loss: 1.0583\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4643 - val_loss: 1.1023\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.4445 - val_loss: 1.0685\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.4519 - val_loss: 1.0874\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4312 - val_loss: 1.0609\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.4213 - val_loss: 1.0826\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.4282 - val_loss: 1.0494\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4227 - val_loss: 1.0628\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4133 - val_loss: 1.0686\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.3967 - val_loss: 1.0988\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4106 - val_loss: 1.0681\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.3893 - val_loss: 1.0454\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.3820 - val_loss: 1.0547\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.3773 - val_loss: 1.0414\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.3602 - val_loss: 1.0197\n",
      "MSE over 1000 samples on the run 0 : 0.9158 \n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 1s 320us/step - loss: 9.1487 - val_loss: 10.0982\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 4.8291 - val_loss: 2.4323\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 1.4085 - val_loss: 1.3213\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.8849 - val_loss: 0.9111\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.7251 - val_loss: 0.8149\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.6723 - val_loss: 0.8115\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.6699 - val_loss: 0.7957\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.6697 - val_loss: 0.8138\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.6567 - val_loss: 0.7872\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.6557 - val_loss: 0.8098\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.6270 - val_loss: 0.8006\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6207 - val_loss: 0.7407\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.6038 - val_loss: 0.7390\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6230 - val_loss: 0.7164\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.5859 - val_loss: 0.6882\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.5792 - val_loss: 0.7140\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5612 - val_loss: 0.7020\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5699 - val_loss: 0.7321\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.5573 - val_loss: 0.7469\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5589 - val_loss: 0.7640\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5288 - val_loss: 0.7135\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.5290 - val_loss: 0.7142\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.5356 - val_loss: 0.7085\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.5150 - val_loss: 0.7437\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5068 - val_loss: 0.7511\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.5051 - val_loss: 0.7283\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.4954 - val_loss: 0.7164\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4981 - val_loss: 0.7363\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.4868 - val_loss: 0.7542\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4811 - val_loss: 0.7695\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4608 - val_loss: 0.7146\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4649 - val_loss: 0.7184\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4557 - val_loss: 0.7637\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.4553 - val_loss: 0.7443\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4489 - val_loss: 0.7740\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.4339 - val_loss: 0.7440\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4120 - val_loss: 0.7561\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4035 - val_loss: 0.7490\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4030 - val_loss: 0.7473\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.3912 - val_loss: 0.7838\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.3988 - val_loss: 0.7551\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.3953 - val_loss: 0.7349\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.3723 - val_loss: 0.7567\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.3647 - val_loss: 0.7730\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.3558 - val_loss: 0.7825\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.3504 - val_loss: 0.7943\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.3507 - val_loss: 0.7984\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.3345 - val_loss: 0.7841\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.3253 - val_loss: 0.7708\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.3090 - val_loss: 0.7644\n",
      "MSE over 1000 samples on the run 1 : 0.8004 \n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 0s 271us/step - loss: 8.5272 - val_loss: 6.0125\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 3.5492 - val_loss: 1.5967\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 1.2169 - val_loss: 1.0295\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.7302 - val_loss: 0.9218\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.6321 - val_loss: 0.9080\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5718 - val_loss: 0.9563\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.5656 - val_loss: 0.9486\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5592 - val_loss: 0.9472\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.5604 - val_loss: 0.9821\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5432 - val_loss: 0.9633\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5241 - val_loss: 0.9418\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.5157 - val_loss: 0.9274\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4870 - val_loss: 0.9458\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4806 - val_loss: 0.9252\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4752 - val_loss: 0.9154\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.4651 - val_loss: 0.8908\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4812 - val_loss: 0.8778\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.4722 - val_loss: 0.9027\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4661 - val_loss: 0.8706\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4535 - val_loss: 0.8788\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4401 - val_loss: 0.9097\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4471 - val_loss: 0.9105\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4545 - val_loss: 0.9037\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4466 - val_loss: 0.9630\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4212 - val_loss: 0.9596\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4246 - val_loss: 0.9327\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4139 - val_loss: 0.9268\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4196 - val_loss: 0.9328\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4157 - val_loss: 0.8865\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4105 - val_loss: 0.9246\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.4140 - val_loss: 0.9318\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3988 - val_loss: 0.9024\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3806 - val_loss: 0.9499\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3881 - val_loss: 0.9252\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.3825 - val_loss: 0.9192\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3743 - val_loss: 0.9038\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3632 - val_loss: 0.9205\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3651 - val_loss: 0.9436\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.3593 - val_loss: 1.0001\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.3337 - val_loss: 0.9690\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.3538 - val_loss: 0.9390\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 20us/step - loss: 0.3302 - val_loss: 0.9457\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3287 - val_loss: 0.9248\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3330 - val_loss: 1.0129\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.3185 - val_loss: 0.9910\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3173 - val_loss: 0.9619\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3125 - val_loss: 0.9798\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.2955 - val_loss: 0.9786\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.2815 - val_loss: 1.0093\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.2820 - val_loss: 0.9697\n",
      "MSE over 1000 samples on the run 2 : 0.7923 \n",
      "Mean of the MSE on 3 independant runs 0.8362 \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "embedding_size = 50\n",
    "print(\"Embedding size : %d\" % (embedding_size))\n",
    "\n",
    "for i in range(0, 3) : \n",
    "\n",
    "    # Load the data\n",
    "    data = load_data(url_reset,params)\n",
    "    user_history = data[\"user_history\"]\n",
    "    item_history = data[\"item_history\"]\n",
    "    rating_history = data[\"rating_history\"]\n",
    "    train_ratings = pd.DataFrame({\"user_id\": user_history, \"item_id\": item_history, \"rating\": rating_history})\n",
    "\n",
    "    nb_users = data[\"nb_users\"]\n",
    "    nb_items = data[\"nb_items\"]\n",
    "    next_user = data[\"next_user\"]\n",
    "    next_item = data[\"next_item\"]\n",
    "\n",
    "    nb_samples = 1000\n",
    "    mse = 0\n",
    "\n",
    "    train_x = [train_ratings[\"user_id\"], train_ratings[\"item_id\"]]\n",
    "    train_y = train_ratings[\"rating\"]\n",
    "\n",
    "    model, history = recommender_system_deep_2(train_x, train_y, embedding_size, nb_users, nb_items)\n",
    "\n",
    "    for j in range(nb_samples) : \n",
    "        time.sleep(0.5)\n",
    "        prediction = model.predict([[next_user], [next_item]])[0][0]\n",
    "        params[\"predicted_score\"] = prediction\n",
    "        next_data = requests.get(url=url_predict, params=params).json()\n",
    "        rating = next_data[\"rating\"]\n",
    "        next_user = next_data[\"next_user\"]\n",
    "        next_item = next_data[\"next_item\"]\n",
    "        mse += mean_square_error(rating, prediction)\n",
    "\n",
    "    mean += mse/nb_samples\n",
    "    print(\"MSE over 1000 samples on the run %d : %.4f \" % (i, mse/nb_samples))\n",
    "\n",
    "print(\"Mean of the MSE on 3 independant runs %.4f \" % (mean/3))\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    On average, the MSE is about 0.84, compared to the baseline with a MSE of 1.8.\n",
    "    </font>\n",
    "    \n",
    "> <font color='blue'>\n",
    "    **We can think that since there is little training data, it may be more preferable to use a less deep method with fewer FC layers, like 1 FC layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_system_deep_1(train_x, train_y, embedding_size, nb_users, nb_items) :    \n",
    "    \n",
    "    user_id_input = Input(shape=[1],name='user')\n",
    "    item_id_input = Input(shape=[1], name='item')\n",
    "    embedding_size = embedding_size\n",
    "    user_embedding = Embedding(output_dim=embedding_size, input_dim=nb_users + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "    item_embedding = Embedding(output_dim=embedding_size, input_dim=nb_items + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    item_vecs = Flatten()(item_embedding)\n",
    "    input_vecs = Concatenate()([user_vecs, item_vecs])\n",
    "    x = Dense(64, activation='relu')(input_vecs)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[user_id_input, item_id_input], outputs=y)\n",
    "    model.compile(optimizer='adam', loss='MSE')\n",
    "    history = model.fit([train_ratings[\"user_id\"], train_ratings[\"item_id\"]], train_ratings[\"rating\"],\n",
    "                        batch_size=64, epochs=50, validation_split=0.1,\n",
    "                        shuffle=True)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size : 50\n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 1s 371us/step - loss: 9.8497 - val_loss: 8.0194\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 6.2673 - val_loss: 3.3526\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 1.9987 - val_loss: 1.4196\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.9553 - val_loss: 1.0161\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.6868 - val_loss: 0.8983\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6125 - val_loss: 0.8598\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5908 - val_loss: 0.8178\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.5839 - val_loss: 0.8071\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.5837 - val_loss: 0.8022\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5800 - val_loss: 0.7970\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.5739 - val_loss: 0.7860\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.5703 - val_loss: 0.7875\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5707 - val_loss: 0.8004\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.5624 - val_loss: 0.7602\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.5597 - val_loss: 0.7504\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5558 - val_loss: 0.7354\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5545 - val_loss: 0.7397\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.5448 - val_loss: 0.7433\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.5399 - val_loss: 0.7300\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.5326 - val_loss: 0.7492\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.5236 - val_loss: 0.7218\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.5149 - val_loss: 0.7196\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.5093 - val_loss: 0.7200\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4962 - val_loss: 0.7029\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.4862 - val_loss: 0.7046\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.4719 - val_loss: 0.6867\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.4629 - val_loss: 0.6881\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4579 - val_loss: 0.7240\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.4524 - val_loss: 0.7104\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4440 - val_loss: 0.7176\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4350 - val_loss: 0.7237\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4297 - val_loss: 0.7412\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4204 - val_loss: 0.7225\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4123 - val_loss: 0.7155\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4075 - val_loss: 0.7113\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4038 - val_loss: 0.7496\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.3946 - val_loss: 0.7366\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.3902 - val_loss: 0.7636\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4010 - val_loss: 0.7351\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.3928 - val_loss: 0.7470\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3810 - val_loss: 0.7554\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.3808 - val_loss: 0.7431\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.3724 - val_loss: 0.7484\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3694 - val_loss: 0.7557\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.3679 - val_loss: 0.7680\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.3656 - val_loss: 0.7611\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3612 - val_loss: 0.7832\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3559 - val_loss: 0.7666\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3549 - val_loss: 0.7969\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3553 - val_loss: 0.8079\n",
      "MSE over 1000 samples on the run 0 : 0.8071 \n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 1s 384us/step - loss: 9.0321 - val_loss: 7.7564\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 18us/step - loss: 5.7623 - val_loss: 3.2399\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 18us/step - loss: 1.8666 - val_loss: 1.1744\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.8932 - val_loss: 0.8240\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.6600 - val_loss: 0.7887\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.5965 - val_loss: 0.7901\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.5714 - val_loss: 0.7940\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 18us/step - loss: 0.5633 - val_loss: 0.7947\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 18us/step - loss: 0.5568 - val_loss: 0.7980\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.5576 - val_loss: 0.8034\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.5514 - val_loss: 0.8060\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.5509 - val_loss: 0.7861\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.5456 - val_loss: 0.8038\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.5426 - val_loss: 0.8008\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 18us/step - loss: 0.5344 - val_loss: 0.8180\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.5353 - val_loss: 0.8258\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.5312 - val_loss: 0.7786\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.5180 - val_loss: 0.7566\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.5113 - val_loss: 0.7707\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5106 - val_loss: 0.7546\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5017 - val_loss: 0.7543\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4909 - val_loss: 0.7549\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.4801 - val_loss: 0.7494\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4674 - val_loss: 0.7673\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.4563 - val_loss: 0.7535\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.4470 - val_loss: 0.7326\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.4403 - val_loss: 0.7174\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.4269 - val_loss: 0.7361\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4210 - val_loss: 0.7638\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 19us/step - loss: 0.4179 - val_loss: 0.7515\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 19us/step - loss: 0.4074 - val_loss: 0.7454\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4055 - val_loss: 0.7351\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 18us/step - loss: 0.3922 - val_loss: 0.7289\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3924 - val_loss: 0.7325\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.3815 - val_loss: 0.7429\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.3766 - val_loss: 0.7448\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.3729 - val_loss: 0.7780\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 19us/step - loss: 0.3684 - val_loss: 0.7622\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.3634 - val_loss: 0.7784\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 18us/step - loss: 0.3582 - val_loss: 0.7819\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.3546 - val_loss: 0.7657\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.3553 - val_loss: 0.7699\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.3514 - val_loss: 0.7770\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.3446 - val_loss: 0.7776\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.3414 - val_loss: 0.7938\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.3391 - val_loss: 0.7913\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 19us/step - loss: 0.3362 - val_loss: 0.7685\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 17us/step - loss: 0.3311 - val_loss: 0.7817\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 18us/step - loss: 0.3260 - val_loss: 0.7759\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 18us/step - loss: 0.3224 - val_loss: 0.7923\n",
      "MSE over 1000 samples on the run 1 : 0.6054 \n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 1s 458us/step - loss: 8.6733 - val_loss: 7.7644\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 5.1777 - val_loss: 3.0116\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 1.5407 - val_loss: 1.1101\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.7816 - val_loss: 0.8016\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 20us/step - loss: 0.5709 - val_loss: 0.7276\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5241 - val_loss: 0.7498\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.5050 - val_loss: 0.7521\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5027 - val_loss: 0.7709\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.5004 - val_loss: 0.7589\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.4877 - val_loss: 0.7906\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4865 - val_loss: 0.7964\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.4816 - val_loss: 0.7768\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4830 - val_loss: 0.7613\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4805 - val_loss: 0.7813\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.4745 - val_loss: 0.7530\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4668 - val_loss: 0.7509\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 20us/step - loss: 0.4596 - val_loss: 0.7300\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.4537 - val_loss: 0.7306\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4400 - val_loss: 0.7279\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.4337 - val_loss: 0.7159\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4199 - val_loss: 0.7065\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.4104 - val_loss: 0.6999\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.4011 - val_loss: 0.6970\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.3940 - val_loss: 0.6917\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.3845 - val_loss: 0.6817\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.3772 - val_loss: 0.6742\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.3659 - val_loss: 0.6758\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.3589 - val_loss: 0.6960\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.3561 - val_loss: 0.6825\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.3516 - val_loss: 0.6860\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.3419 - val_loss: 0.7006\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.3420 - val_loss: 0.6881\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.3378 - val_loss: 0.6865\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.3304 - val_loss: 0.6898\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.3255 - val_loss: 0.6862\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.3235 - val_loss: 0.7020\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.3168 - val_loss: 0.6979\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 46us/step - loss: 0.3178 - val_loss: 0.7054\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.3148 - val_loss: 0.7136\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.3095 - val_loss: 0.7002\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.3052 - val_loss: 0.6957\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.3048 - val_loss: 0.7053\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.2994 - val_loss: 0.6966\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.2959 - val_loss: 0.7055\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.2978 - val_loss: 0.7056\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.2894 - val_loss: 0.7100\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.2940 - val_loss: 0.7008\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.2948 - val_loss: 0.7141\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.2843 - val_loss: 0.6858\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.2819 - val_loss: 0.7041\n",
      "MSE over 1000 samples on the run 2 : 0.6986 \n",
      "Mean of the MSE on 3 independant runs 0.7037 \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "embedding_size = 50\n",
    "print(\"Embedding size : %d\" % (embedding_size))\n",
    "\n",
    "for i in range(0, 3) : \n",
    "\n",
    "    # Load the data\n",
    "    data = load_data(url_reset,params)\n",
    "    user_history = data[\"user_history\"]\n",
    "    item_history = data[\"item_history\"]\n",
    "    rating_history = data[\"rating_history\"]\n",
    "    train_ratings = pd.DataFrame({\"user_id\": user_history, \"item_id\": item_history, \"rating\": rating_history})\n",
    "\n",
    "    nb_users = data[\"nb_users\"]\n",
    "    nb_items = data[\"nb_items\"]\n",
    "    next_user = data[\"next_user\"]\n",
    "    next_item = data[\"next_item\"]\n",
    "\n",
    "    nb_samples = 1000\n",
    "    mse = 0\n",
    "\n",
    "    train_x = [train_ratings[\"user_id\"], train_ratings[\"item_id\"]]\n",
    "    train_y = train_ratings[\"rating\"]\n",
    "\n",
    "    model, history = recommender_system_deep_1(train_x, train_y, embedding_size, nb_users, nb_items)\n",
    "\n",
    "    for j in range(nb_samples) : \n",
    "        time.sleep(0.5)\n",
    "        prediction = model.predict([[next_user], [next_item]])[0][0]\n",
    "        params[\"predicted_score\"] = prediction\n",
    "        next_data = requests.get(url=url_predict, params=params).json()\n",
    "        rating = next_data[\"rating\"]\n",
    "        next_user = next_data[\"next_user\"]\n",
    "        next_item = next_data[\"next_item\"]\n",
    "        mse += mean_square_error(rating, prediction)\n",
    "\n",
    "    mean += mse/nb_samples\n",
    "    print(\"MSE over 1000 samples on the run %d : %.4f \" % (i, mse/nb_samples))\n",
    "\n",
    "print(\"Mean of the MSE on 3 independant runs %.4f \" % (mean/3))\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    **Thus, by using fewer parameters (because fewer FC layers), the model is much more efficient! Because we have very little data (only 2000 lines of data)**\n",
    "    </font>\n",
    "\n",
    "> <font color='blue'>\n",
    "    Let's try to add some L2 regularization to the model and let's see if it is improving MSE.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_system_deep_1(train_x, train_y, embedding_size, nb_users, nb_items) :    \n",
    "    \n",
    "    user_id_input = Input(shape=[1],name='user')\n",
    "    item_id_input = Input(shape=[1], name='item')\n",
    "    embedding_size = embedding_size\n",
    "    user_embedding = Embedding(output_dim=embedding_size, input_dim=nb_users + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "    item_embedding = Embedding(output_dim=embedding_size, input_dim=nb_items + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    item_vecs = Flatten()(item_embedding)\n",
    "    input_vecs = Concatenate()([user_vecs, item_vecs])\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(input_vecs)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[user_id_input, item_id_input], outputs=y)\n",
    "    model.compile(optimizer='adam', loss='MSE')\n",
    "    history = model.fit([train_ratings[\"user_id\"], train_ratings[\"item_id\"]], train_ratings[\"rating\"],\n",
    "                        batch_size=64, epochs=50, validation_split=0.1,\n",
    "                        shuffle=True)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size : 50\n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 1s 601us/step - loss: 10.7206 - val_loss: 8.3998\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 7.1847 - val_loss: 4.1083\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 2.7773 - val_loss: 2.1188\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 1.5482 - val_loss: 1.6481\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 1.2053 - val_loss: 1.4596\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 1.0542 - val_loss: 1.3401\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.9656 - val_loss: 1.2837\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.9116 - val_loss: 1.2421\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.8590 - val_loss: 1.2018\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.8220 - val_loss: 1.1845\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.7888 - val_loss: 1.1500\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.7624 - val_loss: 1.1118\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.7375 - val_loss: 1.0874\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.7170 - val_loss: 1.0672\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.6998 - val_loss: 1.0587\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.6834 - val_loss: 1.0465\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.6707 - val_loss: 1.0653\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.6688 - val_loss: 1.0306\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.6514 - val_loss: 1.0239\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.6437 - val_loss: 1.0017\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.6325 - val_loss: 0.9852\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.6217 - val_loss: 0.9974\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.6193 - val_loss: 0.9790\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.6082 - val_loss: 0.9616\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 55us/step - loss: 0.6032 - val_loss: 0.9526\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.5949 - val_loss: 0.9463\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 52us/step - loss: 0.5900 - val_loss: 0.9685\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.5884 - val_loss: 0.9357\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5790 - val_loss: 0.9312\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.5774 - val_loss: 0.9389\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.5720 - val_loss: 0.9285\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.5657 - val_loss: 0.9185\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.5613 - val_loss: 0.9117\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.5560 - val_loss: 0.9076\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5481 - val_loss: 0.9024\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.5519 - val_loss: 0.8912\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.5407 - val_loss: 0.9189\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5356 - val_loss: 0.9134\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 20us/step - loss: 0.5275 - val_loss: 0.8877\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5272 - val_loss: 0.8856\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.5172 - val_loss: 0.8908\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5108 - val_loss: 0.8895\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 20us/step - loss: 0.5050 - val_loss: 0.8826\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5018 - val_loss: 0.8755\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.4943 - val_loss: 0.8835\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.4860 - val_loss: 0.8497\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.4837 - val_loss: 0.8602\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.4764 - val_loss: 0.8595\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 47us/step - loss: 0.4744 - val_loss: 0.8797\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4666 - val_loss: 0.8985\n",
      "MSE over 1000 samples on the run 0 : 0.7289 \n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 1s 530us/step - loss: 11.1699 - val_loss: 9.3735\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 7.6498 - val_loss: 4.4961\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 2.9264 - val_loss: 1.7895\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 1.5624 - val_loss: 1.3711\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 43us/step - loss: 1.2255 - val_loss: 1.2259\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 1.0871 - val_loss: 1.1488\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 1.0022 - val_loss: 1.0905\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.9425 - val_loss: 1.0483\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.8925 - val_loss: 1.0038\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.8648 - val_loss: 0.9854\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.8306 - val_loss: 0.9515\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.8021 - val_loss: 0.9260\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.7803 - val_loss: 0.9126\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.7634 - val_loss: 0.8865\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.7464 - val_loss: 0.8749\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 49us/step - loss: 0.7333 - val_loss: 0.8774\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.7202 - val_loss: 0.8340\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.7071 - val_loss: 0.8388\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.6986 - val_loss: 0.8224\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.6913 - val_loss: 0.8164\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.6826 - val_loss: 0.8234\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.6752 - val_loss: 0.8273\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.6668 - val_loss: 0.8335\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.6636 - val_loss: 0.8218\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.6586 - val_loss: 0.8237\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.6564 - val_loss: 0.8168\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.6503 - val_loss: 0.8115\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.6472 - val_loss: 0.7996\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.6439 - val_loss: 0.7992\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.6435 - val_loss: 0.8063\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.6330 - val_loss: 0.8106\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.6293 - val_loss: 0.7928\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6270 - val_loss: 0.7873\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.6266 - val_loss: 0.7857\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6216 - val_loss: 0.7768\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.6199 - val_loss: 0.7780\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6156 - val_loss: 0.7669\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.6154 - val_loss: 0.7674\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.6108 - val_loss: 0.7591\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.6100 - val_loss: 0.7688\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.6077 - val_loss: 0.7619\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.6093 - val_loss: 0.7688\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.6055 - val_loss: 0.7497\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.6013 - val_loss: 0.7530\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.6010 - val_loss: 0.7418\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.6057 - val_loss: 0.7295\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.6035 - val_loss: 0.7427\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6011 - val_loss: 0.7591\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.5957 - val_loss: 0.7428\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.5951 - val_loss: 0.7476\n",
      "MSE over 1000 samples on the run 1 : 1.1412 \n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 1s 534us/step - loss: 9.3444 - val_loss: 7.5804\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 6.5554 - val_loss: 4.0158\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 2.8611 - val_loss: 1.8603\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 1.5233 - val_loss: 1.4228\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 1.1335 - val_loss: 1.2685\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.9692 - val_loss: 1.1941\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.8841 - val_loss: 1.1561\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.8227 - val_loss: 1.1178\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.7763 - val_loss: 1.0818\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.7442 - val_loss: 1.0662\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.7157 - val_loss: 1.0552\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.6902 - val_loss: 1.0316\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.6716 - val_loss: 1.0044\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.6558 - val_loss: 0.9887\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.6354 - val_loss: 1.0086\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.6265 - val_loss: 0.9712\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.6118 - val_loss: 0.9663\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.6004 - val_loss: 0.9616\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5945 - val_loss: 0.9443\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.5838 - val_loss: 0.9494\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5744 - val_loss: 0.9376\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.5665 - val_loss: 0.9560\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.5615 - val_loss: 0.9468\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5578 - val_loss: 0.9499\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.5485 - val_loss: 0.9322\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.5420 - val_loss: 0.8844\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.5398 - val_loss: 0.8949\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5356 - val_loss: 0.9059\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5287 - val_loss: 0.9085\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.5255 - val_loss: 0.9094\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.5189 - val_loss: 0.9051\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5169 - val_loss: 0.8952\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5106 - val_loss: 0.8971\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.5061 - val_loss: 0.8846\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.5034 - val_loss: 0.8989\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4990 - val_loss: 0.8859\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.4934 - val_loss: 0.8985\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4931 - val_loss: 0.8785\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.4884 - val_loss: 0.8634\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4886 - val_loss: 0.8530\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.4867 - val_loss: 0.8984\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.4778 - val_loss: 0.8900\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.4741 - val_loss: 0.8813\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4708 - val_loss: 0.8696\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.4699 - val_loss: 0.8281\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.4723 - val_loss: 0.8966\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.4661 - val_loss: 0.8233\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.4641 - val_loss: 0.8389\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.4581 - val_loss: 0.8348\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.4587 - val_loss: 0.8236\n",
      "MSE over 1000 samples on the run 2 : 0.9085 \n",
      "Mean of the MSE on 3 independant runs 0.9262 \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "embedding_size = 50\n",
    "print(\"Embedding size : %d\" % (embedding_size))\n",
    "\n",
    "for i in range(0, 3) : \n",
    "\n",
    "    # Load the data\n",
    "    data = load_data(url_reset,params)\n",
    "    user_history = data[\"user_history\"]\n",
    "    item_history = data[\"item_history\"]\n",
    "    rating_history = data[\"rating_history\"]\n",
    "    train_ratings = pd.DataFrame({\"user_id\": user_history, \"item_id\": item_history, \"rating\": rating_history})\n",
    "\n",
    "    nb_users = data[\"nb_users\"]\n",
    "    nb_items = data[\"nb_items\"]\n",
    "    next_user = data[\"next_user\"]\n",
    "    next_item = data[\"next_item\"]\n",
    "\n",
    "    nb_samples = 1000\n",
    "    mse = 0\n",
    "\n",
    "    train_x = [train_ratings[\"user_id\"], train_ratings[\"item_id\"]]\n",
    "    train_y = train_ratings[\"rating\"]\n",
    "\n",
    "    model, history = recommender_system_deep_1(train_x, train_y, embedding_size, nb_users, nb_items)\n",
    "\n",
    "    for j in range(nb_samples) : \n",
    "        time.sleep(0.5)\n",
    "        prediction = model.predict([[next_user], [next_item]])[0][0]\n",
    "        params[\"predicted_score\"] = prediction\n",
    "        next_data = requests.get(url=url_predict, params=params).json()\n",
    "        rating = next_data[\"rating\"]\n",
    "        next_user = next_data[\"next_user\"]\n",
    "        next_item = next_data[\"next_item\"]\n",
    "        mse += mean_square_error(rating, prediction)\n",
    "\n",
    "    mean += mse/nb_samples\n",
    "    print(\"MSE over 1000 samples on the run %d : %.4f \" % (i, mse/nb_samples))\n",
    "\n",
    "print(\"Mean of the MSE on 3 independant runs %.4f \" % (mean/3))\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    Thus, by adding regularization, it degrades performance because the model is not overfitting. Regularization is not effective here.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Model with using covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_system_deep_covariates(user, item, variable_history, rating, embedding_size, nb_users, nb_items) :    \n",
    "    \n",
    "    user_id_input = Input(shape=[1],name='user')\n",
    "    item_id_input = Input(shape=[1], name='item')\n",
    "    meta_input = Input(shape=[5], name='meta_item')\n",
    "    embedding_size = embedding_size\n",
    "    user_embedding = Embedding(output_dim=embedding_size, input_dim=nb_users + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "    item_embedding = Embedding(output_dim=embedding_size, input_dim=nb_items + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    item_vecs = Flatten()(item_embedding)\n",
    "    input_vecs = Concatenate()([user_vecs, item_vecs, meta_input])\n",
    "    x = Dense(64, activation='relu')(input_vecs)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[user_id_input, item_id_input, meta_input], outputs=y)\n",
    "    model.compile(optimizer='adam', loss='MSE')\n",
    "    history = model.fit([user, item, variable_history], rating,\n",
    "                        batch_size=64, epochs=50, validation_split=0.1,\n",
    "                        shuffle=True)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size : 50\n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 1s 621us/step - loss: 9.5742 - val_loss: 6.8508\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 5.0495 - val_loss: 3.2066\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 2.4348 - val_loss: 2.0816\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 1.5264 - val_loss: 1.5121\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.9475 - val_loss: 1.2050\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.6937 - val_loss: 1.0443\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.5851 - val_loss: 0.9812\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 0.5321 - val_loss: 0.9421\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.5058 - val_loss: 0.9166\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.4868 - val_loss: 0.9244\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4702 - val_loss: 0.9028\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4641 - val_loss: 0.8974\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4544 - val_loss: 0.9067\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4487 - val_loss: 0.9071\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.4405 - val_loss: 0.9030\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.4375 - val_loss: 0.9046\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4280 - val_loss: 0.8798\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.4198 - val_loss: 0.8959\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4121 - val_loss: 0.9007\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4109 - val_loss: 0.8798\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.3918 - val_loss: 0.8704\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.3818 - val_loss: 0.8519\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.3717 - val_loss: 0.8463\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.3669 - val_loss: 0.8405\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.3521 - val_loss: 0.8381\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.3478 - val_loss: 0.8284\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.3390 - val_loss: 0.8043\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.3328 - val_loss: 0.8080\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 51us/step - loss: 0.3258 - val_loss: 0.8182\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.3168 - val_loss: 0.8061\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.3081 - val_loss: 0.8053\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.3034 - val_loss: 0.7944\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.2949 - val_loss: 0.7961\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.2893 - val_loss: 0.8025\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.2856 - val_loss: 0.8044\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.2755 - val_loss: 0.8000\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.2658 - val_loss: 0.8231\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.2627 - val_loss: 0.8169\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.2546 - val_loss: 0.8287\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.2461 - val_loss: 0.8205\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.2399 - val_loss: 0.8395\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.2312 - val_loss: 0.8623\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.2232 - val_loss: 0.8326\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.2168 - val_loss: 0.8317\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.2086 - val_loss: 0.8431\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.2020 - val_loss: 0.8383\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.1937 - val_loss: 0.8507\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.1881 - val_loss: 0.8552\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.1823 - val_loss: 0.8471\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.1754 - val_loss: 0.8600\n",
      "MSE over 1000 samples on the run 0 : 0.9378 \n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 1s 669us/step - loss: 8.0701 - val_loss: 4.8108\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 3.5771 - val_loss: 2.0431\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 1.9296 - val_loss: 1.4588\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 1.2064 - val_loss: 1.1145\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.8177 - val_loss: 1.0006\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.6760 - val_loss: 0.9676\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.6083 - val_loss: 0.9001\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5730 - val_loss: 0.8925\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.5434 - val_loss: 0.8397\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.5246 - val_loss: 0.8336\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.5116 - val_loss: 0.8285\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.5055 - val_loss: 0.8211\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4891 - val_loss: 0.8500\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4849 - val_loss: 0.8186\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4810 - val_loss: 0.8221\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.4727 - val_loss: 0.8135\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4633 - val_loss: 0.8182\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4609 - val_loss: 0.8043\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4606 - val_loss: 0.8043\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4512 - val_loss: 0.8045\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4425 - val_loss: 0.8208\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.4379 - val_loss: 0.8159\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4368 - val_loss: 0.7893\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.4286 - val_loss: 0.8135\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4225 - val_loss: 0.8040\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.4180 - val_loss: 0.8150\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4081 - val_loss: 0.7952\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.3999 - val_loss: 0.8225\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.3951 - val_loss: 0.7892\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.3906 - val_loss: 0.8017\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.3801 - val_loss: 0.8023\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.3725 - val_loss: 0.8135\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.3605 - val_loss: 0.8034\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.3490 - val_loss: 0.8076\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.3356 - val_loss: 0.8284\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.3307 - val_loss: 0.8144\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.3188 - val_loss: 0.8239\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.3056 - val_loss: 0.8161\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.2963 - val_loss: 0.8414\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.2853 - val_loss: 0.8271\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.2705 - val_loss: 0.8308\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.2628 - val_loss: 0.8541\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.2525 - val_loss: 0.8597\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.2410 - val_loss: 0.8675\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.2334 - val_loss: 0.8757\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.2306 - val_loss: 0.8790\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.2163 - val_loss: 0.8835\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.2059 - val_loss: 0.9016\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.2004 - val_loss: 0.8989\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.1922 - val_loss: 0.9080\n",
      "MSE over 1000 samples on the run 1 : 0.9291 \n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 8.7126 - val_loss: 5.9238\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 3.9230 - val_loss: 2.9666\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 2.0615 - val_loss: 2.3015\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 1.3505 - val_loss: 1.5940\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.8942 - val_loss: 1.2353\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.7019 - val_loss: 1.0903\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.6169 - val_loss: 0.9918\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.5774 - val_loss: 0.9285\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.5424 - val_loss: 0.8821\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.5202 - val_loss: 0.8439\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.5059 - val_loss: 0.8176\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4931 - val_loss: 0.7995\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 50us/step - loss: 0.4821 - val_loss: 0.7922\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.4820 - val_loss: 0.7735\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.4746 - val_loss: 0.7746\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.4661 - val_loss: 0.7758\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.4627 - val_loss: 0.7468\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.4566 - val_loss: 0.7669\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.4571 - val_loss: 0.7500\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4508 - val_loss: 0.7406\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.4486 - val_loss: 0.7439\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.4430 - val_loss: 0.7658\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4428 - val_loss: 0.7432\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4323 - val_loss: 0.7623\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4274 - val_loss: 0.7539\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.4243 - val_loss: 0.7577\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.4290 - val_loss: 0.7624\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4149 - val_loss: 0.7579\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4111 - val_loss: 0.7547\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.4072 - val_loss: 0.7432\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.3962 - val_loss: 0.7660\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.3889 - val_loss: 0.7689\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.3848 - val_loss: 0.7637\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.3843 - val_loss: 0.7646\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.3765 - val_loss: 0.7691\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.3701 - val_loss: 0.7642\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.3616 - val_loss: 0.7843\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.3521 - val_loss: 0.7777\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.3452 - val_loss: 0.7971\n",
      "Epoch 40/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.3372 - val_loss: 0.7850\n",
      "Epoch 41/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.3335 - val_loss: 0.7829\n",
      "Epoch 42/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.3240 - val_loss: 0.7794\n",
      "Epoch 43/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.3157 - val_loss: 0.8010\n",
      "Epoch 44/50\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.3066 - val_loss: 0.8025\n",
      "Epoch 45/50\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.2970 - val_loss: 0.8070\n",
      "Epoch 46/50\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.2909 - val_loss: 0.8238\n",
      "Epoch 47/50\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.2832 - val_loss: 0.8372\n",
      "Epoch 48/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.2729 - val_loss: 0.8360\n",
      "Epoch 49/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.2674 - val_loss: 0.8283\n",
      "Epoch 50/50\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.2614 - val_loss: 0.8422\n",
      "MSE over 1000 samples on the run 2 : 0.7702 \n",
      "Mean of the MSE on 3 independant runs 0.8790 \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "embedding_size = 50\n",
    "print(\"Embedding size : %d\" % (embedding_size))\n",
    "\n",
    "for i in range(0, 3) : \n",
    "\n",
    "    # Load the data\n",
    "    data = load_data(url_reset,params)\n",
    "    user_history = data[\"user_history\"]\n",
    "    item_history = data[\"item_history\"]\n",
    "    rating_history = data[\"rating_history\"]\n",
    "    variable_history = np.array(data[\"variables_history\"])\n",
    "    train_ratings = pd.DataFrame({\"user_id\": user_history, \"item_id\": item_history, \"rating\": rating_history})\n",
    "\n",
    "    nb_users = data[\"nb_users\"]\n",
    "    nb_items = data[\"nb_items\"]\n",
    "    next_user = data[\"next_user\"]\n",
    "    next_item = data[\"next_item\"]\n",
    "    next_variable = np.array(data[\"next_variables\"])\n",
    "\n",
    "    nb_samples = 1000\n",
    "    mse = 0\n",
    "\n",
    "    train_x = [train_ratings[\"user_id\"], train_ratings[\"item_id\"]]\n",
    "    train_y = train_ratings[\"rating\"]\n",
    "\n",
    "    model, history = recommender_system_deep_covariates(train_ratings[\"user_id\"], train_ratings[\"item_id\"], \\\n",
    "                                                        variable_history, train_y, embedding_size, nb_users,\\\n",
    "                                                        nb_items)\n",
    "\n",
    "    for j in range(nb_samples) : \n",
    "        time.sleep(0.5)\n",
    "        prediction = model.predict([[next_user], [next_item], next_variable.reshape(1, 5)])[0][0]\n",
    "        params[\"predicted_score\"] = prediction\n",
    "        next_data = requests.get(url=url_predict, params=params).json()\n",
    "        rating = next_data[\"rating\"]\n",
    "        next_user = next_data[\"next_user\"]\n",
    "        next_item = next_data[\"next_item\"]\n",
    "        next_variable = np.array(next_data[\"next_variables\"])\n",
    "        mse += mean_square_error(rating, prediction)\n",
    "\n",
    "    mean += mse/nb_samples\n",
    "    print(\"MSE over 1000 samples on the run %d : %.4f \" % (i, mse/nb_samples))\n",
    "\n",
    "print(\"Mean of the MSE on 3 independant runs %.4f \" % (mean/3))\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='blue'>\n",
    "    We observe that the validation loss starts to increase in the last epochs. So we will use now Early Stopping to avoid this.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_system_deep_covariates_early_stop(user, item, variable_history, rating, embedding_size, nb_users, nb_items, verbose=True) :    \n",
    "    \n",
    "    user_id_input = Input(shape=[1],name='user')\n",
    "    item_id_input = Input(shape=[1], name='item')\n",
    "    meta_input = Input(shape=[5], name='meta_item')\n",
    "    embedding_size = embedding_size\n",
    "    user_embedding = Embedding(output_dim=embedding_size, input_dim=nb_users + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "    item_embedding = Embedding(output_dim=embedding_size, input_dim=nb_items + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    item_vecs = Flatten()(item_embedding)\n",
    "    input_vecs = Concatenate()([user_vecs, item_vecs, meta_input])\n",
    "    x = Dense(64, activation='relu')(input_vecs)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[user_id_input, item_id_input, meta_input], outputs=y)\n",
    "    model.compile(optimizer='adam', loss='MSE')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
    "    history = model.fit([user, item, variable_history], rating,\n",
    "                        batch_size=64, epochs=50, validation_split=0.1,\n",
    "                        shuffle=True, verbose=verbose, callbacks=[early_stopping])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size : 50\n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 9.4843 - val_loss: 6.4334\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 4.4285 - val_loss: 2.3791\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 1.8600 - val_loss: 1.6224\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 1.1695 - val_loss: 1.1278\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.7970 - val_loss: 0.9093\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - ETA: 0s - loss: 0.647 - 0s 37us/step - loss: 0.6536 - val_loss: 0.8330\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.5876 - val_loss: 0.7957\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.5542 - val_loss: 0.7613\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.5377 - val_loss: 0.7311\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.5256 - val_loss: 0.7200\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.5170 - val_loss: 0.7266\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 43us/step - loss: 0.5040 - val_loss: 0.7302\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.4953 - val_loss: 0.6999\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.4916 - val_loss: 0.6871\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.4854 - val_loss: 0.6861\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.4785 - val_loss: 0.6790\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4723 - val_loss: 0.6835\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.4651 - val_loss: 0.6704\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.4613 - val_loss: 0.6686\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.4586 - val_loss: 0.6688\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.4505 - val_loss: 0.6679\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.4428 - val_loss: 0.6549\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.4350 - val_loss: 0.6637\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.4265 - val_loss: 0.6486\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4142 - val_loss: 0.6374\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.3992 - val_loss: 0.6330\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 52us/step - loss: 0.3901 - val_loss: 0.6331\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 59us/step - loss: 0.3765 - val_loss: 0.6474\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 0.3682 - val_loss: 0.6330\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.3569 - val_loss: 0.6240\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.3446 - val_loss: 0.6194\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.3280 - val_loss: 0.6335\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.3218 - val_loss: 0.6069\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.3053 - val_loss: 0.6127\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.2959 - val_loss: 0.6120\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.2853 - val_loss: 0.6080\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.2772 - val_loss: 0.6172\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.2705 - val_loss: 0.6515\n",
      "Epoch 39/50\n",
      "1800/1800 [==============================] - 0s 46us/step - loss: 0.2571 - val_loss: 0.6475\n",
      "MSE over 1000 samples on the run 0 : 0.8358 \n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 3s 1ms/step - loss: 9.1437 - val_loss: 6.6865\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 53us/step - loss: 4.0312 - val_loss: 2.5708\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 50us/step - loss: 1.9277 - val_loss: 1.7792\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 1.3022 - val_loss: 1.3360\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.9069 - val_loss: 1.0750\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.7306 - val_loss: 0.9942\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.6493 - val_loss: 0.9193\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.6093 - val_loss: 0.8655\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 43us/step - loss: 0.5777 - val_loss: 0.8345\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.5577 - val_loss: 0.8280\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.5483 - val_loss: 0.8167\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.5367 - val_loss: 0.8015\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.5290 - val_loss: 0.7985\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.5228 - val_loss: 0.7828\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.5183 - val_loss: 0.7893\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.5127 - val_loss: 0.7755\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4999 - val_loss: 0.7795\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.4948 - val_loss: 0.7778\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.4879 - val_loss: 0.7591\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.4784 - val_loss: 0.7393\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.4719 - val_loss: 0.7545\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.4591 - val_loss: 0.7382\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.4520 - val_loss: 0.7315\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.4483 - val_loss: 0.7255\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.4366 - val_loss: 0.7230\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.4284 - val_loss: 0.7345\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.4215 - val_loss: 0.7154\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.4090 - val_loss: 0.7244\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 0.4003 - val_loss: 0.7227\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 0.3927 - val_loss: 0.7295\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 0.3823 - val_loss: 0.7221\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 47us/step - loss: 0.3721 - val_loss: 0.7151\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.3646 - val_loss: 0.7181\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.3552 - val_loss: 0.7383\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.3486 - val_loss: 0.7282\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.3389 - val_loss: 0.7284\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 48us/step - loss: 0.3316 - val_loss: 0.7353\n",
      "Epoch 38/50\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.3194 - val_loss: 0.7460\n",
      "MSE over 1000 samples on the run 1 : 0.7942 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 3s 1ms/step - loss: 7.3977 - val_loss: 5.1457\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 3.3322 - val_loss: 2.4844\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 1.7772 - val_loss: 1.8019\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 1.1298 - val_loss: 1.2987\n",
      "Epoch 5/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.7613 - val_loss: 1.0517\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.6306 - val_loss: 0.9374\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.5830 - val_loss: 0.8449\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.5607 - val_loss: 0.8145\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.5453 - val_loss: 0.7763\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.5382 - val_loss: 0.7717\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.5264 - val_loss: 0.7299\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.5206 - val_loss: 0.7207\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.5148 - val_loss: 0.7246\n",
      "Epoch 14/50\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.5099 - val_loss: 0.7240\n",
      "Epoch 15/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.5022 - val_loss: 0.7151\n",
      "Epoch 16/50\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.4943 - val_loss: 0.7044\n",
      "Epoch 17/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.4923 - val_loss: 0.6924\n",
      "Epoch 18/50\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.4813 - val_loss: 0.6813\n",
      "Epoch 19/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.4768 - val_loss: 0.6909\n",
      "Epoch 20/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.4758 - val_loss: 0.6864\n",
      "Epoch 21/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.4670 - val_loss: 0.6704\n",
      "Epoch 22/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.4566 - val_loss: 0.6709\n",
      "Epoch 23/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.4539 - val_loss: 0.6530\n",
      "Epoch 24/50\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.4400 - val_loss: 0.6540\n",
      "Epoch 25/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.4312 - val_loss: 0.6510\n",
      "Epoch 26/50\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.4233 - val_loss: 0.6631\n",
      "Epoch 27/50\n",
      "1800/1800 [==============================] - 0s 50us/step - loss: 0.4115 - val_loss: 0.6469\n",
      "Epoch 28/50\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 0.4008 - val_loss: 0.6379\n",
      "Epoch 29/50\n",
      "1800/1800 [==============================] - 0s 43us/step - loss: 0.3879 - val_loss: 0.6304\n",
      "Epoch 30/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.3726 - val_loss: 0.6276\n",
      "Epoch 31/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.3667 - val_loss: 0.6197\n",
      "Epoch 32/50\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.3553 - val_loss: 0.6339\n",
      "Epoch 33/50\n",
      "1800/1800 [==============================] - 0s 43us/step - loss: 0.3443 - val_loss: 0.6449\n",
      "Epoch 34/50\n",
      "1800/1800 [==============================] - 0s 52us/step - loss: 0.3311 - val_loss: 0.6565\n",
      "Epoch 35/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.3215 - val_loss: 0.6475\n",
      "Epoch 36/50\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.3065 - val_loss: 0.6535\n",
      "Epoch 37/50\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.2982 - val_loss: 0.6520\n",
      "MSE over 1000 samples on the run 2 : 1.0412 \n",
      "Mean of the MSE on 3 independant runs 0.8904 \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "embedding_size = 50\n",
    "print(\"Embedding size : %d\" % (embedding_size))\n",
    "\n",
    "for i in range(0, 3) : \n",
    "\n",
    "    # Load the data\n",
    "    data = load_data(url_reset,params)\n",
    "    user_history = data[\"user_history\"]\n",
    "    item_history = data[\"item_history\"]\n",
    "    rating_history = data[\"rating_history\"]\n",
    "    variable_history = np.array(data[\"variables_history\"])\n",
    "    train_ratings = pd.DataFrame({\"user_id\": user_history, \"item_id\": item_history, \"rating\": rating_history})\n",
    "\n",
    "    nb_users = data[\"nb_users\"]\n",
    "    nb_items = data[\"nb_items\"]\n",
    "    next_user = data[\"next_user\"]\n",
    "    next_item = data[\"next_item\"]\n",
    "    next_variable = np.array(data[\"next_variables\"])\n",
    "\n",
    "    nb_samples = 1000\n",
    "    mse = 0\n",
    "\n",
    "    train_x = [train_ratings[\"user_id\"], train_ratings[\"item_id\"]]\n",
    "    train_y = train_ratings[\"rating\"]\n",
    "\n",
    "    model, history = recommender_system_deep_covariates_early_stop(train_ratings[\"user_id\"], train_ratings[\"item_id\"], \\\n",
    "                                                        variable_history, train_y, embedding_size, nb_users,\\\n",
    "                                                        nb_items)\n",
    "\n",
    "    for j in range(nb_samples) : \n",
    "        time.sleep(0.5)\n",
    "        prediction = model.predict([[next_user], [next_item], next_variable.reshape(1, 5)])[0][0]\n",
    "        params[\"predicted_score\"] = prediction\n",
    "        next_data = requests.get(url=url_predict, params=params).json()\n",
    "        rating = next_data[\"rating\"]\n",
    "        next_user = next_data[\"next_user\"]\n",
    "        next_item = next_data[\"next_item\"]\n",
    "        next_variable = np.array(next_data[\"next_variables\"])\n",
    "        mse += mean_square_error(rating, prediction)\n",
    "\n",
    "    mean += mse/nb_samples\n",
    "    print(\"MSE over 1000 samples on the run %d : %.4f \" % (i, mse/nb_samples))\n",
    "\n",
    "print(\"Mean of the MSE on 3 independant runs %.4f \" % (mean/3))\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After many tests : we have arrived at the following model which allows us to have the best MSE : \n",
    "- Deep model with 1 FC layer\n",
    "- 1 FC layer with 128 neuron units\n",
    "- Use Drop out (0.5)\n",
    "- with using Early Stopping\n",
    "- MSE over 1000 samples on the run 0 : 0.6388 \n",
    "- MSE over 1000 samples on the run 1 : 0.7189 \n",
    "- MSE over 1000 samples on the run 2 : 0.5353 \n",
    "- Mean of the MSE on 3 independant runs 0.6310 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_system_deep_covariates_early_stop(user, item, variable_history, rating, embedding_size, nb_users, nb_items, verbose=True) :    \n",
    "    \n",
    "    user_id_input = Input(shape=[1],name='user')\n",
    "    item_id_input = Input(shape=[1], name='item')\n",
    "    meta_input = Input(shape=[5], name='meta_item')\n",
    "    embedding_size = embedding_size\n",
    "    user_embedding = Embedding(output_dim=embedding_size, input_dim=nb_users + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "    item_embedding = Embedding(output_dim=embedding_size, input_dim=nb_items + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    item_vecs = Flatten()(item_embedding)\n",
    "    input_vecs = Concatenate()([user_vecs, item_vecs, meta_input])\n",
    "    x = Dense(128, activation='relu')(input_vecs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[user_id_input, item_id_input, meta_input], outputs=y)\n",
    "    model.compile(optimizer='adam', loss='MSE')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    history = model.fit([user, item, variable_history], rating,\n",
    "                        batch_size=64, epochs=50, validation_split=0.1,\n",
    "                        shuffle=True, verbose=verbose, callbacks=[early_stopping])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size : 50\n",
      "MSE over 1000 samples on the run 0 : 0.6388 \n",
      "MSE over 1000 samples on the run 1 : 0.7189 \n",
      "MSE over 1000 samples on the run 2 : 0.5353 \n",
      "Mean of the MSE on 3 independant runs 0.6310 \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "embedding_size = 50\n",
    "print(\"Embedding size : %d\" % (embedding_size))\n",
    "\n",
    "for i in range(0, 3) : \n",
    "\n",
    "    # Load the data\n",
    "    data = load_data(url_reset,params)\n",
    "    user_history = data[\"user_history\"]\n",
    "    item_history = data[\"item_history\"]\n",
    "    rating_history = data[\"rating_history\"]\n",
    "    variable_history = np.array(data[\"variables_history\"])\n",
    "    train_ratings = pd.DataFrame({\"user_id\": user_history, \"item_id\": item_history, \"rating\": rating_history})\n",
    "\n",
    "    nb_users = data[\"nb_users\"]\n",
    "    nb_items = data[\"nb_items\"]\n",
    "    next_user = data[\"next_user\"]\n",
    "    next_item = data[\"next_item\"]\n",
    "    next_variable = np.array(data[\"next_variables\"])\n",
    "\n",
    "    nb_samples = 200\n",
    "    mse = 0\n",
    "\n",
    "    train_x = [train_ratings[\"user_id\"], train_ratings[\"item_id\"]]\n",
    "    train_y = train_ratings[\"rating\"]\n",
    "\n",
    "    model, history = recommender_system_deep_covariates_early_stop(train_ratings[\"user_id\"], train_ratings[\"item_id\"], \\\n",
    "                                                        variable_history, train_y, embedding_size, nb_users,\\\n",
    "                                                        nb_items, verbose=False)\n",
    "\n",
    "    for j in range(nb_samples) : \n",
    "        time.sleep(0.5)\n",
    "        prediction = model.predict([[next_user], [next_item], next_variable.reshape(1, 5)])[0][0]\n",
    "        params[\"predicted_score\"] = prediction\n",
    "        next_data = requests.get(url=url_predict, params=params).json()\n",
    "        rating = next_data[\"rating\"]\n",
    "        next_user = next_data[\"next_user\"]\n",
    "        next_item = next_data[\"next_item\"]\n",
    "        next_variable = np.array(next_data[\"next_variables\"])\n",
    "        mse += mean_square_error(rating, prediction)\n",
    "\n",
    "    mean += mse/nb_samples\n",
    "    print(\"MSE over 1000 samples on the run %d : %.4f \" % (i, mse/nb_samples))\n",
    "\n",
    "print(\"Mean of the MSE on 3 independant runs %.4f \" % (mean/3))\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model, to_file='model.png')\n",
    "Image(filename='model.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
